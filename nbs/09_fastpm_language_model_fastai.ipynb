{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.tabular import * \n",
    "from fastai.text import *\n",
    "from exp.eventlog import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 2012-04-23T00:00:00.000+02:00\n",
      "failed to parse date: 2011-10-01T00:38:44.546+02:00\n",
      "failed to parse date: 2012-03-14T16:04:54.681+01:00\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.BPIC_2012)\n",
    "log = import_xes(path,extensions=False,classifiers=False,schema=False,log_attributes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-09-30 22:39:38.875000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-10-01 09:36:46.437000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trace_id  event_id org:resource lifecycle:transition  \\\n",
       "0   173688         0          112             COMPLETE   \n",
       "1   173688         1          112             COMPLETE   \n",
       "2   173688         2          112             COMPLETE   \n",
       "3   173688         3          112             SCHEDULE   \n",
       "4   173688         4         None                START   \n",
       "\n",
       "             concept:name                   time:timestamp  \n",
       "0             A_SUBMITTED 2011-09-30 22:38:44.546000+00:00  \n",
       "1       A_PARTLYSUBMITTED 2011-09-30 22:38:44.880000+00:00  \n",
       "2           A_PREACCEPTED 2011-09-30 22:39:37.906000+00:00  \n",
       "3  W_Completeren aanvraag 2011-09-30 22:39:38.875000+00:00  \n",
       "4  W_Completeren aanvraag 2011-10-01 09:36:46.437000+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = log.events\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A_ACCEPTED</th>\n",
       "      <th>A_ACTIVATED</th>\n",
       "      <th>A_APPROVED</th>\n",
       "      <th>A_CANCELLED</th>\n",
       "      <th>A_DECLINED</th>\n",
       "      <th>A_FINALIZED</th>\n",
       "      <th>A_PARTLYSUBMITTED</th>\n",
       "      <th>A_PREACCEPTED</th>\n",
       "      <th>A_REGISTERED</th>\n",
       "      <th>A_SUBMITTED</th>\n",
       "      <th>...</th>\n",
       "      <th>O_SELECTED</th>\n",
       "      <th>O_SENT</th>\n",
       "      <th>O_SENT_BACK</th>\n",
       "      <th>W_Afhandelen leads</th>\n",
       "      <th>W_Beoordelen fraude</th>\n",
       "      <th>W_Completeren aanvraag</th>\n",
       "      <th>W_Nabellen incomplete dossiers</th>\n",
       "      <th>W_Nabellen offertes</th>\n",
       "      <th>W_Valideren aanvraag</th>\n",
       "      <th>W_Wijzigen contractgegevens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262170</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262172</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262173</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262174</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262175</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262176</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262177</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262178</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262179</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262180</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262182</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262183</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262184</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262185</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262186</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262187</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262188</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262191</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262193</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262194</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262195</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262196</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262197</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262198</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262199</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262200 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A_ACCEPTED  A_ACTIVATED  A_APPROVED  A_CANCELLED  A_DECLINED  \\\n",
       "0                0            0           0            0           0   \n",
       "1                0            0           0            0           0   \n",
       "2                0            0           0            0           0   \n",
       "3                0            0           0            0           0   \n",
       "4                0            0           0            0           0   \n",
       "...            ...          ...         ...          ...         ...   \n",
       "262195           0            0           0            0           0   \n",
       "262196           0            0           0            0           0   \n",
       "262197           0            0           0            0           0   \n",
       "262198           0            0           0            0           1   \n",
       "262199           0            0           0            0           0   \n",
       "\n",
       "        A_FINALIZED  A_PARTLYSUBMITTED  A_PREACCEPTED  A_REGISTERED  \\\n",
       "0                 0                  0              0             0   \n",
       "1                 0                  1              0             0   \n",
       "2                 0                  0              1             0   \n",
       "3                 0                  0              0             0   \n",
       "4                 0                  0              0             0   \n",
       "...             ...                ...            ...           ...   \n",
       "262195            0                  1              0             0   \n",
       "262196            0                  0              0             0   \n",
       "262197            0                  0              0             0   \n",
       "262198            0                  0              0             0   \n",
       "262199            0                  0              0             0   \n",
       "\n",
       "        A_SUBMITTED  ...  O_SELECTED  O_SENT  O_SENT_BACK  W_Afhandelen leads  \\\n",
       "0                 1  ...           0       0            0                   0   \n",
       "1                 0  ...           0       0            0                   0   \n",
       "2                 0  ...           0       0            0                   0   \n",
       "3                 0  ...           0       0            0                   0   \n",
       "4                 0  ...           0       0            0                   0   \n",
       "...             ...  ...         ...     ...          ...                 ...   \n",
       "262195            0  ...           0       0            0                   0   \n",
       "262196            0  ...           0       0            0                   1   \n",
       "262197            0  ...           0       0            0                   1   \n",
       "262198            0  ...           0       0            0                   0   \n",
       "262199            0  ...           0       0            0                   1   \n",
       "\n",
       "        W_Beoordelen fraude  W_Completeren aanvraag  \\\n",
       "0                         0                       0   \n",
       "1                         0                       0   \n",
       "2                         0                       0   \n",
       "3                         0                       1   \n",
       "4                         0                       1   \n",
       "...                     ...                     ...   \n",
       "262195                    0                       0   \n",
       "262196                    0                       0   \n",
       "262197                    0                       0   \n",
       "262198                    0                       0   \n",
       "262199                    0                       0   \n",
       "\n",
       "        W_Nabellen incomplete dossiers  W_Nabellen offertes  \\\n",
       "0                                    0                    0   \n",
       "1                                    0                    0   \n",
       "2                                    0                    0   \n",
       "3                                    0                    0   \n",
       "4                                    0                    0   \n",
       "...                                ...                  ...   \n",
       "262195                               0                    0   \n",
       "262196                               0                    0   \n",
       "262197                               0                    0   \n",
       "262198                               0                    0   \n",
       "262199                               0                    0   \n",
       "\n",
       "        W_Valideren aanvraag  W_Wijzigen contractgegevens  \n",
       "0                          0                            0  \n",
       "1                          0                            0  \n",
       "2                          0                            0  \n",
       "3                          0                            0  \n",
       "4                          0                            0  \n",
       "...                      ...                          ...  \n",
       "262195                     0                            0  \n",
       "262196                     0                            0  \n",
       "262197                     0                            0  \n",
       "262198                     0                            0  \n",
       "262199                     0                            0  \n",
       "\n",
       "[262200 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df['concept:name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A_SUBMITTED A_PARTLYSUBMITTED A_PREACCEPTED W_Completeren aanvraag W_Completeren aanvraag A_ACCEPTED O_SELECTED A_FINALIZED O_CREATED O_SENT W_Nabellen offertes W_Completeren aanvraag W_Nabellen offertes W_Nabellen offertes W_Nabellen offertes W_Nabellen offertes W_Nabellen offertes O_SENT_BACK W_Valideren aanvraag W_Nabellen offertes W_Valideren aanvraag A_REGISTERED A_APPROVED O_ACCEPTED A_ACTIVATED W_Valideren aanvraag'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = df.groupby(\"trace_id\")[\"concept:name\"].apply(lambda x: \" \".join(x))\n",
    "hi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A_SUBMITTED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'W_Completeren aanvraag', 'A_ACCEPTED', 'O_SELECTED',\n",
       "       'A_FINALIZED', 'O_CREATED', 'O_SENT', 'W_Nabellen offertes', 'O_SENT_BACK', 'W_Valideren aanvraag',\n",
       "       'A_REGISTERED', 'A_APPROVED', 'O_ACCEPTED', 'A_ACTIVATED', 'O_CANCELLED', 'W_Wijzigen contractgegevens',\n",
       "       'A_DECLINED', 'A_CANCELLED', 'W_Afhandelen leads', 'O_DECLINED', 'W_Nabellen incomplete dossiers',\n",
       "       'W_Beoordelen fraude'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = df['concept:name'].unique()\n",
    "display(values, len(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "dataframe = pd.DataFrame([[0,10,'J'],[0,12,'A'],[0,15,'K'],[0,2,'O'],[0,52,'B'],\n",
    "                          [1,23,'S'],[1,21,'E'],[1,92,'B']], \n",
    "                         columns=['A','B','C'])\n",
    "display(dataframe)\n",
    "\n",
    "dataframe = dataframe.groupby('A')['C'].apply(lambda x: ''.join(x))\n",
    "display(dataframe)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert traces so every trace only has one row\n",
    "def convert_traces(df):\n",
    "    return Series(dict(event_id = ' '.join(df['event_id'].map(str)),\n",
    "                       org_resource = ' '.join(df['org:resource'].map(str)),\n",
    "                       lifecycle_transition = ' '.join(df['lifecycle:transition'].map(str)),\n",
    "                       concept_name = ' '.join(df['concept:name'].map(str)),\n",
    "                       time_timestamp = ' '.join(df['time:timestamp'].map(str))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert traces generalized\n",
    "def conv_traces(df):\n",
    "    d = {}\n",
    "    for i in df.columns:\n",
    "        d[i] = ' '.join(df[i].map(str))\n",
    "        \n",
    "    return Series(d)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.groupby('trace_id')[df.columns[df.columns!='trace_id']].apply(conv_traces)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Text Trainingsdaten als Liste\n",
    "concept_names = df['concept:name'].to_list()\n",
    "display(concept_names[:2])\n",
    "\n",
    "# Text Trainingsdaten isoliert in neuem Dataframe\n",
    "text_df = df['concept:name']\n",
    "text_train_df = text_df[:10000]\n",
    "text_valid_df = text_df[10000:13086]\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing FastAi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(events, trace_attributes, cat_columns, date_columns, num_columns, vocab=None):\n",
    "    if vocab is None:\n",
    "        vocab = {}\n",
    "        \n",
    "    events = events.copy()\n",
    "    trace_attributes = trace_attributes.copy()\n",
    "    \n",
    "    for d in date_columns:\n",
    "        if d in list(events): \n",
    "            cat, con = add_datepart(events,d,utc=True)\n",
    "            for i in range(len(cat)):\n",
    "                if cat[i] not in cat_columns:\n",
    "                    cat_columns.append(cat[i])\n",
    "            if con not in num_columns:\n",
    "                num_columns.append(con)\n",
    "                \n",
    "        if d in list(trace_attributes): \n",
    "            cat, con = add_datepart(trace_attributes,d,utc=True)\n",
    "            for i in range(len(cat)):\n",
    "                if cat[i] not in cat_columns:\n",
    "                    cat_columns.append(cat[i])\n",
    "            if con not in num_columns:\n",
    "                num_columns.append(con)            \n",
    "        \n",
    "    for c in cat_columns: \n",
    "        if c in vocab.keys():\n",
    "            proc = vocab[c]\n",
    "            \n",
    "            if c in list(events):\n",
    "                events[c] = proc(events[c])\n",
    "            if c in list(trace_attributes):\n",
    "                trace_attributes[c] = proc(trace_attributes[c])\n",
    "            \n",
    "        else:\n",
    "            if c in list(events): \n",
    "                proc = CategoryProcessor(default_spec_tok)\n",
    "                events[c] = proc(events[c])\n",
    "            if c in list(trace_attributes): \n",
    "                proc = CategoryProcessor()\n",
    "                trace_attributes[c] = proc(trace_attributes[c])\n",
    "\n",
    "            vocab[c] = proc   \n",
    "        \n",
    "    for n in num_columns:\n",
    "        if n in vocab:\n",
    "            mean, std = vocab[n]\n",
    "            \n",
    "            if n in list(events):\n",
    "                events[n] = events[n].astype(float)\n",
    "                events[n] = (events[n]-mean) / (1e-7 + std)\n",
    "            if n in list(trace_attributes):\n",
    "                trace_attributes[n] = trace_attributes[n].astype(float)\n",
    "                trace_attributes[n] = (trace_attributes[n]-mean) / (1e-7 + std)\n",
    "                \n",
    "        else:\n",
    "            if n in list(events): \n",
    "                events[n] = events[n].astype(float)\n",
    "                mean, std = events[n].mean(), events[n].std()\n",
    "                events[n] = (events[n]-mean) / (1e-7 + std)\n",
    "            if n in list(trace_attributes): \n",
    "                trace_attributes[n] = trace_attributes[n].astype(float)\n",
    "                mean, std = trace_attributes[n].mean(), trace_attributes[n].std()\n",
    "                trace_attributes[n] = (trace_attributes[n]-mean) / (1e-7 + std)\n",
    "\n",
    "            vocab[n] = mean, std\n",
    "        \n",
    "    return events, trace_attributes, cat_columns, num_columns, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res\n",
    "\n",
    "class Processor():\n",
    "    def process(self, items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self,default_token=None): \n",
    "        self.vocab=None\n",
    "        self.default_token=default_token\n",
    "\n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            if self.default_token is not None:\n",
    "                for o in reversed(self.default_token):\n",
    "                    if o in self.vocab: self.vocab.remove(o)\n",
    "                    self.vocab.insert(0, o)\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return self.vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_numericalize=CategoryProcessor(default_spec_tok)\n",
    "user_numericalize(df['org:resource'])\n",
    "\n",
    "activity_numericalize=CategoryProcessor(default_spec_tok)\n",
    "activity_numericalize(df['concept:name']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.71 s, sys: 230 µs, total: 2.71 s\n",
      "Wall time: 2.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def create_traces(df):\n",
    "    ll=[]\n",
    "    trace_ids=[]\n",
    "    \n",
    "    for n, g in df.groupby('trace_id'):\n",
    "        ea = (g['concept:name'])\n",
    "        eu = (g['org:resource'])\n",
    "        ll.append([ea, eu])\n",
    "        trace_ids.append(n)\n",
    "\n",
    "    df2 = pd.DataFrame(ll, columns=[\"Activities\",'Users'])\n",
    "    df2.index = trace_ids\n",
    "    return df2\n",
    "\n",
    "data = create_traces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A_SUBMITTED',\n",
       " 'A_PARTLYSUBMITTED',\n",
       " 'A_PREACCEPTED',\n",
       " 'W_Completeren aanvraag',\n",
       " 'W_Completeren aanvraag',\n",
       " 'A_ACCEPTED',\n",
       " 'O_SELECTED',\n",
       " 'A_FINALIZED',\n",
       " 'O_CREATED',\n",
       " 'O_SENT',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Completeren aanvraag',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Nabellen offertes',\n",
       " 'O_SENT_BACK',\n",
       " 'W_Valideren aanvraag',\n",
       " 'W_Nabellen offertes',\n",
       " 'W_Valideren aanvraag',\n",
       " 'A_REGISTERED',\n",
       " 'A_APPROVED',\n",
       " 'O_ACCEPTED',\n",
       " 'A_ACTIVATED',\n",
       " 'W_Valideren aanvraag']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_numericalize.deprocess(activity_numericalize(data['Activities'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activities</th>\n",
       "      <th>Users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173688</th>\n",
       "      <td>0                A_SUBMITTED\n",
       "1          A_PART...</td>\n",
       "      <td>0       112\n",
       "1       112\n",
       "2       112\n",
       "3       11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173691</th>\n",
       "      <td>26               A_SUBMITTED\n",
       "27         A_PART...</td>\n",
       "      <td>26      112\n",
       "27      112\n",
       "28      112\n",
       "29      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173694</th>\n",
       "      <td>65                     A_SUBMITTED\n",
       "66         ...</td>\n",
       "      <td>65       112\n",
       "66       112\n",
       "67       112\n",
       "68     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173697</th>\n",
       "      <td>124          A_SUBMITTED\n",
       "125    A_PARTLYSUBMIT...</td>\n",
       "      <td>124    112\n",
       "125    112\n",
       "126    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173700</th>\n",
       "      <td>127          A_SUBMITTED\n",
       "128    A_PARTLYSUBMIT...</td>\n",
       "      <td>127    112\n",
       "128    112\n",
       "129    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173703</th>\n",
       "      <td>130               A_SUBMITTED\n",
       "131         A_PA...</td>\n",
       "      <td>130      112\n",
       "131      112\n",
       "132      112\n",
       "133    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173706</th>\n",
       "      <td>139               A_SUBMITTED\n",
       "140         A_PA...</td>\n",
       "      <td>139      112\n",
       "140      112\n",
       "141      112\n",
       "142    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173709</th>\n",
       "      <td>153               A_SUBMITTED\n",
       "154         A_PA...</td>\n",
       "      <td>153      112\n",
       "154      112\n",
       "155      112\n",
       "156    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173712</th>\n",
       "      <td>165               A_SUBMITTED\n",
       "166         A_PA...</td>\n",
       "      <td>165      112\n",
       "166      112\n",
       "167      112\n",
       "168    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173715</th>\n",
       "      <td>179               A_SUBMITTED\n",
       "180         A_PA...</td>\n",
       "      <td>179      112\n",
       "180      112\n",
       "181      112\n",
       "182    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173718</th>\n",
       "      <td>203               A_SUBMITTED\n",
       "204         A_PA...</td>\n",
       "      <td>203      112\n",
       "204      112\n",
       "205      112\n",
       "206    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173721</th>\n",
       "      <td>280                       A_SUBMITTED\n",
       "281     ...</td>\n",
       "      <td>280      112\n",
       "281      112\n",
       "282      112\n",
       "283    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173724</th>\n",
       "      <td>315               A_SUBMITTED\n",
       "316         A_PA...</td>\n",
       "      <td>315      112\n",
       "316      112\n",
       "317      112\n",
       "318    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173727</th>\n",
       "      <td>335          A_SUBMITTED\n",
       "336    A_PARTLYSUBMIT...</td>\n",
       "      <td>335    112\n",
       "336    112\n",
       "337    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173730</th>\n",
       "      <td>338               A_SUBMITTED\n",
       "339         A_PA...</td>\n",
       "      <td>338      112\n",
       "339      112\n",
       "340      112\n",
       "341    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173733</th>\n",
       "      <td>414           A_SUBMITTED\n",
       "415     A_PARTLYSUBM...</td>\n",
       "      <td>414      112\n",
       "415      112\n",
       "416      112\n",
       "417    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173736</th>\n",
       "      <td>420                       A_SUBMITTED\n",
       "421     ...</td>\n",
       "      <td>420      112\n",
       "421      112\n",
       "422      112\n",
       "423    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173739</th>\n",
       "      <td>475                       A_SUBMITTED\n",
       "476     ...</td>\n",
       "      <td>475      112\n",
       "476      112\n",
       "477      112\n",
       "478    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173742</th>\n",
       "      <td>523               A_SUBMITTED\n",
       "524         A_PA...</td>\n",
       "      <td>523      112\n",
       "524      112\n",
       "525      112\n",
       "526    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173745</th>\n",
       "      <td>563               A_SUBMITTED\n",
       "564         A_PA...</td>\n",
       "      <td>563      112\n",
       "564      112\n",
       "565      112\n",
       "566    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173748</th>\n",
       "      <td>582               A_SUBMITTED\n",
       "583         A_PA...</td>\n",
       "      <td>582      112\n",
       "583      112\n",
       "584      112\n",
       "585    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173751</th>\n",
       "      <td>620               A_SUBMITTED\n",
       "621         A_PA...</td>\n",
       "      <td>620      112\n",
       "621      112\n",
       "622      112\n",
       "623    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173754</th>\n",
       "      <td>644               A_SUBMITTED\n",
       "645         A_PA...</td>\n",
       "      <td>644      112\n",
       "645      112\n",
       "646      112\n",
       "647    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173757</th>\n",
       "      <td>686          A_SUBMITTED\n",
       "687    A_PARTLYSUBMIT...</td>\n",
       "      <td>686    112\n",
       "687    112\n",
       "688    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173760</th>\n",
       "      <td>689               A_SUBMITTED\n",
       "690         A_PA...</td>\n",
       "      <td>689      112\n",
       "690      112\n",
       "691      112\n",
       "692    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173763</th>\n",
       "      <td>715           A_SUBMITTED\n",
       "716     A_PARTLYSUBM...</td>\n",
       "      <td>715      112\n",
       "716      112\n",
       "717      112\n",
       "718    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173766</th>\n",
       "      <td>721               A_SUBMITTED\n",
       "722         A_PA...</td>\n",
       "      <td>721      112\n",
       "722      112\n",
       "723      112\n",
       "724    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173769</th>\n",
       "      <td>750          A_SUBMITTED\n",
       "751    A_PARTLYSUBMIT...</td>\n",
       "      <td>750    112\n",
       "751    112\n",
       "752    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173772</th>\n",
       "      <td>753          A_SUBMITTED\n",
       "754    A_PARTLYSUBMIT...</td>\n",
       "      <td>753    112\n",
       "754    112\n",
       "755    112\n",
       "Name: org:res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173775</th>\n",
       "      <td>756               A_SUBMITTED\n",
       "757         A_PA...</td>\n",
       "      <td>756      112\n",
       "757      112\n",
       "758      112\n",
       "759    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214289</th>\n",
       "      <td>261867               A_SUBMITTED\n",
       "261868       ...</td>\n",
       "      <td>261867      112\n",
       "261868      112\n",
       "261869      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214292</th>\n",
       "      <td>261894          A_SUBMITTED\n",
       "261895    A_PARTLY...</td>\n",
       "      <td>261894    112\n",
       "261895    112\n",
       "261896    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214295</th>\n",
       "      <td>261897          A_SUBMITTED\n",
       "261898    A_PARTLY...</td>\n",
       "      <td>261897    112\n",
       "261898    112\n",
       "261899    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214298</th>\n",
       "      <td>261900               A_SUBMITTED\n",
       "261901       ...</td>\n",
       "      <td>261900      112\n",
       "261901      112\n",
       "261902      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214301</th>\n",
       "      <td>261909           A_SUBMITTED\n",
       "261910     A_PART...</td>\n",
       "      <td>261909      112\n",
       "261910      112\n",
       "261911      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214304</th>\n",
       "      <td>261917               A_SUBMITTED\n",
       "261918       ...</td>\n",
       "      <td>261917      112\n",
       "261918      112\n",
       "261919      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214307</th>\n",
       "      <td>261933          A_SUBMITTED\n",
       "261934    A_PARTLY...</td>\n",
       "      <td>261933    112\n",
       "261934    112\n",
       "261935    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214310</th>\n",
       "      <td>261936               A_SUBMITTED\n",
       "261937       ...</td>\n",
       "      <td>261936      112\n",
       "261937      112\n",
       "261938      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214313</th>\n",
       "      <td>261954               A_SUBMITTED\n",
       "261955       ...</td>\n",
       "      <td>261954      112\n",
       "261955      112\n",
       "261956      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214316</th>\n",
       "      <td>261979          A_SUBMITTED\n",
       "261980    A_PARTLY...</td>\n",
       "      <td>261979    112\n",
       "261980    112\n",
       "261981    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214319</th>\n",
       "      <td>261982               A_SUBMITTED\n",
       "261983       ...</td>\n",
       "      <td>261982      112\n",
       "261983      112\n",
       "261984      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214322</th>\n",
       "      <td>261992           A_SUBMITTED\n",
       "261993     A_PART...</td>\n",
       "      <td>261992      112\n",
       "261993      112\n",
       "261994      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214325</th>\n",
       "      <td>261998           A_SUBMITTED\n",
       "261999     A_PART...</td>\n",
       "      <td>261998      112\n",
       "261999      112\n",
       "262000      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214328</th>\n",
       "      <td>262004          A_SUBMITTED\n",
       "262005    A_PARTLY...</td>\n",
       "      <td>262004    112\n",
       "262005    112\n",
       "262006    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214331</th>\n",
       "      <td>262007          A_SUBMITTED\n",
       "262008    A_PARTLY...</td>\n",
       "      <td>262007    112\n",
       "262008    112\n",
       "262009    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214334</th>\n",
       "      <td>262010           A_SUBMITTED\n",
       "262011     A_PART...</td>\n",
       "      <td>262010      112\n",
       "262011      112\n",
       "262012      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214337</th>\n",
       "      <td>262016           A_SUBMITTED\n",
       "262017     A_PART...</td>\n",
       "      <td>262016      112\n",
       "262017      112\n",
       "262018      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214340</th>\n",
       "      <td>262022          A_SUBMITTED\n",
       "262023    A_PARTLY...</td>\n",
       "      <td>262022    112\n",
       "262023    112\n",
       "262024    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214343</th>\n",
       "      <td>262025           A_SUBMITTED\n",
       "262026     A_PART...</td>\n",
       "      <td>262025      112\n",
       "262026      112\n",
       "262027      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214346</th>\n",
       "      <td>262031               A_SUBMITTED\n",
       "262032       ...</td>\n",
       "      <td>262031      112\n",
       "262032      112\n",
       "262033      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214349</th>\n",
       "      <td>262047           A_SUBMITTED\n",
       "262048     A_PART...</td>\n",
       "      <td>262047      112\n",
       "262048      112\n",
       "262049      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214352</th>\n",
       "      <td>262053               A_SUBMITTED\n",
       "262054       ...</td>\n",
       "      <td>262053      112\n",
       "262054      112\n",
       "262055      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214355</th>\n",
       "      <td>262073               A_SUBMITTED\n",
       "262074       ...</td>\n",
       "      <td>262073      112\n",
       "262074      112\n",
       "262075      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214358</th>\n",
       "      <td>262095               A_SUBMITTED\n",
       "262096       ...</td>\n",
       "      <td>262095      112\n",
       "262096      112\n",
       "262097      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214361</th>\n",
       "      <td>262111               A_SUBMITTED\n",
       "262112       ...</td>\n",
       "      <td>262111      112\n",
       "262112      112\n",
       "262113      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214364</th>\n",
       "      <td>262139               A_SUBMITTED\n",
       "262140       ...</td>\n",
       "      <td>262139      112\n",
       "262140      112\n",
       "262141      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214367</th>\n",
       "      <td>262168          A_SUBMITTED\n",
       "262169    A_PARTLY...</td>\n",
       "      <td>262168    112\n",
       "262169    112\n",
       "262170    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214370</th>\n",
       "      <td>262171           A_SUBMITTED\n",
       "262172     A_PART...</td>\n",
       "      <td>262171      112\n",
       "262172      112\n",
       "262173      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214373</th>\n",
       "      <td>262177               A_SUBMITTED\n",
       "262178       ...</td>\n",
       "      <td>262177      112\n",
       "262178      112\n",
       "262179      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214376</th>\n",
       "      <td>262194           A_SUBMITTED\n",
       "262195     A_PART...</td>\n",
       "      <td>262194      112\n",
       "262195      112\n",
       "262196      11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13087 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Activities  \\\n",
       "173688  0                A_SUBMITTED\n",
       "1          A_PART...   \n",
       "173691  26               A_SUBMITTED\n",
       "27         A_PART...   \n",
       "173694  65                     A_SUBMITTED\n",
       "66         ...   \n",
       "173697  124          A_SUBMITTED\n",
       "125    A_PARTLYSUBMIT...   \n",
       "173700  127          A_SUBMITTED\n",
       "128    A_PARTLYSUBMIT...   \n",
       "...                                                   ...   \n",
       "214364  262139               A_SUBMITTED\n",
       "262140       ...   \n",
       "214367  262168          A_SUBMITTED\n",
       "262169    A_PARTLY...   \n",
       "214370  262171           A_SUBMITTED\n",
       "262172     A_PART...   \n",
       "214373  262177               A_SUBMITTED\n",
       "262178       ...   \n",
       "214376  262194           A_SUBMITTED\n",
       "262195     A_PART...   \n",
       "\n",
       "                                                    Users  \n",
       "173688  0       112\n",
       "1       112\n",
       "2       112\n",
       "3       11...  \n",
       "173691  26      112\n",
       "27      112\n",
       "28      112\n",
       "29      11...  \n",
       "173694  65       112\n",
       "66       112\n",
       "67       112\n",
       "68     ...  \n",
       "173697  124    112\n",
       "125    112\n",
       "126    112\n",
       "Name: org:res...  \n",
       "173700  127    112\n",
       "128    112\n",
       "129    112\n",
       "Name: org:res...  \n",
       "...                                                   ...  \n",
       "214364  262139      112\n",
       "262140      112\n",
       "262141      11...  \n",
       "214367  262168    112\n",
       "262169    112\n",
       "262170    112\n",
       "Name...  \n",
       "214370  262171      112\n",
       "262172      112\n",
       "262173      11...  \n",
       "214373  262177      112\n",
       "262178      112\n",
       "262179      11...  \n",
       "214376  262194      112\n",
       "262195      112\n",
       "262196      11...  \n",
       "\n",
       "[13087 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns batches of size\n",
    "       n_seqs * n_steps from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       n_seqs: Batch size, the number of sequences per batch\n",
    "       n_steps: Number of sequence steps per batch\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        \n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(np_arr):\n",
    "    shuffle = np_arr[np.random.permutation(len(np_arr))]\n",
    "    start = int(len(np_arr)*0.8)\n",
    "    end = int(len(np_arr)*0.9)\n",
    "    train, valid, test = shuffle[:start], shuffle[start:end], shuffle[end:]\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50  3 23 99 ... 62 79 90 12] [31 55 53 49 38 77 47 25 32 22] [40 82 63 56 94  1 80 45 76 52]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([z for z in range(0,100)])\n",
    "tr, va, te = random_split(a)\n",
    "\n",
    "print(tr, va, te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um nur die Aktivitäten zu benutzen vorerst data[\"Activities\"].values benutzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_split(df):\n",
    "    \n",
    "    train, valid, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])    \n",
    "    print(len(train), len(valid), len(test))\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469 1309 1309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activities</th>\n",
       "      <th>Users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204885</th>\n",
       "      <td>203968               A_SUBMITTED\n",
       "203969       ...</td>\n",
       "      <td>203968      112\n",
       "203969      112\n",
       "203970      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211396</th>\n",
       "      <td>245599            A_SUBMITTED\n",
       "245600      A_PA...</td>\n",
       "      <td>245599      112\n",
       "245600      112\n",
       "245601      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183289</th>\n",
       "      <td>65613               A_SUBMITTED\n",
       "65614         ...</td>\n",
       "      <td>65613      112\n",
       "65614      112\n",
       "65615      112\n",
       "6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174003</th>\n",
       "      <td>2225          A_SUBMITTED\n",
       "2226    A_PARTLYSUBM...</td>\n",
       "      <td>2225    112\n",
       "2226    112\n",
       "2227    112\n",
       "Name: org:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178194</th>\n",
       "      <td>30471                       A_SUBMITTED\n",
       "30472 ...</td>\n",
       "      <td>30471      112\n",
       "30472      112\n",
       "30473      112\n",
       "3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Activities  \\\n",
       "204885  203968               A_SUBMITTED\n",
       "203969       ...   \n",
       "211396  245599            A_SUBMITTED\n",
       "245600      A_PA...   \n",
       "183289  65613               A_SUBMITTED\n",
       "65614         ...   \n",
       "174003  2225          A_SUBMITTED\n",
       "2226    A_PARTLYSUBM...   \n",
       "178194  30471                       A_SUBMITTED\n",
       "30472 ...   \n",
       "\n",
       "                                                    Users  \n",
       "204885  203968      112\n",
       "203969      112\n",
       "203970      11...  \n",
       "211396  245599      112\n",
       "245600      112\n",
       "245601      11...  \n",
       "183289  65613      112\n",
       "65614      112\n",
       "65615      112\n",
       "6...  \n",
       "174003  2225    112\n",
       "2226    112\n",
       "2227    112\n",
       "Name: org:...  \n",
       "178194  30471      112\n",
       "30472      112\n",
       "30473      112\n",
       "3...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training, valid, test = rand_split(data)\n",
    "display(training[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10469 1309 1309\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activities</th>\n",
       "      <th>Users</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196476</th>\n",
       "      <td>146811           A_SUBMITTED\n",
       "146812     A_PART...</td>\n",
       "      <td>146811      112\n",
       "146812      112\n",
       "146813      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207080</th>\n",
       "      <td>219843          A_SUBMITTED\n",
       "219844    A_PARTLY...</td>\n",
       "      <td>219843    112\n",
       "219844    112\n",
       "219845    112\n",
       "Name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183314</th>\n",
       "      <td>65735               A_SUBMITTED\n",
       "65736         ...</td>\n",
       "      <td>65735      112\n",
       "65736      112\n",
       "65737      112\n",
       "6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207419</th>\n",
       "      <td>221734           A_SUBMITTED\n",
       "221735     A_PART...</td>\n",
       "      <td>221734      112\n",
       "221735      112\n",
       "221736      11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176266</th>\n",
       "      <td>17807               A_SUBMITTED\n",
       "17808         ...</td>\n",
       "      <td>17807      112\n",
       "17808      112\n",
       "17809      112\n",
       "1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Activities  \\\n",
       "196476  146811           A_SUBMITTED\n",
       "146812     A_PART...   \n",
       "207080  219843          A_SUBMITTED\n",
       "219844    A_PARTLY...   \n",
       "183314  65735               A_SUBMITTED\n",
       "65736         ...   \n",
       "207419  221734           A_SUBMITTED\n",
       "221735     A_PART...   \n",
       "176266  17807               A_SUBMITTED\n",
       "17808         ...   \n",
       "\n",
       "                                                    Users  \n",
       "196476  146811      112\n",
       "146812      112\n",
       "146813      11...  \n",
       "207080  219843    112\n",
       "219844    112\n",
       "219845    112\n",
       "Name...  \n",
       "183314  65735      112\n",
       "65736      112\n",
       "65737      112\n",
       "6...  \n",
       "207419  221734      112\n",
       "221735      112\n",
       "221736      11...  \n",
       "176266  17807      112\n",
       "17808      112\n",
       "17809      112\n",
       "1...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training, valid, test = rand_split(data)\n",
    "display(training[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPDataSet():\n",
    "    def __init__(self, activities, bs=64, bptt=70, shuffle=False):\n",
    "        self.bs, self.bptt, self.shuffle = bs,bptt,shuffle\n",
    "        total_len = sum([len(t) for t in activities])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.a_batched = self.batchify(activities)\n",
    "        #self.u_batched = self.batchify(users)\n",
    "\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        a_source = self.a_batched[idx % self.bs]\n",
    "        #u_source = self.u_batched[idx % self.bs]\n",
    "\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return ((a_source[seq_idx:seq_idx+self.bptt]),\n",
    "                (a_source[seq_idx+1:seq_idx+self.bptt+1]))\n",
    "    \n",
    "        #return ((a_source[seq_idx:seq_idx+self.bptt],u_source[seq_idx:seq_idx+self.bptt]),\n",
    "         #        (a_source[seq_idx+1:seq_idx+self.bptt+1],u_source[seq_idx+1:seq_idx+self.bptt+1]))\n",
    "    \n",
    "    def batchify(self,d):\n",
    "        texts = d\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([torch.cat((tensor([2]), tensor(t), tensor([3]))) for t in texts])\n",
    "        return stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-7ef4e1bff884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mactivities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Activities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Users'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activities, bs, bptt, shuffle)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#self.u_batched = self.batchify(users)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mtensor\u001b[0;34m(x, *rest)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# XXX: Pytorch bug in dataloader using num_workers>0; TODO: create repro and report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor is int32: upgrading to int64; for better performance use int64 input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "bs = 128\n",
    "activities = data['Activities'].values\n",
    "users = data['Users'].values\n",
    "dl = DataLoader(PPDataSet(activities, shuffle=False), batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "xb, yb = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sum(len(a) for a in activities)\n",
    "k / len(activities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Batch\n",
    "# xb: Input Data, yb: labels (-> Window moved by one)\n",
    "# xb[0], yb[0]: Activities\n",
    "# xb[1], yb[1]: Users\n",
    "# xb[0][0], yb[0][0]: First Window of Activities\n",
    "# xb[1][0], yb[1][0]: First Window of Users\n",
    "\n",
    "xb[0][0], yb[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'xxrep', 'xxwrep', 'xxup', 'xxmaj', 'A_SUBMITTED', 'A_PARTLYSUBMITTED', 'A_PREACCEPTED', 'W_Completeren aanvraag', 'A_ACCEPTED', 'O_SELECTED', 'A_FINALIZED', 'O_CREATED', 'O_SENT', 'W_Nabellen offertes', 'O_SENT_BACK', 'W_Valideren aanvraag', 'A_REGISTERED', 'A_APPROVED', 'O_ACCEPTED', 'A_ACTIVATED', 'O_CANCELLED', 'W_Wijzigen contractgegevens', 'A_DECLINED', 'A_CANCELLED', 'W_Afhandelen leads', 'O_DECLINED', 'W_Nabellen incomplete dossiers', 'W_Beoordelen fraude'] 32\n"
     ]
    }
   ],
   "source": [
    "tokens = activity_numericalize.vocab\n",
    "print(tokens, len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Jakob\"\n",
    "\n",
    "chars = tuple(set(text))\n",
    "\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char = \"a\"\n",
    "x = np.array([[char2int[char]]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, activity, h=None, cuda=False, top_k=None):\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "\n",
    "    if h is None:\n",
    "        h = model.init_hidden(1)\n",
    "\n",
    "    activity = np.array([activity])\n",
    "    z = one_hot_encode(activity, len(model.vocab))\n",
    "    inputs = torch.from_numpy(z)\n",
    "\n",
    "    if cuda:\n",
    "        inputs = inputs.cuda()\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = model.forward(inputs, h)\n",
    "\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    #print(\"Softmax: \"+ str(p))\n",
    "\n",
    "    if cuda:\n",
    "        p = p.cpu()\n",
    "\n",
    "    if top_k is None:\n",
    "        top_activities = np.arange(len(model.vocab))\n",
    "    else:\n",
    "        p, top_activites = p.topk(top_k)\n",
    "        top_activities = top_activities.numpy().squeeze()\n",
    "\n",
    "    p = p.numpy().squeeze()\n",
    "    #print(p)\n",
    "    prediction = np.random.choice(top_activities, p=p/p.sum())\n",
    "\n",
    "    return prediction, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9f275160aa82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "network.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(network,[0])[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(['A_APPROVED']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating dictionaries to be able to go back and forth\n",
    "        self.vocab = tokens\n",
    "        self.int2word = dict(enumerate(self.vocab))\n",
    "        self.word2int = {w: ii for ii, w in self.int2word.items()}\n",
    "        \n",
    "        # Defining the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.vocab), n_hidden, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # Defining a dropout layer (to reduce the chance of overfitting)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # Defining the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.vocab))\n",
    "        \n",
    "        # Initializing the weights\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network.\n",
    "            The inputs are x and the hidden/cell state \"hc\". '''\n",
    "        \n",
    "        # Get x and the new hidden state and cell memory (h, c) from the lstm\n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        # Pass x through the dropout layer\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using \"view\" (reshaping the tensor for fc layer)\n",
    "        x = x.view(x.size()[0] * x.size()[1], self.n_hidden)\n",
    "        \n",
    "        # Pass x through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Return x and the hidden state (h, c)\n",
    "        return x, (h, c)\n",
    "    \n",
    " \n",
    "    \n",
    "#    def predict(self, activity, h=None, cuda=False, top_k=None):\n",
    "#        ''' Given one activity, predict the next activity.\n",
    "#            Returns the predicted word and the hidden state. '''\n",
    "#        \n",
    "#        if cuda:\n",
    "#            self.cuda()\n",
    "#        else:\n",
    "#            self.cpu()\n",
    "#        \n",
    "#        # initialize the hidden state\n",
    "#        if h is None:\n",
    "#            h = self.init_hidden(1)\n",
    "#        \n",
    "#        # creating our input by grabbing the integer of the given activity\n",
    "#        # and creating a numpy array out of that before one-hot-encoding it\n",
    "#        # finally convert it to a pytorch tensor\n",
    "#        x = np.array([[self.word2int[activity]]])\n",
    "#        x = one_hot_encode(x, len(self.vocab))\n",
    "#        inputs = torch.from_numpy(x)\n",
    "#        \n",
    "#        if cuda:\n",
    "#            inputs = inputs.cuda()\n",
    "#        \n",
    "#        # create the hidden state tuple\n",
    "#        h = tuple([each.data for each in h])\n",
    "#        out, h = self.forward(inputs, h)\n",
    "#\n",
    "#        # Softmax for a probability distribution\n",
    "#        p = F.softmax(out, dim=1).data\n",
    "#        \n",
    "#        if cuda:\n",
    "#            p = p.cpu()\n",
    "#        \n",
    "#        # Select the top_k probabilities or all if top_k = None\n",
    "#        if top_k is None:\n",
    "#            top_activities = np.arange(len(self.vocab))\n",
    "#        else:\n",
    "#            p, top_activities = p.topk(top_k)\n",
    "#            top_activities = top_activities.numpy().squeeze()\n",
    "#        \n",
    "#        p = p.numpy().squeeze()\n",
    "#        activity = np.random.choice(top_activities, p=p/p.sum())\n",
    "#            \n",
    "#        return self.int2word[activity], h\n",
    "        \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer with 0'''\n",
    "        init_range = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        \n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "    \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializing hidden state '''\n",
    "        \n",
    "        # Create two new tensors of size n_layers * n_seqs * n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, n_seqs, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, n_seqs, self.n_hidden).zero_())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train, valid, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, cuda=False, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: LSTM network\n",
    "        train: data to train the network\n",
    "        valid: data to validate the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of word steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping (to prevent exploding gradient problem)\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    '''\n",
    "    \n",
    "    # putting the network in training mode\n",
    "    net.train()\n",
    "    \n",
    "    # Loading Adam Optimizer and CrossEntropy as Loss-function\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    #val_idx = int(len(data)*(1-val_frac))\n",
    "    #data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_words = len(net.vocab)\n",
    "    \n",
    "    \n",
    "    train_activities = train['Activities'].values\n",
    "    valid_activities = valid['Activities'].values\n",
    "    #users = train['Users'].values\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        \n",
    "        for x, y in DataLoader(PPDataSet(train_activities), batch_size=64):\n",
    "            counter += 1\n",
    "            \n",
    "            # changing tensors to numpy so they can be one-hot-encoded\n",
    "            x, y = x.numpy(), y.numpy()\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_words)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # Zero the gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # Pass the input and the hidden state through forward \n",
    "            # and pass the output into the criterion function ( + reshaping the target values)\n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "\n",
    "                for x, y in DataLoader(PPDataSet(valid_activities), batch_size=64):\n",
    "                    # changing tensors to numpy so they can be one-hot-encoded\n",
    "                    x, y = x.numpy(), y.numpy()\n",
    "\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_words)\n",
    "                    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps).type(torch.LongTensor))\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training loop Safe from old Version of NB 11"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def learn(model, train_dl, valid_dl, vocab_size, epochs=10, bs=64, lr=0.005, clip=5, cuda=False):\n",
    "    ''' Training model on training data\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        model: Neural network\n",
    "        train_dl: dataloader for training data\n",
    "        valid_dl: dataloader for validation data\n",
    "        vocab_size: size of the vocab of the model\n",
    "        epochs: Number of epochs to train\n",
    "        bs: batch size\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping (to prevent exploding gradient problem)\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    \n",
    "    # putting the model in training mode\n",
    "    print(model)\n",
    "    model.train()\n",
    "    \n",
    "    # Loading Adam Optimizer and CrossEntropy as Loss-function\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs): \n",
    "        \n",
    "        for x, y in iter(train_dl):\n",
    "            x = x[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            #y = y[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            counter += 1\n",
    "            \n",
    "            model.zero_grad()\n",
    "            inputs = one_hot_encode(x)\n",
    "            targets = y[:, 3] # .type(torch.LongTensor)\n",
    "            #print(inputs, inputs.size())\n",
    "\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            out = model.forward(inputs)\n",
    "            loss = criterion(out, targets.view(bs).type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 2 == 0:\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in iter(valid_dl):\n",
    "                    v_inputs, v_targets = x, y\n",
    "                    \n",
    "                    if cuda:\n",
    "                        v_inputs, v_targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    v_out = model.forward(v_inputs)\n",
    "                    v_loss = criterion(v_out, v_targets.view(bs).type(torch.LongTensor))\n",
    "                    v_loss.backward()\n",
    "                    \n",
    "                    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    opt.step()\n",
    "                    \n",
    "                    val_losses.append(v_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "#    return (DataLoader(PPDataSet(train_ds, shuffle=True), batch_size=bs),\n",
    "#            DataLoader(PPDataSet(valid_ds, shuffle=False), batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data = DataBunch(*get_dls(train, valid, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#len(vocabs[\"concept:name\"].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and print the network\n",
    "vocab_size = len(vocabs[\"concept:name\"].vocab)\n",
    "\n",
    "network = LSTM(vocab_size, n_hidden=256, n_layers=6)\n",
    "learn(network, dl, dl2, vocab_size=vocab_size, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(model, train_dl, valid_dl, vocab_size, epochs=10, bs=64, lr=0.005, clip=5, cuda=False):\n",
    "    ''' Training model on training data\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        model: Neural network\n",
    "        train_dl: dataloader for training data\n",
    "        valid_dl: dataloader for validation data\n",
    "        vocab_size: size of the vocab of the model\n",
    "        epochs: Number of epochs to train\n",
    "        bs: batch size\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping (to prevent exploding gradient problem)\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    \n",
    "    # putting the model in training mode\n",
    "    print(model)\n",
    "    model.train()\n",
    "    \n",
    "    # Loading Adam Optimizer and CrossEntropy as Loss-function\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs): \n",
    "        \n",
    "        for x, y in iter(train_dl):\n",
    "            x = x[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            #y = y[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            counter += 1\n",
    "            \n",
    "            model.zero_grad()\n",
    "            inputs = one_hot_encode(x)\n",
    "            targets = y[:, 3] # .type(torch.LongTensor)\n",
    "            #print(inputs, inputs.size())\n",
    "\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            out = model.forward(inputs)\n",
    "            loss = criterion(out, targets.view(bs).type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 2 == 0:\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in iter(valid_dl):\n",
    "                    v_inputs, v_targets = x, y\n",
    "                    \n",
    "                    if cuda:\n",
    "                        v_inputs, v_targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    v_out = model.forward(v_inputs)\n",
    "                    v_loss = criterion(v_out, v_targets.view(bs).type(torch.LongTensor))\n",
    "                    v_loss.backward()\n",
    "                    \n",
    "                    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    opt.step()\n",
    "                    \n",
    "                    val_losses.append(v_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "\n",
    "#def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "#    return (DataLoader(PPDataSet(train_ds, shuffle=True), batch_size=bs),\n",
    "#            DataLoader(PPDataSet(valid_ds, shuffle=False), batch_size=bs))\n",
    "\n",
    "#data = DataBunch(*get_dls(train, valid, 64))\n",
    "\n",
    "#len(vocabs[\"concept:name\"].vocab)\n",
    "\n",
    "# Initialize and print the network\n",
    "vocab_size = len(vocabs[\"concept:name\"].vocab)\n",
    "\n",
    "network = LSTM(vocab_size, n_hidden=256, n_layers=6)\n",
    "learn(network, dl, dl2, vocab_size=vocab_size, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(32, 256, num_layers=6, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=32, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not determine the shape of object type 'Series'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-01bfc0ef6fad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-824ce1e6e42c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train, valid, epochs, n_seqs, n_steps, lr, clip, cuda, print_every)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPPDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_activities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, activities, bs, bptt, shuffle)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtotal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_batched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m#self.u_batched = self.batchify(users)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36mbatchify\u001b[0;34m(self, d)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-19c30972d880>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36mtensor\u001b[0;34m(x, *rest)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# XXX: Pytorch bug in dataloader using num_workers>0; TODO: create repro and report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor is int32: upgrading to int64; for better performance use int64 input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not determine the shape of object type 'Series'"
     ]
    }
   ],
   "source": [
    "# Initialize and print the network\n",
    "network = LSTM(tokens, n_hidden=256, n_layers=6)\n",
    "print(network)\n",
    "# set n_sequences and n_steps (n_sequences * n_steps = batch_size --> n_batches = data // batch_size)\n",
    "n_seqs = 64\n",
    "n_steps = 70\n",
    "\n",
    "train(network, training, valid, epochs=1, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Monitoring Validation Loss vs. Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "- If your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "- If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "\n",
    "### Approximate number of parameters\n",
    "\n",
    "The two most important parameters that control the model are n_hidden and n_layers. I would advise that you always use n_layers of either 2/3. The n_hidden can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "- The number of parameters in your model. This is printed when you start training.\n",
    "- The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "- I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make n_hidden larger.\n",
    "- I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "\n",
    "### Best models strategy\n",
    "\n",
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Samples of Traces by using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        \n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ' '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(network, 40, prime=[\"A_SUBMITTED\", \"A_PARTLYSUBMITTED\", \"A_PREACCEPTED\"], cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Test Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_suffix_prediction(test, col='Activities', startIndex=1):\n",
    "    \n",
    "    x, y = [], []\n",
    "    for trace in test[col]:\n",
    "        for i in range(startIndex, len(trace)):\n",
    "            x.append(trace[:i])\n",
    "            y.append(trace[i:])\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# x, y = process_data_for_suffix_prediction(test)\n",
    "# list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_next_step_prediction(test, col='Activities', startIndex=1):\n",
    "    \n",
    "    x, y = [], []\n",
    "    for trace in test[col]:\n",
    "        for i in range(startIndex, len(trace)):\n",
    "            x.append(trace[:i])\n",
    "            y.append(trace[i])\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = process_data_for_next_step_prediction(test)\n",
    "list(zip(x,y))\n",
    "\n",
    "print(len(x) == len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_measure(preds, ys):\n",
    "    \n",
    "    summe = 0.0\n",
    "    for p, y in zip(preds, ys):\n",
    "        l = len(p)\n",
    "        d = ed.eval(p, y)\n",
    "        sim = 1 - d/l\n",
    "        summe += sim\n",
    "    \n",
    "    return summe/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_measure(preds, ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    \n",
    "    return (np.array(preds) == np.array(ys)).astype(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_step(net, test, top_k=None, cuda=False):\n",
    "    \n",
    "    x, y = process_data_for_next_step_prediction(test, col='Activities', startIndex=1)\n",
    "\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    predictions = []\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    for value in x:\n",
    "        activities = []\n",
    "        \n",
    "        for i in range(len(value)):\n",
    "            activity, h = net.predict([value[i]], h, cuda=cuda, top_k=top_k)\n",
    "            \n",
    "        activities.append(activity)\n",
    "        activity, h = net.predict([activities[-1]], h, cuda=cuda, top_k=top_k)\n",
    "        predictions.append(activity)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = predict_next_step(network, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_next_step_prediction(test)\n",
    "\n",
    "next_step_measure(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_suffix(net, test, top_k=None, cuda=False):\n",
    "    \n",
    "    x, y = process_data_for_suffix_prediction(test, col='Activities', startIndex=1)\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    h = net.init_hidden(1)\n",
    "    predictions = []\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Get the size of the trace to be predicted\n",
    "        size = len(y[i])\n",
    "        value = x[i]\n",
    "        activities = []\n",
    "            \n",
    "        for j in range(len(value)):\n",
    "            activity, h = net.predict([value[j]], h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "        activities.append(activity)\n",
    "\n",
    "        for k in range(size-1):\n",
    "            activity, h = net.predict([activities[-1]], h, cuda=cuda, top_k=top_k)\n",
    "            activities.append(activity)\n",
    "\n",
    "        print(activities)\n",
    "\n",
    "        predictions.append(activities)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "preds2 = predict_suffix(network, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test, col='Activities', startIndex=1)\n",
    "\n",
    "suffix_measure(preds2, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, data, measurement):\n",
    "    predictions = []\n",
    "    x, y = process_data_for_suffix_prediction(data)\n",
    "    \n",
    "    h = model.init_hidden(1)\n",
    "    \n",
    "    for i in range(0, len(x)):\n",
    "        print(i)\n",
    "        for j in len(0, len(x[i])):\n",
    "            pred, h = model.predict(x[i], h)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "    return suffix_measure(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[8], [8, 9], [8, 9, 10]]\n",
    "\n",
    "for i in range(0, len(x)):\n",
    "    z = np.array([x[i]])\n",
    "    print(z)\n",
    "    \n",
    "    one_hot = np.zeros((np.multiply(*z.shape), 24), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), z.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*z.shape, 24))\n",
    "    print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(network, test, measurement=\"suffix_measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test)\n",
    "model = network\n",
    "preds = model.predict(x)\n",
    "suffix_measure(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_measure(preds, ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    return (np.array(preds) == np.array(ys)).astype(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test)\n",
    "model = PerfectModel(y)\n",
    "preds = model.predict(x)\n",
    "next_step_measure(preds,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a FastAi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextList.label_for_lm??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_df(df=df, path=path, cols='concept:name')\n",
    "                .split_by_idx(valid_idx=[10000,13086])\n",
    "                .label_for_lm()\n",
    "                .databunch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All das preprocessing wird durch die obige Funktion im Hintergrund durchgeführt. FastAi kodiert den Text dabei auf folgende Art und Weise:**\n",
    "\n",
    "- xxbos: Begining of a sentence\n",
    "- xxfld: Represent separate parts of a document like title, summary etc. (separate field for each one)\n",
    "- xxup:  If there's something in all caps, it gets lower cased and a token called xxup will get added to it.\n",
    "- xxunk: token used instead of an uncommon word.\n",
    "- xxmaj: token indicates that there is capitalization of the word. “The” will be tokenized as “xxmaj the“.\n",
    "- xxrep: token indicates repeated word, if you have 29 ! in a row, (i.e. xxrep 29 !)\n",
    "\n",
    "The list of unique possible tokens is called the vocabulary. Below the first 20 unique tokens are printed in order of there frequency. The minimal frequency for a word to be added to the vocabulary is 2, to avoid getting a huge weight matrix (The default vocab size is set to 60000). Since it is easier for machines to deal with numerical values (numbers) the tokens will be replaced by their position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first 20 unique tokens ordered by frequency: \\n\\n' + str(data.vocab.itos[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.train_ds[0][0], data.train_ds[0][0].data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model_learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for pretained model: pretrained_model=...\n",
    "# to avoid underfitting you can specify the amount of dropout (some kind of regularization) with drop_mult=...\n",
    "# arch is a Callable (?) it can be one of the following: \n",
    "# - AWD_LSTM (Merity et al.)\n",
    "# - Transformer decoder (Vaswani et al.) \n",
    "# - TransformerXL (Dai et al.)\n",
    "\n",
    "learn = language_model_learner(data=data, arch=AWD_LSTM, model_dir=\"/home/lahann/.fastpm/dl_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastAi provides a function called **lr_find()** to search through a range of learning rates to find the optimum one for the underlying dataset. Learning rate finder will increase the learning rate after each mini-batch. Eventually, the learning rate is too high that loss will get worse. Now look at the plot (**recorder.plot()**) of learning rate against loss and determine the lowest point (around 1e+00 for the plot below) and go back by one magnitude and choose that as a learning rate (something around 1e-01)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit one cycle:**\n",
    "\n",
    "What is fit_one_cycle()? It is one cycle of learning rate, start low, go up, and then go down again.\n",
    "When we call fit_one_cycle, we are actually passing in a maximum learning rate. The learning starts slow and it increases about half the time and then it decreases about half the time. As you get close to the final answer you need to anneal your learning rate to hone in on it. The motivation behind this is that during the middle of learning when learning rate is higher, the learning rate works as regularization method and keeps the network from over-fitting. This helps the network to avoid steep areas of loss and land better flatter minima. Please refer to this paper (https://arxiv.org/pdf/1803.09820.pdf) by Leslie smith which talks in great detail about neural network hyper-parameter tuning and you can find most of these ideas implemented in fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(10, 1e+00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, 1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit_one_cycle(10, 1e-02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list(learn.model.parameters())  # for getting the weights of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.get_preds??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, y, losses= learn.get_preds(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret = ClassificationInterpretation(learn, preds, y, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation = interpret.y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret.pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationset = interpret.data.valid_ds\n",
    "validationset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(np_arr):\n",
    "    shuffle = np_arr[np.random.permutation(len(np_arr))]\n",
    "    start = int(len(np_arr)*0.8)\n",
    "    end = int(len(np_arr)*0.9)\n",
    "    train, test, valid = shuffle[:start], shuffle[start:end], shuffle[end:]\n",
    "    return train, test, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, valid = random_split(df.values)\n",
    "display(valid[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_next_step_prediction(test,model,start_sequenze=1):\n",
    "    sum=0\n",
    "    acc=0\n",
    "    for t in test:\n",
    "        for a in range(start_sequenze,len(t)-1):\n",
    "            pred=model.predict(t[:a])\n",
    "            print(pred)\n",
    "            if pred==t[a]: acc+=1\n",
    "            sum+=1\n",
    "    return acc/sum\n",
    "\n",
    "eval_next_step_prediction(validation, learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dep_var = 'salary'\n",
    "# categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "# continuous = []\n",
    "# preprocesses = [FillMissing, Categorify, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = TabularList.from_df(df.iloc[800:1000].copy(), path=path, cat_names=categorical, cont_names=continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (TabularList.from_df(df, path=path, cat_names=categorical, cont_names=continuous, procs=preprocesses)\n",
    "#        .split_by_idx(list(range(800, 1000)))\n",
    "#        .label_from_df(cols=dep_var)\n",
    "#        .add_test(test, label=0)\n",
    "#        .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.show_batch(rows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabular_learner??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn = tabular_learner(data, layers=[20, 10], metrics=accuracy)\n",
    "# learn.fit_one_cycle(3, 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path2 = untar_data(URLs.BPIC_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next step prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Playing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp.vocabs[\"concept:name\"].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "padding_index = 1\n",
    "vocab_size = len(tp.vocabs[\"concept:name\"].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tst_model = get_language_model(vocab_size, 300, 300, 2, padding_index)\n",
    "tst_model = tst_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (emb): Embedding(32, 300, padding_idx=1)\n",
       "    (emb_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(32, 300, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (output_dp): RNNDropout()\n",
       "    (decoder): Linear(in_features=300, out_features=32, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0004, -0.0122, -0.0210,  ...,  0.0006,  0.0540,  0.0224],\n",
       "         [ 0.0022, -0.0157, -0.0268,  ..., -0.0063,  0.0502,  0.0171],\n",
       "         [ 0.0013, -0.0147, -0.0332,  ..., -0.0107,  0.0465,  0.0149],\n",
       "         ...,\n",
       "         [ 0.0097, -0.0136, -0.0041,  ...,  0.0548,  0.0716,  0.0674],\n",
       "         [ 0.0084, -0.0158, -0.0058,  ...,  0.0622,  0.0696,  0.0680],\n",
       "         [ 0.0048, -0.0181, -0.0062,  ...,  0.0636,  0.0680,  0.0653]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " [tensor([[[ 1.0967e-02,  2.2706e-02,  1.3375e-02,  ..., -1.2531e-02,\n",
       "             1.7527e-02, -1.9572e-02],\n",
       "           [ 1.1744e-02,  1.9288e-02,  5.2221e-03,  ..., -2.7696e-02,\n",
       "             2.3785e-02, -4.0682e-02],\n",
       "           [ 1.6345e-02,  2.6600e-02,  1.1789e-02,  ..., -2.0935e-02,\n",
       "             2.4581e-02, -3.1756e-02],\n",
       "           ...,\n",
       "           [ 3.0383e-03,  4.4811e-02,  2.6414e-02,  ..., -3.7662e-02,\n",
       "             2.0990e-02, -9.7780e-03],\n",
       "           [-3.5917e-03,  6.2961e-02,  9.6876e-03,  ..., -2.5391e-02,\n",
       "             1.9274e-02, -3.1356e-02],\n",
       "           [-1.4563e-02,  3.0421e-02,  1.1074e-02,  ..., -4.1005e-02,\n",
       "             2.2945e-02, -3.4986e-02]],\n",
       "  \n",
       "          [[ 2.9825e-03,  6.1830e-02,  3.5149e-02,  ..., -3.4414e-02,\n",
       "             5.6671e-02,  6.3736e-03],\n",
       "           [-5.1276e-03,  7.9205e-02,  2.8351e-02,  ..., -2.9411e-02,\n",
       "             6.0090e-02,  7.6445e-03],\n",
       "           [-8.9464e-03,  8.8217e-02,  2.4068e-02,  ..., -2.6221e-02,\n",
       "             5.4788e-02,  8.3620e-03],\n",
       "           ...,\n",
       "           [ 2.5243e-02, -1.6246e-04,  7.2309e-02,  ...,  2.3491e-02,\n",
       "             4.6298e-02, -1.8516e-02],\n",
       "           [ 2.5215e-02, -1.9075e-04,  7.2258e-02,  ...,  2.3497e-02,\n",
       "             4.6318e-02, -1.8502e-02],\n",
       "           [ 2.5194e-02, -2.0950e-04,  7.2226e-02,  ...,  2.3502e-02,\n",
       "             4.6336e-02, -1.8494e-02]],\n",
       "  \n",
       "          [[-7.1699e-03,  4.6605e-02,  3.9846e-02,  ..., -1.1812e-02,\n",
       "             2.5139e-02,  1.2866e-02],\n",
       "           [-2.0339e-03,  3.2207e-02,  3.7269e-02,  ..., -9.5100e-03,\n",
       "             2.3168e-02, -1.2781e-02],\n",
       "           [ 5.4531e-03,  2.5872e-02,  3.2826e-02,  ..., -9.5812e-03,\n",
       "             2.3590e-02, -2.7605e-02],\n",
       "           ...,\n",
       "           [-1.8586e-02,  4.9642e-02, -2.3107e-03,  ..., -2.7519e-02,\n",
       "             2.5737e-02, -2.7772e-02],\n",
       "           [-5.8789e-03,  3.0424e-02,  1.8397e-02,  ..., -2.0479e-02,\n",
       "             2.1381e-02, -3.6881e-02],\n",
       "           [-4.0121e-05,  3.3398e-02,  2.4582e-02,  ..., -5.3179e-03,\n",
       "             2.6654e-02, -2.4500e-03]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1.7696e-02,  4.7741e-02,  3.8868e-02,  ..., -3.6941e-02,\n",
       "             9.6468e-03, -6.0722e-02],\n",
       "           [-3.4091e-03,  3.3923e-02, -4.3098e-03,  ..., -2.7134e-02,\n",
       "             3.9805e-03, -2.6244e-02],\n",
       "           [ 3.7421e-03,  1.3225e-02, -4.0352e-03,  ..., -4.5236e-02,\n",
       "             1.2745e-03, -3.0887e-02],\n",
       "           ...,\n",
       "           [-9.5381e-03,  1.0026e-02,  2.9646e-02,  ..., -4.6405e-02,\n",
       "             1.1002e-02,  4.1454e-03],\n",
       "           [-1.5620e-02,  7.7342e-03,  3.1102e-02,  ..., -5.1210e-02,\n",
       "             1.3190e-02,  5.4730e-03],\n",
       "           [-2.2134e-02,  2.0113e-02,  4.2024e-02,  ..., -3.9109e-02,\n",
       "             4.8912e-02,  1.3356e-02]],\n",
       "  \n",
       "          [[ 1.4082e-02,  3.1020e-02,  8.3277e-03,  ..., -2.4144e-02,\n",
       "             1.1879e-02, -4.2277e-03],\n",
       "           [ 2.1032e-02,  3.0030e-02,  2.6127e-03,  ..., -3.7416e-02,\n",
       "             2.5401e-02, -2.1358e-03],\n",
       "           [ 2.5667e-02,  3.0227e-02, -9.0479e-04,  ..., -4.1368e-02,\n",
       "             3.4178e-02, -3.0580e-03],\n",
       "           ...,\n",
       "           [-1.4323e-02,  2.1751e-02,  1.1397e-02,  ..., -4.6186e-02,\n",
       "             2.5181e-02, -6.7896e-03],\n",
       "           [-1.7434e-02,  3.0186e-02, -2.0696e-03,  ..., -4.6467e-02,\n",
       "             3.0225e-02,  1.1479e-02],\n",
       "           [ 1.5088e-02,  4.1822e-02,  6.2100e-03,  ..., -3.5306e-02,\n",
       "             2.9905e-02,  1.3963e-02]],\n",
       "  \n",
       "          [[-1.9005e-03,  3.4805e-02, -5.6746e-03,  ..., -7.1340e-03,\n",
       "             2.0611e-02, -1.8361e-03],\n",
       "           [-8.8634e-03,  3.4191e-02, -1.4224e-02,  ..., -1.0152e-02,\n",
       "             2.2324e-02,  4.4350e-03],\n",
       "           [-1.5005e-03,  2.6214e-02, -1.2552e-02,  ..., -2.0342e-02,\n",
       "             2.4095e-02,  1.3626e-03],\n",
       "           ...,\n",
       "           [-1.0142e-02,  3.4958e-02, -2.3508e-02,  ..., -2.1955e-02,\n",
       "             3.6304e-02, -2.2752e-02],\n",
       "           [ 4.7869e-03,  3.1838e-02, -2.3706e-02,  ..., -2.3039e-02,\n",
       "             5.2708e-03, -1.9792e-02],\n",
       "           [ 1.1184e-02,  1.3681e-02, -2.8937e-02,  ..., -2.8653e-02,\n",
       "            -6.3814e-03, -3.6989e-02]]], grad_fn=<TransposeBackward0>),\n",
       "  tensor([[[-3.9716e-05, -4.2099e-02, -3.1607e-02,  ..., -9.2493e-03,\n",
       "             8.7051e-03, -2.0867e-02],\n",
       "           [ 8.0346e-03, -4.4953e-02, -2.7031e-02,  ..., -9.1725e-03,\n",
       "             1.0214e-02, -1.9553e-02],\n",
       "           [ 1.1557e-02, -4.5073e-02, -2.2833e-02,  ..., -7.1720e-03,\n",
       "             1.1531e-02, -1.8165e-02],\n",
       "           ...,\n",
       "           [ 1.2542e-02, -4.6686e-02, -7.3718e-03,  ..., -7.5706e-03,\n",
       "             1.7973e-02, -8.5714e-03],\n",
       "           [ 1.1371e-02, -4.1258e-02, -6.5006e-03,  ..., -8.0007e-03,\n",
       "             2.1555e-02, -1.0085e-02],\n",
       "           [ 1.4848e-02, -3.9722e-02, -7.2290e-03,  ..., -9.8525e-03,\n",
       "             1.9530e-02, -1.3725e-02]],\n",
       "  \n",
       "          [[-1.5844e-02, -4.5954e-02, -1.1073e-02,  ..., -1.5943e-02,\n",
       "            -7.7878e-04, -2.5549e-02],\n",
       "           [-6.0238e-03, -4.3778e-02, -7.6648e-03,  ..., -1.7802e-02,\n",
       "             2.9169e-03, -2.4221e-02],\n",
       "           [ 8.4674e-04, -4.3422e-02, -5.3874e-03,  ..., -1.9509e-02,\n",
       "             7.5796e-03, -2.2734e-02],\n",
       "           ...,\n",
       "           [ 2.6875e-02, -5.5143e-02, -2.6840e-02,  ..., -3.8374e-02,\n",
       "             1.1674e-02, -1.8408e-02],\n",
       "           [ 2.6841e-02, -5.5142e-02, -2.6874e-02,  ..., -3.8378e-02,\n",
       "             1.1615e-02, -1.8495e-02],\n",
       "           [ 2.6816e-02, -5.5144e-02, -2.6899e-02,  ..., -3.8378e-02,\n",
       "             1.1567e-02, -1.8563e-02]],\n",
       "  \n",
       "          [[-1.3350e-03, -4.0156e-02, -2.3178e-02,  ..., -1.4489e-02,\n",
       "             1.0018e-02, -1.7276e-02],\n",
       "           [-2.9015e-04, -3.8999e-02, -1.4895e-02,  ..., -1.8139e-02,\n",
       "             9.9690e-03, -1.8625e-02],\n",
       "           [-9.0592e-04, -3.8710e-02, -9.3483e-03,  ..., -2.2078e-02,\n",
       "             1.2306e-02, -2.0996e-02],\n",
       "           ...,\n",
       "           [ 1.1725e-03, -4.0581e-02, -1.8921e-02,  ..., -2.0475e-02,\n",
       "             1.5989e-02, -2.5578e-02],\n",
       "           [ 1.9735e-03, -3.8741e-02, -1.7766e-02,  ..., -2.1452e-02,\n",
       "             1.7879e-02, -2.6215e-02],\n",
       "           [ 3.1345e-03, -3.8111e-02, -1.2858e-02,  ..., -2.4202e-02,\n",
       "             1.7961e-02, -2.3424e-02]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1.0772e-02, -3.4327e-02, -1.8694e-02,  ..., -7.2611e-03,\n",
       "             7.1233e-03, -1.7656e-02],\n",
       "           [-5.2261e-03, -3.3569e-02, -1.8267e-02,  ..., -7.5836e-03,\n",
       "             1.0664e-02, -2.1476e-02],\n",
       "           [ 6.7810e-03, -3.8440e-02, -1.8663e-02,  ..., -7.0799e-03,\n",
       "             1.6991e-02, -2.7428e-02],\n",
       "           ...,\n",
       "           [ 1.9745e-02, -4.5680e-02,  5.3170e-03,  ..., -5.2976e-03,\n",
       "             2.9734e-03, -2.0594e-02],\n",
       "           [ 1.9117e-02, -4.5863e-02,  1.1021e-02,  ..., -5.9443e-03,\n",
       "            -3.0943e-04, -1.9634e-02],\n",
       "           [ 1.6547e-02, -4.5282e-02,  1.1225e-02,  ..., -7.0546e-03,\n",
       "             1.5673e-03, -1.7653e-02]],\n",
       "  \n",
       "          [[ 1.3138e-03, -4.7890e-02, -2.0064e-02,  ..., -1.1823e-02,\n",
       "             5.8210e-03, -8.6810e-03],\n",
       "           [ 7.8426e-03, -4.6681e-02, -1.6104e-02,  ..., -1.6019e-02,\n",
       "             4.8465e-03, -1.4448e-02],\n",
       "           [ 1.1260e-02, -4.3899e-02, -1.4771e-02,  ..., -1.8982e-02,\n",
       "             6.9493e-03, -2.1939e-02],\n",
       "           ...,\n",
       "           [ 1.0293e-02, -5.0551e-02, -3.0394e-04,  ..., -9.3883e-03,\n",
       "             1.2294e-02, -3.4549e-02],\n",
       "           [ 8.2830e-03, -4.8405e-02, -4.7179e-03,  ..., -1.4563e-02,\n",
       "             1.1040e-02, -2.9710e-02],\n",
       "           [ 1.3176e-02, -4.6805e-02, -5.0074e-03,  ..., -2.3456e-02,\n",
       "             5.4181e-03, -2.5759e-02]],\n",
       "  \n",
       "          [[ 5.5042e-04, -4.5757e-02, -2.8919e-02,  ..., -5.7441e-03,\n",
       "             1.7599e-02, -2.7573e-02],\n",
       "           [ 6.3279e-03, -4.5479e-02, -2.5934e-02,  ..., -6.0032e-03,\n",
       "             1.4735e-02, -2.2266e-02],\n",
       "           [ 1.0452e-02, -4.8301e-02, -1.9778e-02,  ..., -3.2193e-03,\n",
       "             1.3297e-02, -1.8183e-02],\n",
       "           ...,\n",
       "           [ 1.9968e-02, -4.5574e-02, -2.4170e-02,  ..., -6.7042e-03,\n",
       "             1.8414e-02, -1.0773e-02],\n",
       "           [ 2.1214e-02, -3.7736e-02, -2.4239e-02,  ..., -2.6828e-03,\n",
       "             2.0302e-02, -1.1421e-02],\n",
       "           [ 2.4618e-02, -3.3457e-02, -2.4033e-02,  ..., -2.6025e-03,\n",
       "             1.8835e-02, -1.2758e-02]]], grad_fn=<TransposeBackward0>)],\n",
       " [tensor([[[ 1.3708e-02,  2.8383e-02,  1.6718e-02,  ..., -0.0000e+00,\n",
       "             2.1908e-02, -2.4465e-02],\n",
       "           [ 1.4681e-02,  2.4111e-02,  6.5276e-03,  ..., -0.0000e+00,\n",
       "             2.9731e-02, -5.0853e-02],\n",
       "           [ 2.0431e-02,  3.3250e-02,  1.4736e-02,  ..., -0.0000e+00,\n",
       "             3.0726e-02, -3.9695e-02],\n",
       "           ...,\n",
       "           [ 3.7979e-03,  5.6013e-02,  3.3018e-02,  ..., -0.0000e+00,\n",
       "             2.6237e-02, -1.2223e-02],\n",
       "           [-4.4896e-03,  7.8701e-02,  1.2109e-02,  ..., -0.0000e+00,\n",
       "             2.4092e-02, -3.9194e-02],\n",
       "           [-1.8204e-02,  3.8026e-02,  1.3843e-02,  ..., -0.0000e+00,\n",
       "             2.8682e-02, -4.3733e-02]],\n",
       "  \n",
       "          [[ 3.7281e-03,  7.7287e-02,  4.3937e-02,  ..., -4.3017e-02,\n",
       "             7.0839e-02,  7.9669e-03],\n",
       "           [-6.4095e-03,  9.9006e-02,  3.5438e-02,  ..., -3.6763e-02,\n",
       "             7.5112e-02,  9.5556e-03],\n",
       "           [-1.1183e-02,  1.1027e-01,  3.0085e-02,  ..., -3.2776e-02,\n",
       "             6.8485e-02,  1.0453e-02],\n",
       "           ...,\n",
       "           [ 3.1554e-02, -2.0308e-04,  9.0386e-02,  ...,  2.9364e-02,\n",
       "             5.7872e-02, -2.3145e-02],\n",
       "           [ 3.1518e-02, -2.3844e-04,  9.0322e-02,  ...,  2.9371e-02,\n",
       "             5.7898e-02, -2.3127e-02],\n",
       "           [ 3.1492e-02, -2.6187e-04,  9.0283e-02,  ...,  2.9377e-02,\n",
       "             5.7919e-02, -2.3117e-02]],\n",
       "  \n",
       "          [[-8.9623e-03,  0.0000e+00,  0.0000e+00,  ..., -1.4766e-02,\n",
       "             0.0000e+00,  1.6082e-02],\n",
       "           [-2.5424e-03,  0.0000e+00,  0.0000e+00,  ..., -1.1887e-02,\n",
       "             0.0000e+00, -1.5976e-02],\n",
       "           [ 6.8164e-03,  0.0000e+00,  0.0000e+00,  ..., -1.1976e-02,\n",
       "             0.0000e+00, -3.4506e-02],\n",
       "           ...,\n",
       "           [-2.3232e-02,  0.0000e+00, -0.0000e+00,  ..., -3.4398e-02,\n",
       "             0.0000e+00, -3.4715e-02],\n",
       "           [-7.3487e-03,  0.0000e+00,  0.0000e+00,  ..., -2.5599e-02,\n",
       "             0.0000e+00, -4.6101e-02],\n",
       "           [-5.0151e-05,  0.0000e+00,  0.0000e+00,  ..., -6.6474e-03,\n",
       "             0.0000e+00, -3.0625e-03]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-2.2120e-02,  5.9676e-02,  4.8585e-02,  ..., -0.0000e+00,\n",
       "             1.2059e-02, -7.5903e-02],\n",
       "           [-4.2613e-03,  4.2404e-02, -5.3872e-03,  ..., -0.0000e+00,\n",
       "             4.9757e-03, -3.2805e-02],\n",
       "           [ 4.6776e-03,  1.6531e-02, -5.0440e-03,  ..., -0.0000e+00,\n",
       "             1.5931e-03, -3.8609e-02],\n",
       "           ...,\n",
       "           [-1.1923e-02,  1.2532e-02,  3.7058e-02,  ..., -0.0000e+00,\n",
       "             1.3753e-02,  5.1818e-03],\n",
       "           [-1.9525e-02,  9.6677e-03,  3.8877e-02,  ..., -0.0000e+00,\n",
       "             1.6488e-02,  6.8412e-03],\n",
       "           [-2.7668e-02,  2.5141e-02,  5.2530e-02,  ..., -0.0000e+00,\n",
       "             6.1139e-02,  1.6695e-02]],\n",
       "  \n",
       "          [[ 1.7602e-02,  3.8774e-02,  1.0410e-02,  ..., -3.0180e-02,\n",
       "             1.4849e-02, -5.2846e-03],\n",
       "           [ 2.6290e-02,  3.7538e-02,  3.2658e-03,  ..., -4.6770e-02,\n",
       "             3.1751e-02, -2.6697e-03],\n",
       "           [ 3.2083e-02,  3.7784e-02, -1.1310e-03,  ..., -5.1710e-02,\n",
       "             4.2723e-02, -3.8225e-03],\n",
       "           ...,\n",
       "           [-1.7904e-02,  2.7189e-02,  1.4246e-02,  ..., -5.7732e-02,\n",
       "             3.1476e-02, -8.4870e-03],\n",
       "           [-2.1793e-02,  3.7733e-02, -2.5870e-03,  ..., -5.8084e-02,\n",
       "             3.7781e-02,  1.4349e-02],\n",
       "           [ 1.8860e-02,  5.2278e-02,  7.7625e-03,  ..., -4.4133e-02,\n",
       "             3.7381e-02,  1.7454e-02]],\n",
       "  \n",
       "          [[-2.3756e-03,  4.3506e-02, -0.0000e+00,  ..., -8.9175e-03,\n",
       "             2.5764e-02, -2.2951e-03],\n",
       "           [-1.1079e-02,  4.2739e-02, -0.0000e+00,  ..., -1.2690e-02,\n",
       "             2.7904e-02,  5.5438e-03],\n",
       "           [-1.8756e-03,  3.2767e-02, -0.0000e+00,  ..., -2.5427e-02,\n",
       "             3.0119e-02,  1.7032e-03],\n",
       "           ...,\n",
       "           [-1.2678e-02,  4.3698e-02, -0.0000e+00,  ..., -2.7444e-02,\n",
       "             4.5380e-02, -2.8440e-02],\n",
       "           [ 5.9836e-03,  3.9798e-02, -0.0000e+00,  ..., -2.8799e-02,\n",
       "             6.5885e-03, -2.4740e-02],\n",
       "           [ 1.3980e-02,  1.7102e-02, -0.0000e+00,  ..., -3.5817e-02,\n",
       "            -7.9767e-03, -4.6236e-02]]], grad_fn=<MulBackward0>),\n",
       "  tensor([[[-3.9716e-05, -4.2099e-02, -3.1607e-02,  ..., -9.2493e-03,\n",
       "             8.7051e-03, -2.0867e-02],\n",
       "           [ 8.0346e-03, -4.4953e-02, -2.7031e-02,  ..., -9.1725e-03,\n",
       "             1.0214e-02, -1.9553e-02],\n",
       "           [ 1.1557e-02, -4.5073e-02, -2.2833e-02,  ..., -7.1720e-03,\n",
       "             1.1531e-02, -1.8165e-02],\n",
       "           ...,\n",
       "           [ 1.2542e-02, -4.6686e-02, -7.3718e-03,  ..., -7.5706e-03,\n",
       "             1.7973e-02, -8.5714e-03],\n",
       "           [ 1.1371e-02, -4.1258e-02, -6.5006e-03,  ..., -8.0007e-03,\n",
       "             2.1555e-02, -1.0085e-02],\n",
       "           [ 1.4848e-02, -3.9722e-02, -7.2290e-03,  ..., -9.8525e-03,\n",
       "             1.9530e-02, -1.3725e-02]],\n",
       "  \n",
       "          [[-1.5844e-02, -4.5954e-02, -1.1073e-02,  ..., -1.5943e-02,\n",
       "            -7.7878e-04, -2.5549e-02],\n",
       "           [-6.0238e-03, -4.3778e-02, -7.6648e-03,  ..., -1.7802e-02,\n",
       "             2.9169e-03, -2.4221e-02],\n",
       "           [ 8.4674e-04, -4.3422e-02, -5.3874e-03,  ..., -1.9509e-02,\n",
       "             7.5796e-03, -2.2734e-02],\n",
       "           ...,\n",
       "           [ 2.6875e-02, -5.5143e-02, -2.6840e-02,  ..., -3.8374e-02,\n",
       "             1.1674e-02, -1.8408e-02],\n",
       "           [ 2.6841e-02, -5.5142e-02, -2.6874e-02,  ..., -3.8378e-02,\n",
       "             1.1615e-02, -1.8495e-02],\n",
       "           [ 2.6816e-02, -5.5144e-02, -2.6899e-02,  ..., -3.8378e-02,\n",
       "             1.1567e-02, -1.8563e-02]],\n",
       "  \n",
       "          [[-1.3350e-03, -4.0156e-02, -2.3178e-02,  ..., -1.4489e-02,\n",
       "             1.0018e-02, -1.7276e-02],\n",
       "           [-2.9015e-04, -3.8999e-02, -1.4895e-02,  ..., -1.8139e-02,\n",
       "             9.9690e-03, -1.8625e-02],\n",
       "           [-9.0592e-04, -3.8710e-02, -9.3483e-03,  ..., -2.2078e-02,\n",
       "             1.2306e-02, -2.0996e-02],\n",
       "           ...,\n",
       "           [ 1.1725e-03, -4.0581e-02, -1.8921e-02,  ..., -2.0475e-02,\n",
       "             1.5989e-02, -2.5578e-02],\n",
       "           [ 1.9735e-03, -3.8741e-02, -1.7766e-02,  ..., -2.1452e-02,\n",
       "             1.7879e-02, -2.6215e-02],\n",
       "           [ 3.1345e-03, -3.8111e-02, -1.2858e-02,  ..., -2.4202e-02,\n",
       "             1.7961e-02, -2.3424e-02]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-1.0772e-02, -3.4327e-02, -1.8694e-02,  ..., -7.2611e-03,\n",
       "             7.1233e-03, -1.7656e-02],\n",
       "           [-5.2261e-03, -3.3569e-02, -1.8267e-02,  ..., -7.5836e-03,\n",
       "             1.0664e-02, -2.1476e-02],\n",
       "           [ 6.7810e-03, -3.8440e-02, -1.8663e-02,  ..., -7.0799e-03,\n",
       "             1.6991e-02, -2.7428e-02],\n",
       "           ...,\n",
       "           [ 1.9745e-02, -4.5680e-02,  5.3170e-03,  ..., -5.2976e-03,\n",
       "             2.9734e-03, -2.0594e-02],\n",
       "           [ 1.9117e-02, -4.5863e-02,  1.1021e-02,  ..., -5.9443e-03,\n",
       "            -3.0943e-04, -1.9634e-02],\n",
       "           [ 1.6547e-02, -4.5282e-02,  1.1225e-02,  ..., -7.0546e-03,\n",
       "             1.5673e-03, -1.7653e-02]],\n",
       "  \n",
       "          [[ 1.3138e-03, -4.7890e-02, -2.0064e-02,  ..., -1.1823e-02,\n",
       "             5.8210e-03, -8.6810e-03],\n",
       "           [ 7.8426e-03, -4.6681e-02, -1.6104e-02,  ..., -1.6019e-02,\n",
       "             4.8465e-03, -1.4448e-02],\n",
       "           [ 1.1260e-02, -4.3899e-02, -1.4771e-02,  ..., -1.8982e-02,\n",
       "             6.9493e-03, -2.1939e-02],\n",
       "           ...,\n",
       "           [ 1.0293e-02, -5.0551e-02, -3.0394e-04,  ..., -9.3883e-03,\n",
       "             1.2294e-02, -3.4549e-02],\n",
       "           [ 8.2830e-03, -4.8405e-02, -4.7179e-03,  ..., -1.4563e-02,\n",
       "             1.1040e-02, -2.9710e-02],\n",
       "           [ 1.3176e-02, -4.6805e-02, -5.0074e-03,  ..., -2.3456e-02,\n",
       "             5.4181e-03, -2.5759e-02]],\n",
       "  \n",
       "          [[ 5.5042e-04, -4.5757e-02, -2.8919e-02,  ..., -5.7441e-03,\n",
       "             1.7599e-02, -2.7573e-02],\n",
       "           [ 6.3279e-03, -4.5479e-02, -2.5934e-02,  ..., -6.0032e-03,\n",
       "             1.4735e-02, -2.2266e-02],\n",
       "           [ 1.0452e-02, -4.8301e-02, -1.9778e-02,  ..., -3.2193e-03,\n",
       "             1.3297e-02, -1.8183e-02],\n",
       "           ...,\n",
       "           [ 1.9968e-02, -4.5574e-02, -2.4170e-02,  ..., -6.7042e-03,\n",
       "             1.8414e-02, -1.0773e-02],\n",
       "           [ 2.1214e-02, -3.7736e-02, -2.4239e-02,  ..., -2.6828e-03,\n",
       "             2.0302e-02, -1.1421e-02],\n",
       "           [ 2.4618e-02, -3.3457e-02, -2.4033e-02,  ..., -2.6025e-03,\n",
       "             1.8835e-02, -1.2758e-02]]], grad_fn=<TransposeBackward0>)])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tst_model(xb[:, 3].long())\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8960, 32]), 8960)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0].shape, 128*70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def accuracy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return accuracy(input.view(bs * sl, -1), target.contiguous().view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "decoded, raw_outputs, outputs = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 70]), torch.Size([8960, 32]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[:, 3].size(), decoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 70])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb[:, 3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0190)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_flat(decoded, yb[:, 3].long()) # funktioniert nur mit contiguous in accuracy und .long in der eingabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4703, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_flat(decoded,yb[:, 3].long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 70)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs,sl = yb[:, 3].size()\n",
    "bs,sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8960"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(yb[:, 3].contiguous().view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0004, -0.0122, -0.0210,  ...,  0.0006,  0.0540,  0.0224],\n",
       "        [ 0.0022, -0.0157, -0.0268,  ..., -0.0063,  0.0502,  0.0171],\n",
       "        [ 0.0013, -0.0147, -0.0332,  ..., -0.0107,  0.0465,  0.0149],\n",
       "        ...,\n",
       "        [ 0.0097, -0.0136, -0.0041,  ...,  0.0548,  0.0716,  0.0674],\n",
       "        [ 0.0084, -0.0158, -0.0058,  ...,  0.0622,  0.0696,  0.0680],\n",
       "        [ 0.0048, -0.0181, -0.0062,  ...,  0.0636,  0.0680,  0.0653]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#(torch.argmax(decoded, dim=1) == yb[:, 3].contiguous().view(bs * sl).cuda()).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8960, 32])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([128, 70, 300]), torch.Size([128, 70, 300])],\n",
       " [torch.Size([128, 70, 300]), torch.Size([128, 70, 300])])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in raw_outputs], [o.size() for o in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
