{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for Process Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "# %load_ext line_profiler\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.eventlog import *\n",
    "from exp.dl_utils import *\n",
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "from pyxdameraulevenshtein import damerau_levenshtein_distance, normalized_damerau_levenshtein_distance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_df(df, path):\n",
    "    # Prepend dtypes to the top of df\n",
    "    df.to_pickle(path+'.pickle')\n",
    "\n",
    "def load_df(path):\n",
    "    return pd.read_pickle(path+'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def store_df(df, path):\n",
    "    # Prepend dtypes to the top of df\n",
    "    df2 = df.copy()\n",
    "    df2.loc[-1] = df2.dtypes\n",
    "    df2.index = df2.index + 1\n",
    "    df2.sort_index(inplace=True)\n",
    "    # Then save it to a csv\n",
    "    df2.to_csv(path, index=False)\n",
    "\n",
    "def load_df(path):\n",
    "    # Read types first line of csv\n",
    "    dtypes = {key:value for (key,value) in pd.read_csv(path,    \n",
    "              nrows=1).iloc[0].to_dict().items() if 'date' not in value}\n",
    "\n",
    "    parse_dates = [key for (key,value) in pd.read_csv(path, \n",
    "                   nrows=1).iloc[0].to_dict().items() if 'date' in value]\n",
    "    # Read the rest of the lines with the types from above\n",
    "    return pd.read_csv(path, dtype=dtypes, parse_dates=parse_dates, skiprows=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mse(output, targ): \n",
    "    if not torch.is_tensor(output): output=tensor(output)\n",
    "    if not torch.is_tensor(targ): targ=tensor(targ)\n",
    "    return (output.squeeze(-1) - targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_unknown=0\n",
    "_padding=1\n",
    "_special_token=['__UNKNOWN__','__PADDING__']\n",
    "\n",
    "class LabelEncoder():\n",
    "    def __init__(self,special_token=_special_token,unknown=_unknown,padding=_padding): \n",
    "        self.unknown=unknown\n",
    "        self.special_token=special_token\n",
    "    def fit(self, x):\n",
    "        self.vocab=list(np.unique(x))\n",
    "        for o in reversed(self.special_token):\n",
    "            if o in self.vocab: self.vocab.remove(o)\n",
    "            self.vocab.insert(0, o)\n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = defaultdict(int,{v:k for k,v in enumerate(self.vocab)})\n",
    "    def transform(self,x): return [self.otoi.get(o, self.unknown) for o in x]\n",
    "    def inverse_transform(self,x): return [self.vocab[i] for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=['a', 'b','c']\n",
    "#test=['a','c','d']\n",
    "#le=LabelEncoder()\n",
    "#le.fit(train)\n",
    "#print(le.vocab)\n",
    "#print(le.otoi)\n",
    "#trans=le.transform(test)\n",
    "#print(trans)\n",
    "#print(le.inverse_transform(trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_split_traces(d,split=0.8,trace_id='trace_id'):\n",
    "    traces=d[trace_id].drop_duplicates()\n",
    "    shuffled=traces.iloc[np.random.permutation(len(traces))].values\n",
    "    split=int(len(traces)*split)\n",
    "    return shuffled[:split],shuffled[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def label_encode(x,label_encoders):\n",
    "    x[pd.isnull(x)]  = 'is missing' \n",
    "    if not x.name in label_encoders:\n",
    "        label_encoders[x.name]=LabelEncoder()\n",
    "        label_encoders[x.name].fit(x)\n",
    "    return label_encoders[x.name].transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def calc_suffix(groups,col):\n",
    "    suffix=[]\n",
    "    for n,g in groups:\n",
    "        c=g[col].values\n",
    "        for i in range(len(c)):\n",
    "            suffix.append(list(c[i:][1:]))\n",
    "    return suffix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp=pd.DataFrame({'c':[1,2,3,4,5,6]})\n",
    "#tmp.index=['a','b','a','b','a','b']\n",
    "#tmp['suffix_correct']=[[3,5],[4,6],[5],[6],[],[]]\n",
    "\n",
    "\n",
    "\n",
    "#tmp.sort_index(inplace=True)\n",
    "#g=tmp.groupby(tmp.index,sort=False)\n",
    "\n",
    "#tmp['suffix_calc']=calc_suffix(g,'c')\n",
    "#tmp['next activity']=g['c'].shift(-1,fill_value=-1).astype(int) \n",
    "#tmp['outcome']=g['c'].transform('last')\n",
    "#tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_trace_df(t,df,label_encoders,trace_id='trace_id'): \n",
    "    res=df[df[trace_id].isin(t)].copy()\n",
    "    g=res.groupby(trace_id,sort=False)\n",
    "    res[['activity','resource']]=res[['activity:name','org:resource']].apply(lambda x: label_encode(x,label_encoders))\n",
    "    res['relative timestamp']=g['time:timestamp'].diff().shift(-1) / np.timedelta64(1, 's')\n",
    "    res[\"relative timestamp\"].fillna(0, inplace=True) # fills every NaN at end of trace with 0\n",
    "    # the value at [-2] is the same as at [-3] due to the 0 of relative timestamp\n",
    "    res[\"next relative timestamp\"] = g[\"relative timestamp\"].transform(lambda x: x.cumsum().shift(-1))\n",
    "    res[\"next relative timestamp\"].fillna(0, inplace=True)\n",
    "    res['duration to end']=g['time:timestamp'].transform(lambda x: (x.iloc[-1]-x) / np.timedelta64(1, 's'))\n",
    "    res['next activity']=g['activity'].shift(-1,fill_value=-1).astype(int) \n",
    "    res['next resource']=g['resource'].shift(-1,fill_value=-1).astype(int) \n",
    "    res['last resource']=g['resource'].transform('last')\n",
    "    res['outcome']=g['activity'].transform('last')\n",
    "    res['activity suffix']=calc_suffix(g,'activity')\n",
    "    res['resource suffix']=calc_suffix(g,'resource')\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def preprocess_and_train_test_split(dataset,cache=False,override=False): \n",
    "    dataset_name=dataset.split('=')[-1].split('.')[0]\n",
    "    path=f\"./.cache/datasets/{dataset_name}\"\n",
    "    if cache and os.path.isdir(path) and not override:\n",
    "        train_df=load_df(f\"{path}/train\")\n",
    "        test_df=load_df(f\"{path}/test\")\n",
    "        return train_df, test_df\n",
    "    log=import_xes(untar_data(dataset))\n",
    "    df=pd.merge(log.events,log.traceAttributes,left_on='trace_id',right_index=True)\n",
    "    df['activity:name']=df['concept:name']+'_'+df['lifecycle:transition']\n",
    "    # missing values (NaN) in duration to next event einf√ºllen:\n",
    "    #df['duration to next event'].fillna(0, inplace=True) \n",
    "    label_encoders={}\n",
    "    train_trace_ids,test_trace_ids=random_split_traces(df,0.9)\n",
    "    train_df=get_trace_df(train_trace_ids,df,label_encoders)\n",
    "    test_df=get_trace_df(test_trace_ids,df,label_encoders)\n",
    "    if cache and (not os.path.isdir(path) or override):\n",
    "        if override:  shutil.rmtree(path)\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "        except OSError:\n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "        else:\n",
    "            print (\"Successfully created the directory %s\" % path)\n",
    "            store_df(train_df,f\"{path}/train\")\n",
    "            store_df(test_df,f\"{path}/test\")\n",
    "    return train_df,test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train_df,test_df=preprocess_and_train_test_split(URLs.BPIC_2012,cache=True,override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_PPP_Preprocessing.ipynb to exp/Preprocessing.py\r\n"
     ]
    }
   ],
   "source": [
    "! /home/lahann/anaconda3/envs/fastpm/bin/python notebook2script.py 000_PPP_Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
