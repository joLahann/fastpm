{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext line_profiler\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.Models import *\n",
    "from exp.Preprocessing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the directory ./.cache/datasets/BPI_Challenge_2012\n",
      "CPU times: user 39.4 s, sys: 759 ms, total: 40.1 s\n",
      "Wall time: 39.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>REG_DATE</th>\n",
       "      <th>AMOUNT_REQ</th>\n",
       "      <th>activity:name</th>\n",
       "      <th>activity</th>\n",
       "      <th>resource</th>\n",
       "      <th>relative timestamp</th>\n",
       "      <th>next relative timestamp</th>\n",
       "      <th>duration to end</th>\n",
       "      <th>next activity</th>\n",
       "      <th>next resource</th>\n",
       "      <th>last resource</th>\n",
       "      <th>outcome</th>\n",
       "      <th>activity suffix</th>\n",
       "      <th>resource suffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_SUBMITTED_COMPLETE</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>0.334</td>\n",
       "      <td>53.360</td>\n",
       "      <td>1072732.480</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>[8, 9, 26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 3...</td>\n",
       "      <td>[54, 54, 54, 70, 15, 15, 15, 15, 15, 70, 70, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PARTLYSUBMITTED_COMPLETE</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>53.026</td>\n",
       "      <td>54.329</td>\n",
       "      <td>1072732.146</td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>[9, 26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 31, ...</td>\n",
       "      <td>[54, 54, 70, 15, 15, 15, 15, 15, 70, 70, 70, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>A_PREACCEPTED_COMPLETE</td>\n",
       "      <td>9</td>\n",
       "      <td>54</td>\n",
       "      <td>0.969</td>\n",
       "      <td>39481.891</td>\n",
       "      <td>1072679.120</td>\n",
       "      <td>26</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>[26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 31, 33,...</td>\n",
       "      <td>[54, 70, 15, 15, 15, 15, 15, 70, 70, 70, 70, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-09-30 22:39:38.875000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag_SCHEDULE</td>\n",
       "      <td>26</td>\n",
       "      <td>54</td>\n",
       "      <td>39427.562</td>\n",
       "      <td>39838.762</td>\n",
       "      <td>1072678.151</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>[27, 2, 16, 7, 14, 18, 32, 25, 33, 31, 33, 31,...</td>\n",
       "      <td>[70, 15, 15, 15, 15, 15, 70, 70, 70, 70, 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-10-01 09:36:46.437000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "      <td>W_Completeren aanvraag_START</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>356.871</td>\n",
       "      <td>39984.697</td>\n",
       "      <td>1033250.589</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>[2, 16, 7, 14, 18, 32, 25, 33, 31, 33, 31, 33,...</td>\n",
       "      <td>[15, 15, 15, 15, 15, 70, 70, 70, 70, 24, 24, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trace_id  event_id org:resource lifecycle:transition  \\\n",
       "0   173688         0          112             COMPLETE   \n",
       "1   173688         1          112             COMPLETE   \n",
       "2   173688         2          112             COMPLETE   \n",
       "3   173688         3          112             SCHEDULE   \n",
       "4   173688         4         None                START   \n",
       "\n",
       "             concept:name                   time:timestamp  \\\n",
       "0             A_SUBMITTED 2011-09-30 22:38:44.546000+00:00   \n",
       "1       A_PARTLYSUBMITTED 2011-09-30 22:38:44.880000+00:00   \n",
       "2           A_PREACCEPTED 2011-09-30 22:39:37.906000+00:00   \n",
       "3  W_Completeren aanvraag 2011-09-30 22:39:38.875000+00:00   \n",
       "4  W_Completeren aanvraag 2011-10-01 09:36:46.437000+00:00   \n",
       "\n",
       "                           REG_DATE AMOUNT_REQ  \\\n",
       "0  2011-10-01 00:38:44.546000+02:00      20000   \n",
       "1  2011-10-01 00:38:44.546000+02:00      20000   \n",
       "2  2011-10-01 00:38:44.546000+02:00      20000   \n",
       "3  2011-10-01 00:38:44.546000+02:00      20000   \n",
       "4  2011-10-01 00:38:44.546000+02:00      20000   \n",
       "\n",
       "                     activity:name  activity  resource  relative timestamp  \\\n",
       "0             A_SUBMITTED_COMPLETE        11        54               0.334   \n",
       "1       A_PARTLYSUBMITTED_COMPLETE         8        54              53.026   \n",
       "2           A_PREACCEPTED_COMPLETE         9        54               0.969   \n",
       "3  W_Completeren aanvraag_SCHEDULE        26        54           39427.562   \n",
       "4     W_Completeren aanvraag_START        27        70             356.871   \n",
       "\n",
       "   next relative timestamp  duration to end  next activity  next resource  \\\n",
       "0                   53.360      1072732.480              8             54   \n",
       "1                   54.329      1072732.146              9             54   \n",
       "2                39481.891      1072679.120             26             54   \n",
       "3                39838.762      1072678.151             27             70   \n",
       "4                39984.697      1033250.589              2             15   \n",
       "\n",
       "   last resource  outcome                                    activity suffix  \\\n",
       "0              8       34  [8, 9, 26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 3...   \n",
       "1              8       34  [9, 26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 31, ...   \n",
       "2              8       34  [26, 27, 2, 16, 7, 14, 18, 32, 25, 33, 31, 33,...   \n",
       "3              8       34  [27, 2, 16, 7, 14, 18, 32, 25, 33, 31, 33, 31,...   \n",
       "4              8       34  [2, 16, 7, 14, 18, 32, 25, 33, 31, 33, 31, 33,...   \n",
       "\n",
       "                                     resource suffix  \n",
       "0  [54, 54, 54, 70, 15, 15, 15, 15, 15, 70, 70, 7...  \n",
       "1  [54, 54, 70, 15, 15, 15, 15, 15, 70, 70, 70, 7...  \n",
       "2  [54, 70, 15, 15, 15, 15, 15, 70, 70, 70, 70, 2...  \n",
       "3  [70, 15, 15, 15, 15, 15, 70, 70, 70, 70, 24, 2...  \n",
       "4  [15, 15, 15, 15, 15, 70, 70, 70, 70, 24, 24, 4...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df, test_df = preprocess_and_train_test_split(URLs.BPIC_2012,cache=True,override=True)\n",
    "train_df.head(5)\n",
    "\n",
    "# duration to next event zu relative timestamp, plus spalte next relative timestamp hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative timestamp</th>\n",
       "      <th>next relative timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.294</td>\n",
       "      <td>41.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>0.177</td>\n",
       "      <td>42.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>0.174</td>\n",
       "      <td>43.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217</th>\n",
       "      <td>0.323</td>\n",
       "      <td>49.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>0.168</td>\n",
       "      <td>40.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261828</th>\n",
       "      <td>0.721</td>\n",
       "      <td>37.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261909</th>\n",
       "      <td>0.432</td>\n",
       "      <td>16.884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261936</th>\n",
       "      <td>0.222</td>\n",
       "      <td>43.717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261954</th>\n",
       "      <td>0.254</td>\n",
       "      <td>43.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262022</th>\n",
       "      <td>0.219</td>\n",
       "      <td>35.670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1309 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        relative timestamp  next relative timestamp\n",
       "203                  0.294                   41.604\n",
       "949                  0.177                   42.554\n",
       "1155                 0.174                   43.258\n",
       "1217                 0.323                   49.811\n",
       "1342                 0.168                   40.677\n",
       "...                    ...                      ...\n",
       "261828               0.721                   37.120\n",
       "261909               0.432                   16.884\n",
       "261936               0.222                   43.717\n",
       "261954               0.254                   43.800\n",
       "262022               0.219                   35.670\n",
       "\n",
       "[1309 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[\"relative timestamp\", \"next relative timestamp\"]][test_df[\"activity\"] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#g = test_df.groupby(\"trace_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#def predict_results(g, index, clf, col):\n",
    "#    tmp = g.apply(lambda x: x[col].values[index:-1])\n",
    "#    tmp = tmp[tmp.apply(lambda x: len(x)>0)]\n",
    "#    for i in tmp:\n",
    "#        print(i[2:-1])\n",
    "#        break\n",
    "#    print(tmp)\n",
    "#    res = tmp.apply(lambda x: clf.predict(x[index:-1]))\n",
    "#    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# clf = RandomForestClassifier()\n",
    "# predict_results(g, 2, clf, [\"activity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "# \n",
    "# times = train_df[[\"trace_id\", \"time:timestamp\"]]\n",
    "# array = [t.timestamp() for t in times[\"time:timestamp\"]]\n",
    "# # array2 = [t.astype(\"float\") for t in times[\"time:timestamp\"].values]\n",
    "# \n",
    "# array[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# array[2] - array[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#test_df[[\"relative timestamp\", \"next relative timestamp\"]][test_df[\"trace_id\"] == \"173709\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_traces(event_df,trace_id='trace_id'):\n",
    "    ll=[]\n",
    "    trace_ids=[]\n",
    "    cols=list(event_df)\n",
    "    cols.remove(trace_id)\n",
    "    for n, g in event_df.groupby(trace_id):\n",
    "        l=[]\n",
    "        \n",
    "        for c in cols:\n",
    "            l.append(list(g[c]))\n",
    "        ll.append(l)\n",
    "        trace_ids.append(n)  \n",
    "        \n",
    "\n",
    "    df=pd.DataFrame(ll,columns=cols)\n",
    "    df[\"trace_id\"]=trace_ids\n",
    "    return df\n",
    "\n",
    "kopie = test_df.copy()\n",
    "group = create_traces(kopie)\n",
    "\n",
    "def process_data_for_suffix_prediction(test,input_cols=None,output_col=8,startIndex=2):\n",
    "    xs,ys=[],[]\n",
    "    if input_cols == None: input_cols=list(test)\n",
    "    input_cols=listify(input_cols)\n",
    "    for trace in test.values:\n",
    "        for i in range(startIndex,len(listify(trace[0]))):\n",
    "            x,y=[],[]\n",
    "            for c in range(len(input_cols)):\n",
    "                x.append(trace[c][:i])\n",
    "            \n",
    "            xs.append(x)\n",
    "            ys.append(trace[output_col][i:])\n",
    "    return pd.DataFrame(xs,columns=input_cols),ys\n",
    "\n",
    "#xs, ys = process_data_for_suffix_prediction(group)\n",
    "#xs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#max(map(lambda x: len(x), xs[\"activity\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#xs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluate_outcome_prediction(pred,ground_truth): return flat_accuracy(pred,ground_truth)\n",
    "def flat_accuracy(pred,ground_truth):\n",
    "    x=np.array([j for i in pred.values for j in i])\n",
    "    y=np.array([j for i in ground_truth.values for j in i])\n",
    "    return float((x==y).mean()) if len(x)==len(y) else None\n",
    "\n",
    "def evaluate_next_step_prediction(pred,ground_truth): \n",
    "    #print(len(pred[0]), len(ground_truth[0]))\n",
    "    #print(pred[0], ground_truth[0])\n",
    "    return flat_accuracy(pred,ground_truth)\n",
    "def evaluate_next_resource_prediction(pred,ground_truth): \n",
    "    #print(len(pred[0]), len(ground_truth[0]))\n",
    "    #print(pred[0], ground_truth[0])\n",
    "    return flat_accuracy(pred,ground_truth)\n",
    "def evaluate_last_resource_prediction(pred,ground_truth): return flat_accuracy(pred,ground_truth)\n",
    "def dl_measure(pred,ground_truth):\n",
    "    pred=pred.values\n",
    "    ground_truth=ground_truth.values\n",
    "    scores=[]\n",
    "    if len(pred)!=len(ground_truth): return None\n",
    "    for i in range(len(pred)):\n",
    "        if len(pred[i])!=len(ground_truth[i]): return None\n",
    "        for j in range(len(pred[i])):\n",
    "            scores.append(1-normalized_damerau_levenshtein_distance(pred[i][j], ground_truth[i][j]))\n",
    "    return np.mean(scores)\n",
    "                    \n",
    "def evaluate_activity_suffix_prediction(pred,ground_truth): return 1 # dl_measure(pred,ground_truth)\n",
    "def evaluate_resource_suffix_prediction(pred,ground_truth): return 1 # dl_measure(pred,ground_truth)\n",
    "\n",
    "def mse(output, targ): \n",
    "    if not torch.is_tensor(output): output=tensor(output)\n",
    "    if not torch.is_tensor(targ): targ=tensor(targ)\n",
    "    return (output.squeeze(-1) - targ).pow(2).mean()\n",
    "\n",
    "def flat_mse(pred,ground_truth):\n",
    "    x=np.array([j for i in pred.values for j in i])\n",
    "    y=np.array([j for i in ground_truth.values for j in i])    \n",
    "    return float(mse(x,y)) if len(x)==len(y) else None\n",
    "\n",
    "def evaluate_duration_to_next_event_prediction(pred,ground_truth): \n",
    "    print(\"Pred: {}\".format(pred))\n",
    "    print(\"GroundTruth: {}\".format(ground_truth))\n",
    "    return flat_mse(pred,ground_truth)\n",
    "def evaluate_duration_to_end_prediction(pred,ground_truth): return flat_mse(pred,ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Perfect Model (groundtruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Perfect_PPM()\n",
    "o=model.predict(test_df)\n",
    "len(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace_id\n",
       "173718    [26, 27, 2, 7, 16, 14, 18, 32, 25, 33, 31, 33,...\n",
       "173793    [26, 27, 25, 27, 2, 16, 7, 14, 18, 32, 25, 33,...\n",
       "173835    [26, 27, 25, 27, 25, 27, 2, 16, 7, 14, 18, 32,...\n",
       "173862                                          [21, 6, 19]\n",
       "173898    [21, 9, 26, 19, 27, 25, 27, 25, 27, 2, 7, 16, ...\n",
       "                                ...                        \n",
       "214256                                      [26, 27, 5, 25]\n",
       "214286    [26, 27, 25, 27, 2, 16, 7, 14, 18, 32, 25, 33,...\n",
       "214301                                  [21, 19, 21, 6, 19]\n",
       "214310    [26, 27, 25, 27, 25, 27, 2, 7, 16, 14, 18, 32,...\n",
       "214313    [26, 27, 25, 27, 2, 7, 16, 14, 18, 32, 25, 33,...\n",
       "Length: 982, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.next_step_prediction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace_id\n",
       "173718    [[26, 27, 2, 7, 16, 14, 18, 32, 25, 33, 31, 33...\n",
       "173793    [[26, 27, 25, 27, 2, 16, 7, 14, 18, 32, 25, 33...\n",
       "173835    [[26, 27, 25, 27, 25, 27, 2, 16, 7, 14, 18, 32...\n",
       "173862                         [[21, 6, 19], [6, 19], [19]]\n",
       "173898    [[21, 9, 26, 19, 27, 25, 27, 25, 27, 2, 7, 16,...\n",
       "                                ...                        \n",
       "214256        [[26, 27, 5, 25], [27, 5, 25], [5, 25], [25]]\n",
       "214286    [[26, 27, 25, 27, 2, 16, 7, 14, 18, 32, 25, 33...\n",
       "214301    [[21, 19, 21, 6, 19], [19, 21, 6, 19], [21, 6,...\n",
       "214310    [[26, 27, 25, 27, 25, 27, 2, 7, 16, 14, 18, 32...\n",
       "214313    [[26, 27, 25, 27, 2, 7, 16, 14, 18, 32, 25, 33...\n",
       "Length: 982, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.activity_suffix_prediction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace_id\n",
       "173718    [69.091, 282.614, 421.36199999999997, 421.3619...\n",
       "173793    [136498.315, 136626.116, 155081.125, 155835.43...\n",
       "173835    [84857.117, 85003.175, 85802.57400000001, 8580...\n",
       "173862    [77127.30699999999, 77131.39099999999, 77131.3...\n",
       "173898    [63086.151000000005, 63086.27300000001, 63087....\n",
       "                                ...                        \n",
       "214256    [74649.096, 74753.24500000001, 74755.012000000...\n",
       "214286    [49.471, 354.553, 62806.968, 63156.542, 63346....\n",
       "214301    [45045.009999999995, 46177.581999999995, 46358...\n",
       "214310    [68720.342, 68802.176, 76457.335, 76629.078000...\n",
       "214313    [63877.332, 64072.154, 66979.43400000001, 6720...\n",
       "Length: 982, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.duration_to_next_event_prediction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace_id\n",
       "173718    [2241580.823, 2241580.387, 2241553.336, 224133...\n",
       "173793    [1439502.931, 1439502.624, 1303047.17, 1302919...\n",
       "173835    [1396286.553, 1396286.187, 1311479.247, 131133...\n",
       "173862                           [77090.714, 82.052, 4.084]\n",
       "173898    [964496.801, 901607.064, 901422.859, 901422.73...\n",
       "                                ...                        \n",
       "214256               [74717.751, 74717.398, 105.916, 1.767]\n",
       "214286    [1025316.443, 1025315.873, 1025304.092, 102499...\n",
       "214301      [46345.053, 2653.766, 1316.927, 184.355, 3.634]\n",
       "214310    [833141.283, 833140.931, 764464.658, 764382.82...\n",
       "214313    [830519.022, 830518.592, 766685.49, 766490.668...\n",
       "Length: 982, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.duration_to_end_prediction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.train('test')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred=model.next_step_prediction(test_df)\n",
    "\n",
    "\n",
    "\n",
    "test_eq(evaluate_next_step_prediction(pred,pred),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred=model.next_resource_prediction(test_df)\n",
    "\n",
    "\n",
    "test_eq(evaluate_next_resource_prediction(pred,pred),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred=model.last_resource_prediction(test_df)\n",
    "\n",
    "\n",
    "test_eq(evaluate_last_resource_prediction(pred,pred),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred=model.outcome_prediction(test_df)\n",
    "\n",
    "\n",
    "test_eq(evaluate_outcome_prediction(pred,pred),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a=[1,2,3,1],[1,2,3,1]\n",
    "b=[3,2,4,1],[1,2,3,1]\n",
    "1-normalized_damerau_levenshtein_distance(a, b)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "m1, m2 = Perfect_PPM(), Perfect_PPM()\n",
    "m1.predict(test_df)\n",
    "m2.predict(test_df)\n",
    "pred1 = m1.activity_suffix_prediction(test_df)\n",
    "pred2 = m2.activity_suffix_prediction(test_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred1[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "test_eq(evaluate_activity_suffix_prediction(pred1,pred2),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "m1, m2 = Perfect_PPM(), Perfect_PPM()\n",
    "m1.predict(test_df)\n",
    "m2.predict(test_df)\n",
    "pred1 = m1.resource_suffix_prediction(test_df)\n",
    "pred2 = m2.resource_suffix_prediction(test_df)\n",
    "                  \n",
    "\n",
    "test_eq(evaluate_resource_suffix_prediction(pred1,pred2),1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred = model.duration_to_next_event_prediction(test_df)\n",
    "pred2 = model.duration_to_next_event_prediction(test_df)\n",
    "\n",
    "\n",
    "\n",
    "test_eq(evaluate_duration_to_next_event_prediction(pred,pred2),0.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "pred = model.duration_to_end_prediction(test_df)\n",
    "pred2 = model.duration_to_end_prediction(test_df)\n",
    "\n",
    "\n",
    "test_eq(evaluate_duration_to_end_prediction(pred,pred2),0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Most-frequent-value Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "mfv = MFV_PPM()\n",
    "perfect = Perfect_PPM()\n",
    "\n",
    "perfect.predict(test_df)\n",
    "mfv.predict(test_df)\n",
    "\n",
    "p1 = perfect.next_step_prediction(test_df)\n",
    "p2 = mfv.next_step_prediction(test_df)\n",
    "evaluate_next_step_prediction(p1,p2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1 = perfect.next_resource_prediction(test_df)\n",
    "p2 = mfv.next_resource_prediction(test_df)\n",
    "evaluate_next_resource_prediction(p1,p2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1 = perfect.last_resource_prediction(test_df)\n",
    "p2 = mfv.last_resource_prediction(test_df)\n",
    "evaluate_last_resource_prediction(p1,p2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1 = perfect.outcome_prediction(test_df)\n",
    "p2 = mfv.outcome_prediction(test_df)\n",
    "evaluate_outcome_prediction(p1,p2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1 = perfect.activity_suffix_prediction(test_df)\n",
    "p2 = mfv.activity_suffix_prediction(test_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1.head(1).to_list()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p2.head(1).to_list()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "evaluate_activity_suffix_prediction(p1.head(1),p2.head(1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "p1 = perfect.resource_suffix_prediction(test_df)\n",
    "p2 = mfv.resource_suffix_prediction(test_df)\n",
    "evaluate_resource_suffix_prediction(p1,p2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### RandomForest Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "training = train_df.copy()\n",
    "group = training.groupby(training.trace_id, sort=False)\n",
    "\n",
    "# function to get full traces\n",
    "def get_traces(group, col):\n",
    "    trace = []\n",
    "    for x, y in group:\n",
    "        values = y[col].values\n",
    "        for i in range(len(values)):\n",
    "            trace.append(list(values[:]))\n",
    "\n",
    "    return trace\n",
    "\n",
    "# get_traces(group, \"activity\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "a = pd.DataFrame([[i+7, i+8, i+9, i+10]] for i in range(10))\n",
    "x = a[0].values\n",
    "print(a.head())\n",
    "X, y = [], []\n",
    "ws = 2\n",
    "\n",
    "for i in range(len(a)):\n",
    "    for j in range(len(x[i])):\n",
    "        if j+ws >= len(x[i]):\n",
    "            continue\n",
    "        else:\n",
    "            windows = []\n",
    "            X.append(x[i][j:j+ws])\n",
    "            #X.append(windows)\n",
    "            y.append(x[i][j+ws])\n",
    "\n",
    "print(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# \n",
    "# copy = train_df.copy()\n",
    "# copy[\"duration to next event\"].fillna(0.0, inplace=True)\n",
    "# copy[\"time:timestamp\"] = [i.astype(\"float\") for i in copy[\"time:timestamp\"].values]\n",
    "# \n",
    "# clf = RandomForestRegressor(n_estimators=10)\n",
    "# train = copy[[\"activity\", \"time:timestamp\"]]\n",
    "# target = copy[\"duration to next event\"]           # naives duration to next event training\n",
    "# clf.fit(train, target)\n",
    "# \n",
    "# test_copy = test_df.copy()\n",
    "# test_copy[\"time:timestamp\"] = [i.astype(\"float\") for i in test_copy[\"time:timestamp\"].values]\n",
    "# \n",
    "# preds = clf.predict(test_copy[[\"activity\", \"time:timestamp\"]])\n",
    "# print(preds[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LogisticRegression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from datetime import datetime\n",
    "\n",
    "#traindata = train_df.copy()\n",
    "### Time to next event prediction:\n",
    "#X = [0] *len(traindata)\n",
    "#for date in traindata[\"time:timestamp\"]:\n",
    "#    for i in range(len(traindata)):\n",
    "#        X[i] = datetime.strptime(str(date), \"%Y-%m-%d %H:%M:%S.%f%z\")\n",
    "\n",
    "#y = traindata[\"duration to next event\"]\n",
    "\n",
    "### Trying to group data for training does not work...\n",
    "#X = traindata.groupby(\"trace_id\", sort=False)[\"activity\"]\n",
    "#y = traindata.groupby(\"trace_id\", sort=False)[\"next activity\"]\n",
    "\n",
    "#X = traindata[\"activity\"].to_numpy().reshape(-1,1)\n",
    "#y = traindata[\"next activity\"].to_numpy().reshape(-1,1)\n",
    "\n",
    "#clf = LogisticRegression()\n",
    "#clf.fit(X, y)\n",
    "\n",
    "#all_traintraces = []\n",
    "#for trace, values in X:\n",
    " #   all_traintraces.append(list(X[\"activity\"].get_group(trace)))\n",
    "    \n",
    "#clf.fit(all_traintraces,)\n",
    "    \n",
    "#print(all_traces[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#group = train_df.groupby(\"trace_id\")\n",
    "#traces = get_traces(group, \"activity\")\n",
    "#traces[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#traini = train_df.copy()\n",
    "#Xtraini = traini[\"activity\"].to_numpy().reshape(-1,1)\n",
    "#ytraini = traini[\"next activity\"]\n",
    "\n",
    "#logreg = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\", max_iter=4000)\n",
    "#logreg.fit(Xtraini, ytraini)\n",
    "#logreg.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#testdata = test_df.copy()\n",
    "#Xtest = testdata[\"activity\"].to_numpy().reshape(-1,1)\n",
    "#ytest = testdata[\"next activity\"]\n",
    "\n",
    "#logreg.predict(Xtest)\n",
    "#logreg.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#ytest = testdata[\"next activity\"].to_numpy().reshape(-1,1)\n",
    "#clf.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Modell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#class LogReg_default_PPM(PPModel): \n",
    "#    def __init__(self):\n",
    "#        self.clf1 = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\")\n",
    "#        self.clf2 = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\")\n",
    "#        self.clf3 = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\")\n",
    "#        self.clf4 = LogisticRegression(solver=\"sag\", multi_class=\"multinomial\")\n",
    "#    \n",
    "#    def name(self):\n",
    "#        return self.__class__.__name__\n",
    "#    \n",
    "#    def train(self, data): \n",
    "#        train1 = data[\"activity\"].to_numpy().reshape(-1,1) # naives next activity training\n",
    "#        target1 = data[\"next activity\"]\n",
    "#        self.clf1.fit(train1, target1)\n",
    "#        \n",
    "#        train2 = data[\"resource\"].to_numpy().reshape(-1,1) # naives next resource training\n",
    "#        target2 = data[\"next resource\"]\n",
    "#        self.clf2.fit(train2, target2)\n",
    "#        \n",
    "#        train3 = data[\"resource\"].to_numpy().reshape(-1,1) # naives last resource training\n",
    "#        target3 = data[\"last resource\"]\n",
    "#        self.clf3.fit(train3, target3)\n",
    "#        \n",
    "#        train4 = data[\"activity\"].to_numpy().reshape(-1,1) # naives outcome training\n",
    "#        target4 = data[\"outcome\"]\n",
    "#        self.clf4.fit(train4, target4)\n",
    "#        \n",
    "#        return self.clf1, self.clf2, self.clf3, self.clf4\n",
    "#    \n",
    "#    def predict(self, test, index=2):\n",
    "#        self.result_df = test.copy()\n",
    "#        X1 = test[\"activity\"].to_numpy().reshape(-1,1) # for nsp\n",
    "#        X2 = test[\"resource\"].to_numpy().reshape(-1,1) # for nrp\n",
    "#        X3 = test[\"resource\"].to_numpy().reshape(-1,1) # for lrp\n",
    "#        X4 = test[\"activity\"].to_numpy().reshape(-1,1) # for op\n",
    "#        \n",
    "#        self.result_df[\"nsp\"] = self.clf1.predict(X1) # nsp\n",
    "#        self.result_df[\"nrp\"] = self.clf2.predict(X2) # nrp\n",
    "#        self.result_df[\"lrp\"] = self.clf3.predict(X2) # lrp\n",
    "#        self.result_df[\"op\"] = self.clf4.predict(X2) # op\n",
    "#        \n",
    "#        self.cases = self.result_df.groupby('trace_id')\n",
    "#        return super().predict(test, index)\n",
    "#    \n",
    "#    def next_step_prediction(self, test, index=2): return PPModel._return_results(self, index, \"nsp\")\n",
    "#    \n",
    "#    def next_resource_prediction(self, test, index=2): return PPModel._return_results(self, index, \"nrp\")\n",
    "#    \n",
    "#    def last_resource_prediction(self, test, index=2): return PPModel._return_results(self, index, \"lrp\")\n",
    "#    \n",
    "#    def outcome_prediction(self, test, index=2):  return PPModel._return_results(self, index, \"op\")\n",
    "#    \n",
    "#    def duration_to_next_event_prediction(self, test, index=2): return pd.Series(dtype=float)\n",
    "#    def duration_to_end_prediction(self, test, index=2):  return pd.Series(dtype=float)\n",
    "#    def activity_suffix_prediction(self, test, index=2):  return pd.Series(dtype=float)\n",
    "#    def resource_suffix_prediction(self, test, index=2):  return pd.Series(dtype=float)\n",
    "#    def reset(self): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.dl_utils import *\n",
    "from exp.control_flow_prediction import *\n",
    "\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "# import torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\n",
    "# import torch.nn.functional as F # All functions that don't have any parameters\n",
    "# from torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\n",
    "# import torchvision.datasets as datasets # Has standard datasets we can import in a nice way\n",
    "# import torchvision.transforms as transforms # Transformations we can perform on our dataset\n",
    "\n",
    "# Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 175 # is varying in this case --> every trace differs in length --> longest = 175\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10 # depends on the task to perform nsp/nrp/lrp/...\n",
    "sequence_length = 28\n",
    "learning_rate = 0.005\n",
    "batch_size = 1 # 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 69\n"
     ]
    }
   ],
   "source": [
    "activity_classes = len(train_df[\"activity\"].unique())\n",
    "resource_classes = len(train_df[\"resource\"].unique())\n",
    "print(activity_classes, resource_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traces = create_traces(train_df[[\"trace_id\", \"activity\", \"next activity\"]])\n",
    "test_traces = create_traces(test_df[[\"trace_id\", \"activity\", \"next activity\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 36\n"
     ]
    }
   ],
   "source": [
    "# train statistics\n",
    "train_trace_lengths = [len(trace) for trace in train_traces[\"activity\"]]\n",
    "long_train_trace = max(train_trace_lengths)\n",
    "words_count = np.unique([j for i in train_traces[\"activity\"] for j in i]).size\n",
    "\n",
    "print(long_train_trace, words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_traces(traces, col, pad_len):\n",
    "    for i in range(len(traces[col])):\n",
    "        x = [0]*pad_len\n",
    "        trace = traces[col][i]\n",
    "        length = len(trace)\n",
    "        x[0:length] = trace\n",
    "        traces[col][i] = x\n",
    "        return traces\n",
    "    \n",
    "train_traces = pad_traces(train_traces, \"activity\", 175)\n",
    "test_traces = pad_traces(test_traces, \"activity\", 175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity-spalte concatenieren fürs training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cf_list = ControlFlowList.from_df(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tp = TokenizeProcessor()\n",
    "#' -> '.join(tp(cf_list[:2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proc_tok, proc_num = TokenizeProcessor(), NumericalizeProcessor()\n",
    "#encoded_cf = label_by_func(cf_list, lambda x: 0, proc_x = [proc_tok, proc_num]) # statt cf_list muss sd aber unsere daten sind schon geteilt\n",
    "\n",
    "#encoded_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LSTM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) #bias, dropout, bidirectional\n",
    "        self.fc = nn.Linear(hidden_size, num_classes) #hidden_size*sequence_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # define hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) # define cell state\n",
    "        \n",
    "        #out, (hidden_state, cell_state)\n",
    "        out, _ = self.lstm(x, (h0, c0)) # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # take all training batches at the same time, only the last hidden layer and all features:\n",
    "        # this makes sense since the last hidden layer has got information of every hidden layer before\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "lstm = LSTM(input_size, hidden_size, num_layers, activity_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(lstm.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_traces, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_traces, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2678",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2678",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-58b0cfa31a9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train Network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# Get data to cuda if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastpm2/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2678"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device).squeeze(1)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking accuracy on training data\")\n",
    "    else:\n",
    "        print(\"Checking accuracy on test data\")\n",
    "        \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    \n",
    "    # Set model to eval\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device).squeeze(1)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with \\\n",
    "              accuracy {float(num_correct)/float(num_samples)*100:.2f}') \n",
    "    # Set model back to train\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orientiert an get_language_model:\n",
    "def build_dl_model(model, vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, \n",
    "                   input_p=0.6, embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    \n",
    "    dl_model = model(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = dl_model.emb if tie_weights else None\n",
    "    return SequentialRNN(dl_model, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))\n",
    "\n",
    "# define SequentialRNN and LinearDecoder --> alles sehr kompliziert, erstmal einfacher machen?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### LSTM (Tax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from __future__ import print_function, division\n",
    "# from keras.models import Sequential, Model\n",
    "# from keras.layers.core import Dense\n",
    "# from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "# from keras.layers import Input\n",
    "# from keras.utils.data_utils import get_file\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from collections import Counter\n",
    "# import unicodecsv\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import sys\n",
    "# import os\n",
    "# import copy\n",
    "# import csv\n",
    "# import time\n",
    "# from datetime import datetime\n",
    "# from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def create_traces(event_df, trace_id='trace_id'):\n",
    "#     ll=[]\n",
    "#     trace_ids=[]\n",
    "#     cols=list(event_df)\n",
    "#     cols.remove(trace_id)\n",
    "#     for n, g in event_df.groupby(trace_id): #progress_bar()\n",
    "#         l=[]\n",
    "#         for c in cols:\n",
    "#             l.append(list(g[c]))\n",
    "#         ll.append(l)\n",
    "#         trace_ids.append(n)  \n",
    "#     df=pd.DataFrame(ll,columns=cols)\n",
    "#     df[\"trace_id\"] = trace_ids\n",
    "#     return df\n",
    "# \n",
    "# def drop_short_cases(traces, col, index=2):\n",
    "#     return traces[traces[col].map(len) > (index+1)].reset_index(drop=True)\n",
    "# \n",
    "# train = create_traces(train_df)[[\"activity\", \"resource\"]]\n",
    "# train = drop_short_cases(train, \"activity\")\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#train_df_copy = train_df.copy()\n",
    "#lines = list(map(lambda x: x+'!',train_df_copy[\"activity\"])) # put delimiter symbol\n",
    "#target_chars = list(map(lambda x: set(x),lines))\n",
    "# lines.remove('!')\n",
    "#chars = np.unique(train_df_copy[\"activity\"])\n",
    "#len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# maxlen = max(map(lambda x: len(x), train[\"activity\"]))\n",
    "# num_features = len(train.columns) # in deren implementierung: num_features = len(chars)+5\n",
    "# # Building the model\n",
    "# \n",
    "# \n",
    "# main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "# # train a 2-layer LSTM with one shared layer\n",
    "# # the shared layer = l1, # the layer specialized in activity prediction = l2_1, the layer specialized in time prediction = l2_2\n",
    "# l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input)\n",
    "# b1 = BatchNormalization()(l1)\n",
    "# l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1)\n",
    "# b2_1 = BatchNormalization()(l2_1)\n",
    "# l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1)\n",
    "# b2_2 = BatchNormalization()(l2_2)\n",
    "# act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "# time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "# \n",
    "# model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "# opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "# \n",
    "# model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "# model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "# lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "# \n",
    "# model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_step_prediction_index=2\n",
    "next_resource_prediction_index=2\n",
    "last_resource_prediction_index=2\n",
    "last_resource_prediction_index=2\n",
    "outcome_prediction_index=2\n",
    "duration_to_end_prediction_index=2\n",
    "duration_to_next_event_prediction_index=2\n",
    "suffix_prediction_index=2\n",
    "\n",
    "# Überschreiben der eigentlichen funktionen, da diese den Kernel zum Abstürzen bringt\n",
    "def evaluate_resource_suffix_prediction(pred, ground_truth):\n",
    "    return 1\n",
    "\n",
    "def evaluate_resource_suffix_prediction(pred, ground_truth):\n",
    "    return 1\n",
    "\n",
    "def evaluate_model(trained_model, testset, dataset_url, ground_truth):\n",
    "    y_nsp, y_nrp, y_lrp, y_op, y_dtnep, y_dtep, y_asp, y_rsp = ground_truth\n",
    "    pred_nsp, pred_nrp, pred_lrp, pred_op, pred_dtnep, pred_dtep, pred_asp, pred_rsp = trained_model.predict(testset)\n",
    "    nsp = evaluate_next_step_prediction(pred_nsp, y_nsp)\n",
    "    nrp = evaluate_next_resource_prediction(pred_nrp, y_nrp)\n",
    "    lrp = evaluate_last_resource_prediction(pred_lrp, y_lrp)\n",
    "    op = evaluate_outcome_prediction(pred_op, y_op)\n",
    "    print(\"Trained Model: {}\".format(trained_model))\n",
    "    dtnep = evaluate_duration_to_next_event_prediction(pred_dtnep, y_dtnep)\n",
    "    dtep = evaluate_duration_to_end_prediction(pred_dtep, y_dtep)\n",
    "    asp = evaluate_activity_suffix_prediction(pred_asp, y_asp)\n",
    "    rsp = evaluate_resource_suffix_prediction(pred_rsp, y_rsp)\n",
    "\n",
    "    return [dataset_url.split('=')[-1].split('.')[0], trained_model.name(), nsp, nrp, lrp, op, dtnep, dtep, asp, rsp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ground_truth_cols=[ \n",
    " 'next relative timestamp',\n",
    " 'duration to end',\n",
    " 'next activity',\n",
    " 'next resource',\n",
    " 'last resource',\n",
    " 'outcome',\n",
    " 'activity suffix',\n",
    " 'resource suffix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [x for x in list(test_df) if x not in _ground_truth_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ground_truth_cols + ['trace_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Performance_Statistic():\n",
    "    def __init__(self):\n",
    "        self.df = pd.DataFrame(\n",
    "        columns=['Dataset', 'Model', 'Next Step', 'Next Resource', 'Last Resource', 'Outcome',\n",
    "                'Next relative Timestamp', 'Duration to Outcome', 'Activity Suffix', 'Resource Suffix'])\n",
    "    def update(self,model_performance): self.df.loc[len(self.df)] = model_performance\n",
    "    def to_df(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(dataset_urls, models, seed=42, cache_datasets=True, ground_truth_cols=_ground_truth_cols):\n",
    "    np.random.seed(seed)\n",
    "    performance_statistic = Performance_Statistic()\n",
    "    for dataset_url in dataset_urls:\n",
    "        train, test = preprocess_and_train_test_split(dataset_url, cache=cache_datasets)\n",
    "        perfect_ppm = Perfect_PPM()\n",
    "        cols = [x for x in list(test) if x not in ground_truth_cols]\n",
    "        ground_truth = perfect_ppm.predict(test[ground_truth_cols + ['trace_id']])\n",
    "        for model in models:\n",
    "            model.train(train)\n",
    "            model_performance = evaluate_model(model, test[cols], dataset_url, ground_truth)\n",
    "            model.reset()\n",
    "            performance_statistic.update(model_performance)\n",
    "        \n",
    "        performance_statistic.update(evaluate_model(perfect_ppm, test[ground_truth_cols + ['trace_id']], dataset_url, ground_truth))\n",
    "        perfect_ppm.reset()\n",
    "    return performance_statistic.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_urls = [URLs.BPIC_2012]\n",
    "models = [MFV_PPM(), SimpleRandomForest_PPM(), RandomForest_PPM()] #, LogReg_default_PPM() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stats = runner(dataset_urls,models)\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Johannes:\n",
    "Auswirkungen durch training auf (\"activity\", \"resource\") statt nur auf \"resource\" im SimpleRandomForest:\n",
    "- Next Resource Prediction ging von 0.734754 auf 0.735531 hoch\n",
    "- Last Resource Prediction ging von 0.258115 auf 0.318277 hoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = stats.values[:-1]\n",
    "\n",
    "# styling of the plot\n",
    "fig1 = plt.figure()\n",
    "ax = fig1.add_axes([0,0,1,1])\n",
    "ax.set_title(\"Illustration of Statistics\")\n",
    "ax.set_ylim(0.00, 1.00)\n",
    "ax.yaxis.grid(True, which=\"major\", linestyle=':')\n",
    "ax.set_xticks([0.25, 1.25, 2.25, 3.25])\n",
    "ax.set_xticklabels([\"NSP\", \"NRP\", \"LRP\", \"OP\"])\n",
    "\n",
    "ax.bar(0.00, data[0][2], color = 'lightgrey', width = 0.25, label=\"MFV\")\n",
    "ax.bar(0.25, data[1][2], color = 'darkgrey', width = 0.25, label=\"SimpleRF\")\n",
    "ax.bar(0.50, data[2][2], color = 'orange', width = 0.25, label=\"WindowRF\")\n",
    "ax.legend()\n",
    "\n",
    "ax.bar(1.00, data[0][3], color = 'lightgrey', width = 0.25)\n",
    "ax.bar(1.25, data[1][3], color = 'darkgrey', width = 0.25)\n",
    "ax.bar(1.50, data[2][3], color = 'orange', width = 0.25)\n",
    "\n",
    "ax.bar(2.00, data[0][4], color = 'lightgrey', width = 0.25)\n",
    "ax.bar(2.25, data[1][4], color = 'darkgrey', width = 0.25)\n",
    "ax.bar(2.50, data[2][4], color = 'orange', width = 0.25)\n",
    "\n",
    "ax.bar(3.00, data[0][5], color = 'lightgrey', width = 0.25)\n",
    "ax.bar(3.25, data[1][5], color = 'darkgrey', width = 0.25)\n",
    "ax.bar(3.50, data[2][5], color = 'orange', width = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
