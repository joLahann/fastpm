{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.eventlog import *\n",
    "from exp.dl_utils import *\n",
    "\n",
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 2012-04-23T00:00:00.000+02:00\n",
      "failed to parse date: 2011-10-01T00:38:44.546+02:00\n",
      "failed to parse date: 2012-03-14T16:04:54.681+01:00\n"
     ]
    }
   ],
   "source": [
    "path = untar_data(URLs.BPIC_2012)\n",
    "log = import_xes(path, extensions=False, classifiers=False, schema=False, log_attributes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>REG_DATE</th>\n",
       "      <th>AMOUNT_REQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>173688</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_SUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.546000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173688</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PARTLYSUBMITTED</td>\n",
       "      <td>2011-09-30 22:38:44.880000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>173688</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>A_PREACCEPTED</td>\n",
       "      <td>2011-09-30 22:39:37.906000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>173688</td>\n",
       "      <td>3</td>\n",
       "      <td>112</td>\n",
       "      <td>SCHEDULE</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-09-30 22:39:38.875000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173688</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>START</td>\n",
       "      <td>W_Completeren aanvraag</td>\n",
       "      <td>2011-10-01 09:36:46.437000+00:00</td>\n",
       "      <td>2011-10-01 00:38:44.546000+02:00</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trace_id  event_id org:resource lifecycle:transition  \\\n",
       "0   173688         0          112             COMPLETE   \n",
       "1   173688         1          112             COMPLETE   \n",
       "2   173688         2          112             COMPLETE   \n",
       "3   173688         3          112             SCHEDULE   \n",
       "4   173688         4         None                START   \n",
       "\n",
       "             concept:name                   time:timestamp  \\\n",
       "0             A_SUBMITTED 2011-09-30 22:38:44.546000+00:00   \n",
       "1       A_PARTLYSUBMITTED 2011-09-30 22:38:44.880000+00:00   \n",
       "2           A_PREACCEPTED 2011-09-30 22:39:37.906000+00:00   \n",
       "3  W_Completeren aanvraag 2011-09-30 22:39:38.875000+00:00   \n",
       "4  W_Completeren aanvraag 2011-10-01 09:36:46.437000+00:00   \n",
       "\n",
       "                           REG_DATE AMOUNT_REQ  \n",
       "0  2011-10-01 00:38:44.546000+02:00      20000  \n",
       "1  2011-10-01 00:38:44.546000+02:00      20000  \n",
       "2  2011-10-01 00:38:44.546000+02:00      20000  \n",
       "3  2011-10-01 00:38:44.546000+02:00      20000  \n",
       "4  2011-10-01 00:38:44.546000+02:00      20000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Trace Attributes and Event Attributes first in one df. It is easier to copy over the trace attributes.\n",
    "\n",
    "df = pd.merge(log.events, log.traceAttributes, left_on='trace_id', right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_id = \"trace_id\"\n",
    "\n",
    "def random_split_traces(d, split=0.8, trace_id='trace_id'):\n",
    "    traces = d[trace_id].drop_duplicates()\n",
    "    shuffled = traces.iloc[np.random.permutation(len(traces))].values\n",
    "    split = int(len(traces) * split)\n",
    "    return shuffled[:split], shuffled[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(t, df): \n",
    "    return df[df[trace_id].isin(t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into train_indices and test_indices\n",
    "train_valid_ids, test_ids = random_split_traces(df)\n",
    "\n",
    "# Get the Dataframe from the train_idices to split it into train and valid\n",
    "train_valid_df = get_df(train_valid_ids, df)\n",
    "train_ids, valid_ids = random_split_traces(train_valid_df, split=0.9)\n",
    "\n",
    "# Get the Dataframe for train, valid and test\n",
    "train_df = get_df(train_ids, df)\n",
    "valid_df = get_df(valid_ids, df)\n",
    "test_df = get_df(test_ids, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into train_indices and test_indices\n",
    "#train_ids, test_ids = random_split_traces(df)\n",
    "\n",
    "# Get the Dataframe from the train_idices to split it into train and valid\n",
    "#train_valid_df = get_df(train_valid_ids, df)\n",
    "#train_valid_ids, valid_ids = random_split_traces(train_valid_df, split=0.9)\n",
    "\n",
    "# Get the Dataframe for train, valid and test\n",
    "#train_df = get_df(train_ids, df)\n",
    "#valid_df = get_df(valid_ids, df)\n",
    "#test_df = get_df(test_ids, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cont_column(x, mean, std, eps=1e-7): \n",
    "    return (x-mean)/(eps+std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "cat_names = ['event_id', 'org:resource', 'lifecycle:transition', 'concept:name']\n",
    "date_names = ['time:timestamp', 'REG_DATE']\n",
    "cont_names = ['AMOUNT_REQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res\n",
    "\n",
    "class Processor():\n",
    "    def process(self, items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self,default_token=None): \n",
    "        self.vocab = None\n",
    "        self.default_token = default_token\n",
    "\n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            if self.default_token is not None:\n",
    "                for o in reversed(self.default_token):\n",
    "                    if o in self.vocab: self.vocab.remove(o)\n",
    "                    self.vocab.insert(0, o)\n",
    "            self.otoi = {v: k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    \n",
    "    def proc1(self, item): return self.otoi[item]\n",
    "\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    \n",
    "    def deproc1(self, idx): return self.vocab[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False, utc=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    df = df.copy()\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, utc=utc,infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    cols=[]\n",
    "    for n in attr: \n",
    "        col_name=targ_pre +\"_\"+ n\n",
    "        df[col_name] = getattr(fld.dt, n.lower())\n",
    "        cols.append(col_name)\n",
    "    df[targ_pre + '_Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "    return df, cols, targ_pre + '_Elapsed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceProcessor(Processor):\n",
    "    def __init__(self, cat_names, cont_names, date_names, vocabs={}):\n",
    "        self.vocabs = vocabs\n",
    "        self.cat_names, self.cont_names, self.date_names = cat_names, cont_names, date_names\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        cat_names, cont_names = self.cat_names[:], self.cont_names[:]\n",
    "        for d in self.date_names:\n",
    "            df, cat, cont = add_datepart(df, d, utc=True)\n",
    "            cat_names += listify(cat)    \n",
    "            cont_names += listify(cont)\n",
    "\n",
    "        for c in cat_names:\n",
    "            if not c in self.vocabs.keys(): \n",
    "                self.vocabs[c] = CategoryProcessor(default_spec_tok)\n",
    "            df[c] = self.vocabs[c](df[c])\n",
    "            \n",
    "            \n",
    "        for c in cont_names:\n",
    "            df[c] = df[c].astype(float)\n",
    "\n",
    "            if not c in self.vocabs.keys(): \n",
    "                self.vocabs[c] = df[c].mean(), df[c].std()\n",
    "            df[c] = normalize_cont_column(df[c], *self.vocabs[c])\n",
    "    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "167",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c9e3b7dc905d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalid_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# unknown tokens einfügen!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-7f1966da3596>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoryProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_spec_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4f93663513cf>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4f93663513cf>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motoi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4f93663513cf>\u001b[0m in \u001b[0;36mproc1\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mproc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0motoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 167"
     ]
    }
   ],
   "source": [
    "# Encode Data\n",
    "\n",
    "tp = TraceProcessor(cat_names, cont_names, date_names)\n",
    "\n",
    "train_proc = tp(train_df)\n",
    "valid_proc = tp(valid_df)\n",
    "test_proc = tp(test_df) # unknown tokens einfügen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tp.vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traces(event_df, trace_id='trace_id'):\n",
    "    ll = []\n",
    "    trace_ids = []\n",
    "    cols = list(event_df)\n",
    "    cols.remove(trace_id)\n",
    "    for n, g in event_df.groupby(trace_id):\n",
    "        l = []\n",
    "        \n",
    "        for c in cols:\n",
    "            l.append(list(g[c]))\n",
    "        ll.append(l)\n",
    "        trace_ids.append(n)  \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(ll, columns=cols)\n",
    "    df.index = trace_ids\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traces = create_traces(train_proc)\n",
    "valid_traces = create_traces(valid_proc)\n",
    "test_traces = create_traces(test_proc)\n",
    "\n",
    "train_traces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LanguageModel Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMDataSet():\n",
    "    def __init__(self, df, bs=64, bptt=70, shuffle=False):\n",
    "        self.bs, self.bptt, self.shuffle = bs, bptt, shuffle\n",
    "        self.cols = list(df)\n",
    "\n",
    "        total_len = sum(df.apply(lambda x: max([len(listify(x[k])) for k in self.cols]),axis=1))\n",
    "        self.n_batch = total_len // self.bs\n",
    "        self.batched = self.batchify(df)\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched[:, idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        x, y = source[:, seq_idx:seq_idx+self.bptt], source[:, seq_idx+1:seq_idx+self.bptt+1]\n",
    "        return x,y\n",
    "    \n",
    "    def batchify(self,df):\n",
    "        if self.shuffle: df=df.sample(frac=1)\n",
    "        \n",
    "        dd={}\n",
    "        for c in self.cols:\n",
    "            dd[c]=[]\n",
    "        for i, row in df.iterrows():\n",
    "            l = max([len(listify(row[c])) for c in self.cols])\n",
    "            for c in self.cols:\n",
    "                dd[c].append(tensor(row[c]).expand(l))\n",
    "        for c in self.cols:\n",
    "            s = torch.cat([torch.cat((tensor([2.0]), t.float(), tensor([3.0]))) for t in dd[c]])\n",
    "            dd[c] = s[:self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
    "        return torch.stack([dd[c] for c in self.cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 128, 70\n",
    "\n",
    "def get_dls(train_ds, valid_ds,  **kwargs):\n",
    "    return (DataLoader(LMDataSet(train_ds, shuffle=True,bptt=bptt), batch_size=bs),\n",
    "            DataLoader(LMDataSet(valid_ds, shuffle=False,bptt=bptt), batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*get_dls(train_traces, valid_traces))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(data.train_dl)\n",
    "xb, yb = next(iter_dl)\n",
    "\n",
    "# xb hat also die folgende Größe: 128 Einträge, die jeweils 31 Einträge mit 70 Werten enthalten.\n",
    "xb.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10,10)\n",
    "mask = dropout_mask(x, (10,10), 0.5); mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x*mask).std(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = RNNDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "tst_input.shape, dp(tst_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nn.LSTM(5, 2)\n",
    "dp_module = WeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_input = torch.randn(4,20,5)\n",
    "h = (torch.zeros(1,20,2), torch.zeros(1,20,2))\n",
    "x,h = dp_module(tst_input,h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0,100,(8,))\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AWD_LSTM(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input[:, 3].long()\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output) \n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_activity(input, target):\n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    \n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.flatten().long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_activity(input, target): \n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    \n",
    "    return (torch.argmax(input.view(bs * sl, -1), dim=1)==target.flatten().long()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop with AWD-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNTrainer(Callback):\n",
    "    def __init__(self, α, β): self.α,self.β = α,β\n",
    "    \n",
    "    def after_pred(self):\n",
    "        #Save the extra outputs for later and only returns the true output.\n",
    "        self.raw_out,self.out = self.pred[1],self.pred[2]\n",
    "        self.run.pred = self.pred[0]\n",
    "    \n",
    "    def after_loss(self):\n",
    "        #AR and TAR\n",
    "        if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean()\n",
    "        if self.β != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "                \n",
    "    def begin_epoch(self):\n",
    "        pass\n",
    "        #Shuffle the texts at the beginning of the epoch\n",
    "        #if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_index = 1\n",
    "vocab_size = len(tp.vocabs[\"concept:name\"].vocab)\n",
    "emb_sz, nh, nl = 300, 300, 2\n",
    "\n",
    "model = get_language_model(vocab_size, emb_sz, nh, nl, padding_index, input_p=0.6, output_p=0.4, weight_p=0.5, \n",
    "                           embed_p=0.1, hidden_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback,accuracy_activity),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       partial(RNNTrainer, α=2., β=1.),\n",
    "       ProgressBarCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, cross_entropy_activity, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.fit(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awd_lstm = learn.model\n",
    "# awd_lstm(xb.cuda()).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "def learn(model, train_dl, valid_dl, vocab_size, epochs=10, bs=64, lr=0.005, clip=5, cuda=False):\n",
    "    ''' Training model on training data\n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        model: Neural network\n",
    "        train_dl: dataloader for training data\n",
    "        valid_dl: dataloader for validation data\n",
    "        vocab_size: size of the vocab of the model\n",
    "        epochs: Number of epochs to train\n",
    "        bs: batch size\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping (to prevent exploding gradient problem)\n",
    "        cuda: Train with CUDA on a GPU\n",
    "    '''\n",
    "    \n",
    "    # putting the model in training mode\n",
    "    print(model)\n",
    "    model.train()\n",
    "    \n",
    "    # Loading Adam Optimizer and CrossEntropy as Loss-function\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    counter = 0\n",
    "    for e in range(epochs): \n",
    "        \n",
    "        for x, y in iter(train_dl):\n",
    "            x = x[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            #y = y[:, 3]  # number of the desired column --> in this case the concept:name\n",
    "            counter += 1\n",
    "            \n",
    "            model.zero_grad()\n",
    "            inputs = one_hot_encode(x)\n",
    "            targets = y[:, 3] # .type(torch.LongTensor)\n",
    "            #print(inputs, inputs.size())\n",
    "\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "            out = model.forward(inputs)\n",
    "            loss = criterion(out, targets.view(bs).type(torch.LongTensor))\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % 2 == 0:\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in iter(valid_dl):\n",
    "                    v_inputs, v_targets = x, y\n",
    "                    \n",
    "                    if cuda:\n",
    "                        v_inputs, v_targets = inputs.cuda(), targets.cuda()\n",
    "                    \n",
    "                    v_out = model.forward(v_inputs)\n",
    "                    v_loss = criterion(v_out, v_targets.view(bs).type(torch.LongTensor))\n",
    "                    v_loss.backward()\n",
    "                    \n",
    "                    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                    opt.step()\n",
    "                    \n",
    "                    val_losses.append(v_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "#    return (DataLoader(PPDataSet(train_ds, shuffle=True), batch_size=bs),\n",
    "#            DataLoader(PPDataSet(valid_ds, shuffle=False), batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data = DataBunch(*get_dls(train, valid, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#len(vocabs[\"concept:name\"].vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize and print the network\n",
    "vocab_size = len(vocabs[\"concept:name\"].vocab)\n",
    "\n",
    "network = LSTM(vocab_size, n_hidden=256, n_layers=6)\n",
    "learn(network, dl, dl2, vocab_size=vocab_size, epochs=10, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Monitoring Validation Loss vs. Training Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "- If your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "- If your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "\n",
    "### Approximate number of parameters\n",
    "\n",
    "The two most important parameters that control the model are n_hidden and n_layers. I would advise that you always use n_layers of either 2/3. The n_hidden can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "- The number of parameters in your model. This is printed when you start training.\n",
    "- The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "- I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make n_hidden larger.\n",
    "- I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "\n",
    "### Best models strategy\n",
    "\n",
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_traces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_next_step_prediction(test, input_cols=None, output_col=3, startIndex=1):\n",
    "    xs, ys = [], []\n",
    "    if input_cols == None: \n",
    "        input_cols=list(test)\n",
    "        \n",
    "    input_cols = listify(input_cols)\n",
    "    for trace in test.values:\n",
    "        for i in range(startIndex, len(listify(trace[0]))):\n",
    "            x, y = [], []\n",
    "            for c in range(len(input_cols)):\n",
    "                x.append(trace[c][:i])\n",
    "            \n",
    "            xs.append(x)\n",
    "            ys.append(trace[output_col][i])\n",
    "            \n",
    "    return pd.DataFrame(xs, columns=input_cols), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_step(model, df):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = []\n",
    "    for e in df.values:\n",
    "        t = torch.stack([tensor(e[c]).float() for c in range(len(list(df)))])\n",
    "        pred = model(t[None])\n",
    "        preds.append(pred[0][-1].tolist())\n",
    "        \n",
    "    return np.argmax(np.array(preds), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_measure(preds, ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    return (np.array(preds) == np.array(ys)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x, y = process_data_for_next_step_prediction(test_traces)\n",
    "preds = predict_next_step(awd_lstm, x)\n",
    "next_step_measure(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suffix Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_suffix_prediction(test, input_cols=None, output_col=3, startIndex=1):\n",
    "    xs, ys = [], []\n",
    "    if input_cols == None: \n",
    "        input_cols = list(test)\n",
    "        \n",
    "    input_cols = listify(input_cols)\n",
    "    for trace in test.values:\n",
    "        for i in range(startIndex, len(listify(trace[0]))):\n",
    "            x, y = [], []\n",
    "            for c in range(len(input_cols)):\n",
    "                x.append(trace[c][:i])\n",
    "            \n",
    "            xs.append(x)\n",
    "            ys.append(trace[output_col][i:])\n",
    "            \n",
    "    return pd.DataFrame(xs, columns=input_cols), ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_suffix(model, df):\n",
    "    rl = []\n",
    "\n",
    "    for x in progress_bar(df.values):\n",
    "        t = torch.stack([tensor(x[c]).float() for c in range(len(list(df)))])\n",
    "        p = tensor(-1)\n",
    "        res = []\n",
    "        \n",
    "        while p.int() != 3: # 3: eos token\n",
    "            pred = model(t[None])\n",
    "            pred = pred[0][-1]\n",
    "            p = torch.multinomial(torch.softmax(pred, 0), 1).float()\n",
    "           # p=torch.argmax(pred,0).float()[None]\n",
    "            if p.int() != 3 or len(res) == 0: \n",
    "                res.append(p)\n",
    "                \n",
    "            k = torch.cat((t[3],p))\n",
    "            t = torch.stack([k for c in range(len(list(df)))])\n",
    "\n",
    "\n",
    "        res = torch.cat(res,0).int().tolist()\n",
    "        rl.append(res)\n",
    "        \n",
    "    return rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_measure(preds, ys):\n",
    "    sim = []\n",
    "    edits = []\n",
    "    for p, y in zip(preds, ys):\n",
    "        l = max(len(p),len(y))\n",
    "        d = ed.eval(p, y)\n",
    "        edits.append(abs(d))\n",
    "        sim.append(1-(abs(d)/l))\n",
    "    return np.array(edits).mean(), np.array(edits).min(), np.array(edits).max(), np.array(sim).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preds = predict_suffix(awd_lstm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_edit, min_edit, max_edit, sim = suffix_measure(preds, y)\n",
    "mean_edit, min_edit, max_edit, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_suffix_prediction(test,cols=None,startIndex=1):\n",
    "    x, y = {}, {}\n",
    "    if cols == None: \n",
    "        cols = list(test)\n",
    "        \n",
    "    cols = listify(cols)\n",
    "    for col in cols:\n",
    "        x[col], y[col] = [], []\n",
    "        for trace in test[col]: \n",
    "            for i in range(startIndex,len(listify(trace))):\n",
    "                x[col].append(trace[:i])\n",
    "                y[col].append(trace[i:])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test_proc, cols='concept:name')\n",
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x['event_id']),len(y['event_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_suffix_prediction(test, cols=None, startIndex=1):\n",
    "    x, y = {}, {}\n",
    "    if cols == None: cols=list(test)\n",
    "    cols = listify(cols)\n",
    "    for col in cols:\n",
    "        x[col], y[col] = [], []\n",
    "        for trace in test.index: # test.index statt test[col]\n",
    "            x[col].append(trace)\n",
    "            y[col].append(trace)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_suffix_prediction(test_orig)\n",
    "list(zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_next_step_prediction(test, col='Activities', startIndex=1):\n",
    "    \n",
    "    x, y = [], []\n",
    "    for trace in test[col]:\n",
    "        for i in range(startIndex, len(trace)):\n",
    "            x.append(trace[:i])\n",
    "            y.append(trace[i])\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_next_step_prediction(test)\n",
    "list(zip(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suffix_measure(preds, ys):\n",
    "    \n",
    "    summe = 0.0\n",
    "    for p, y in zip(preds, ys):\n",
    "        l = len(p)\n",
    "        d = ed.eval(p, y)\n",
    "        sim = 1 - d/l\n",
    "        summe += sim\n",
    "    \n",
    "    return summe/len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step_measure(preds, ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    \n",
    "    return (np.array(preds) == np.array(ys)).astype(float).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_step(net, test, top_k=None, cuda=False):\n",
    "    \n",
    "    x, y = process_data_for_next_step_prediction(test, col='Activities', startIndex=1)\n",
    "\n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    h = net.init_hidden(1)\n",
    "    predictions = []\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    for value in x:\n",
    "        activities = []\n",
    "        \n",
    "        for i in range(len(value)):\n",
    "            activity, h = net.predict([value[i]], h, cuda=cuda, top_k=top_k)\n",
    "            \n",
    "        activities.append(activity)\n",
    "        activity, h = net.predict([activities[-1]], h, cuda=cuda, top_k=top_k)\n",
    "        predictions.append(activity)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = process_data_for_next_step_prediction(test)\n",
    "\n",
    "next_step_measure(preds, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_suffix(net, test, top_k=None, cuda=False):\n",
    "    \n",
    "    x, y = process_data_for_suffix_prediction(test, col='Activities', startIndex=1)\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    h = net.init_hidden(1)\n",
    "    predictions = []\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        # Get the size of the trace to be predicted\n",
    "        size = len(y[i])\n",
    "        value = x[i]\n",
    "        activities = []\n",
    "            \n",
    "        for j in range(len(value)):\n",
    "            activity, h = net.predict([value[j]], h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "        activities.append(activity)\n",
    "\n",
    "        for k in range(size-1):\n",
    "            activity, h = net.predict([activities[-1]], h, cuda=cuda, top_k=top_k)\n",
    "            activities.append(activity)\n",
    "\n",
    "        print(activities)\n",
    "\n",
    "        predictions.append(activities)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
