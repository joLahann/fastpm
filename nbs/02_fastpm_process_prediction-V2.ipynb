{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Prediction\n",
    "==\n",
    " - Load Data\n",
    " - Categorize / Normalize / Fillmissing\n",
    " - Create Datastructure for language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.eventlog import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.dl_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import editdistance as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log=import_xes(untar_data(URLs.BPIC_2012))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.traceAttributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Merge Trace Attributes and Event Attributes first in one df. It is easier to copy over the trace attributes \n",
    "2. Create Traces from DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.merge(log.events,log.traceAttributes,left_on='trace_id',right_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_traces(event_df,trace_id='trace_id'):\n",
    "    ll=[]\n",
    "    trace_ids=[]\n",
    "    cols=list(event_df)\n",
    "    cols.remove(trace_id)\n",
    "    for n, g in event_df.groupby(trace_id):\n",
    "        l=[]\n",
    "        \n",
    "        for c in cols:\n",
    "            l.append(list(g[c]))\n",
    "        ll.append(l)\n",
    "        trace_ids.append(n)  \n",
    "        \n",
    "\n",
    "    df=pd.DataFrame(ll,columns=cols)\n",
    "    df.index=trace_ids\n",
    "    return df\n",
    "traces=create_traces(df)\n",
    "traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=[[1,2],[3,4,5]]\n",
    "\n",
    "[item for sublist in k for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Split in Train, Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Split first only in train set and test set. The train set is used to train the model. The test set is used to test the model later on. Let the model create the validation set on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def random_split_traces(d,split=0.8,trace_id='trace_id'):\n",
    "    traces=d[trace_id].drop_duplicates()\n",
    "    shuffled=traces.iloc[np.random.permutation(len(traces))].values\n",
    "    split=int(len(traces)*split)\n",
    "    return shuffled[:split],shuffled[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train,test=random_split_traces(df,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TracesDatabunch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom data bunch class for traces. The data bunch includes the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data bunch can split the data into train set and validation set.\n",
    "- The data bunch can encode and decode the data. It keeps track about the encoding vocabulary, i.e. it creates the vocabulary while encoding the training set and applies the training vocabulary on the validation set and the test set.\n",
    "- The data bunch creates the data sets and the pytorch data loaders that are used in the training loop to train the pytorch models. It supports multiple training styles including language model training, suffix prediction training and next step prediction training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps to implement:\n",
    "--\n",
    "1. Get files -> Create custom TraceList\n",
    "2. Split validation set\n",
    "    - random \n",
    "3. Process Data:\n",
    "    - Dates, Continuous Variables, Categorical Variables\n",
    "4. Transform to tensor\n",
    "5. DataLoader\n",
    "6. DataBunch\n",
    "7. Add test set (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)\n",
    "    return x\n",
    "\n",
    "class ItemList(ListContainer):\n",
    "    def __init__(self, items, path='.', tfms=None):\n",
    "        super().__init__(items)\n",
    "        self.path,self.tfms = Path(path),tfms\n",
    "\n",
    "    def __repr__(self): return f'{super().__repr__()}\\nPath: {self.path}'\n",
    "    \n",
    "    def new(self, items, cls=None):\n",
    "        if cls is None: cls=self.__class__\n",
    "        return cls(items, self.path, tfms=self.tfms)\n",
    "    \n",
    "    def  get(self, i): return i\n",
    "    def _get(self, i): return compose(self.get(i), self.tfms)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        if isinstance(res,list): return [self._get(o) for o in res]\n",
    "        return self._get(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Trace List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ListContainer():\n",
    "    def __init__(self, items): self.items = listify(items)\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, (int,slice)): return self.items[idx]\n",
    "        if isinstance(idx[0],bool):\n",
    "            assert len(idx)==len(self) # bool mask\n",
    "            return [o for m,o in zip(idx,self.items) if m]\n",
    "        return [self.items[i] for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceList(ListContainer):\n",
    "    def __init__(self,items,df,cat_names,cont_names,date_names,trace_id):\n",
    "        super().__init__(items)\n",
    "        self.cat_names,self.cont_names,self.date_names = cat_names,cont_names,date_names\n",
    "        self.df=df\n",
    "        self.trace_id=trace_id\n",
    "   \n",
    "    @classmethod\n",
    "    def from_df(cls, df, date_names=[],cat_names=[], cont_names=[],trace_id='trace_id')->'TraceList':\n",
    "        \"Get the list of inputs in the `col` of `path/csv_name`.\"\n",
    "        return cls(items=list(df[trace_id].drop_duplicates()),df=df.copy(),date_names=date_names, cat_names=cat_names, cont_names=cont_names,trace_id=trace_id)\n",
    "    \n",
    "    def get(self, o): return self.df[self.df[self.trace_id].isin(listify(self.items[o]))]\n",
    "    \n",
    "    def _get(self, o): return self.df[self.df[self.trace_id].isin(listify(self.items[o]))].values\n",
    "    \n",
    "    def new(self, items, cls=None,df=None,cat_names=None,cont_names=None,date_names=None,trace_id=None):\n",
    "        if cls is None: cls=self.__class__\n",
    "        if cat_names is None: cat_names=self.cat_names\n",
    "        if cont_names is None: cont_names=self.cont_names\n",
    "        if date_names is None: date_names=self.date_names\n",
    "        if trace_id is None: trace_id=self.trace_id\n",
    "        if df is None: \n",
    "            print('hm')\n",
    "            df=self.df[self.df[self.trace_id].isin(items)]\n",
    "        return cls(items,df,cat_names,cont_names,date_names,trace_id)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx,list): return [self._get(o) for o in idx]\n",
    "        return self._get(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df[df['trace_id'].isin(train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data and create vocab\n",
    "cat_columns=['event_id','org:resource','lifecycle:transition','concept:name',]\n",
    "date_columns=['time:timestamp','REG_DATE']\n",
    "con_columns=['AMOUNT_REQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il=TraceList.from_df(data,cat_names=cat_columns,cont_names=con_columns,date_names=date_columns)\n",
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il.get(slice(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(il[[1,2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Split in Train Set and Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "def random_splitter(fn, p_valid): return random.random() < p_valid\n",
    "def split_by_func(items, f):\n",
    "    mask = [f(o) for o in items]\n",
    "    # `None` values will be filtered out\n",
    "    f = [o for o,m in zip(items,mask) if m==False]\n",
    "    t = [o for o,m in zip(items,mask) if m==True ]\n",
    "    return f,t\n",
    "\n",
    "class SplitData():\n",
    "    def __init__(self, train, valid): self.train,self.valid = train,valid\n",
    "    \n",
    "    @classmethod\n",
    "    def split_by_func(cls, il, f):\n",
    "        lists = map(il.new, split_by_func(il.items, f))\n",
    "        return cls(*lists)\n",
    "\n",
    "    def __repr__(self): return f'{self.__class__.__name__}\\nTrain: {self.train}\\nValid: {self.valid}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1));sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(sd.train.df),len(sd.valid.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#events_enc,traceAttributes_enc,cat_columns,num_columns,vocabs=encode_data(log.events,log.traceAttributes,cat_columns,date_columns,num_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datepart(df, fldname, drop=True, time=False,utc=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    df=df.copy()\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, utc=utc,infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    cols=[]\n",
    "    for n in attr: \n",
    "        col_name=targ_pre +\"_\"+ n\n",
    "        df[col_name] = getattr(fld.dt, n.lower())\n",
    "        cols.append(col_name)\n",
    "    df[targ_pre + '_Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "    return df,cols,targ_pre + '_Elapsed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_cont_column(x, mean, std,eps=1e-7): return (x-mean)/(eps + std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res\n",
    "\n",
    "class Processor():\n",
    "    def process(self, items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self,default_token=None): \n",
    "        self.vocab=None\n",
    "        self.default_token=default_token\n",
    "\n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            if self.default_token is not None:\n",
    "                for o in reversed(self.default_token):\n",
    "                    if o in self.vocab: self.vocab.remove(o)\n",
    "                    self.vocab.insert(0, o)\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return self.vocab[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceProcessor(Processor):\n",
    "    def __init__(self,vocabs={}):\n",
    "        self.vocabs=vocabs\n",
    "        self.df=tl.df\n",
    "        self.date_names=tl.date_names\n",
    "        self.cat_names=tl.cat_names\n",
    "        self.cont_names=tl.cont_names\n",
    "        self.tl=tl\n",
    "    def __call__(self,tl):\n",
    "        df=self.df\n",
    "        cat_names,cont_names=tl.cat_names,tl.cont_names\n",
    "        for d in tl.date_names:\n",
    "            df,cat, cont = add_datepart(df,d,utc=True)\n",
    "            cat_names+=listify(cat)    \n",
    "            cont_names+=listify(cont)    \n",
    "        for c in cat_names:\n",
    "            if not c in self.vocabs.keys(): \n",
    "                self.vocabs[c] = CategoricalProcessor(default_spec_tok)\n",
    "            df[c]=self.vocabs[c](df[c])\n",
    "            \n",
    "        for c in cont_names:\n",
    "            if not c in self.vocabs.keys(): \n",
    "                df[c]=df[c].astype(float)\n",
    "                self.vocabs[c]=df[c].mean(),df[c].std()\n",
    "            df[c]=normalize_cont_column(df[c], *self.vocabs[c])\n",
    "    \n",
    "        return self.tl.new(self.tl.items,df=df,cat_names=cat_names,cont_names=cont_names)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def deprocess(self,items,columns):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.train.cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=TraceProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed=tp(sd.train)\n",
    "valid_processed=tp(sd.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_processed.get(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(valid_processed),len(train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_data=df[df['trace_id'].isin(test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_tl=TraceList.from_df(test_data,cat_names=cat_columns,cont_names=con_columns,date_names=date_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_processed=tp(test_tl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_processed.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_traces=create_traces(test_processed.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_data_for_suffix_prediction(test,cols=None,startIndex=1):\n",
    "    x,y={},{}\n",
    "    if cols == None: cols=list(test)\n",
    "    cols=listify(cols)\n",
    "    for col in cols:\n",
    "        x[col],y[col]=[],[]\n",
    "        for trace in test[col]: \n",
    "            for i in range(startIndex,len(listify(trace))):\n",
    "                x[col].append(trace[:i])\n",
    "                y[col].append(trace[i:])\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y=process_data_for_suffix_prediction(test_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_data_for_next_step_prediction(test,col=None,startIndex=1):\n",
    "    x,y=[],[]\n",
    "    traces=test.values\n",
    "    if col!=None: traces=test[col].values\n",
    "    for trace in traces:\n",
    "        for i in range(startIndex,len(trace)):\n",
    "            x.append(flatten_ir_list(listify(trace[:i])))\n",
    "            y.append(flatten_ir_list(listify(trace[i])))\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def suffix_measure(preds,ys,col='concept:name'):\n",
    "    sum=0.0\n",
    "    for p,y in zip(preds[col],ys[col]):\n",
    "        l=len(p)\n",
    "        d=ed.eval(p,y)\n",
    "        sim=1-d/l\n",
    "        sum+=sim\n",
    "    return sum/len(preds[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def next_step_measure(preds,ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    return (np.array(preds)==np.array(ys)).astype(float).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
