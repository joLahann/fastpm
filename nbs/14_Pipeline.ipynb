{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from exp.eventlog import *\n",
    "from exp.dl_utils import *\n",
    "from collections import OrderedDict\n",
    "\n",
    "import editdistance as ed\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, merge=True):\n",
    "    path = untar_data(data)\n",
    "    log = import_xes(path, extensions=False, classifiers=False, schema=False, log_attributes=False)\n",
    "    if merge:\n",
    "        df = pd.merge(log.events, log.traceAttributes, left_on='trace_id', right_index=True)\n",
    "    else: \n",
    "        df = log.events()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 1970-01-01T00:00:00.000+01:00\n",
      "failed to parse date: 2012-04-23T00:00:00.000+02:00\n",
      "failed to parse date: 2011-10-01T00:38:44.546+02:00\n",
      "failed to parse date: 2012-03-14T16:04:54.681+01:00\n",
      "failed to parse date: 1970-01-01T00:00:00.000Z\n",
      "failed to parse date: 2016-01-01T09:51:15.304Z\n",
      "failed to parse date: 2017-02-01T23:00:00.000Z\n",
      "failed to parse date: 2017-02-01T14:11:03.499Z\n"
     ]
    }
   ],
   "source": [
    "df0 = load_data(URLs.BPIC_2012)\n",
    "#df1 = load_data(URLs.BPIC_2017)\n",
    "#df2 = load_data(URLs.BPIC_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trace_id</th>\n",
       "      <th>event_id</th>\n",
       "      <th>Action</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>EventOrigin</th>\n",
       "      <th>EventID</th>\n",
       "      <th>lifecycle:transition</th>\n",
       "      <th>time:timestamp</th>\n",
       "      <th>LoanGoal</th>\n",
       "      <th>ApplicationType</th>\n",
       "      <th>RequestedAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>0</td>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>A_Create Application</td>\n",
       "      <td>Application</td>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>complete</td>\n",
       "      <td>2016-01-01 09:51:15.304000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>1</td>\n",
       "      <td>statechange</td>\n",
       "      <td>User_1</td>\n",
       "      <td>A_Submitted</td>\n",
       "      <td>Application</td>\n",
       "      <td>ApplState_1582051990</td>\n",
       "      <td>complete</td>\n",
       "      <td>2016-01-01 09:51:15.352000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>2</td>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1298499574</td>\n",
       "      <td>schedule</td>\n",
       "      <td>2016-01-01 09:51:15.774000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>3</td>\n",
       "      <td>Deleted</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1673366067</td>\n",
       "      <td>withdraw</td>\n",
       "      <td>2016-01-01 09:52:36.392000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Application_652823628</td>\n",
       "      <td>4</td>\n",
       "      <td>Created</td>\n",
       "      <td>User_1</td>\n",
       "      <td>W_Complete application</td>\n",
       "      <td>Workflow</td>\n",
       "      <td>Workitem_1493664571</td>\n",
       "      <td>schedule</td>\n",
       "      <td>2016-01-01 09:52:36.403000+00:00</td>\n",
       "      <td>Existing loan takeover</td>\n",
       "      <td>New credit</td>\n",
       "      <td>20000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                trace_id  event_id       Action org:resource  \\\n",
       "0  Application_652823628         0      Created       User_1   \n",
       "1  Application_652823628         1  statechange       User_1   \n",
       "2  Application_652823628         2      Created       User_1   \n",
       "3  Application_652823628         3      Deleted       User_1   \n",
       "4  Application_652823628         4      Created       User_1   \n",
       "\n",
       "             concept:name  EventOrigin                EventID  \\\n",
       "0    A_Create Application  Application  Application_652823628   \n",
       "1             A_Submitted  Application   ApplState_1582051990   \n",
       "2          W_Handle leads     Workflow    Workitem_1298499574   \n",
       "3          W_Handle leads     Workflow    Workitem_1673366067   \n",
       "4  W_Complete application     Workflow    Workitem_1493664571   \n",
       "\n",
       "  lifecycle:transition                   time:timestamp  \\\n",
       "0             complete 2016-01-01 09:51:15.304000+00:00   \n",
       "1             complete 2016-01-01 09:51:15.352000+00:00   \n",
       "2             schedule 2016-01-01 09:51:15.774000+00:00   \n",
       "3             withdraw 2016-01-01 09:52:36.392000+00:00   \n",
       "4             schedule 2016-01-01 09:52:36.403000+00:00   \n",
       "\n",
       "                 LoanGoal ApplicationType  RequestedAmount  \n",
       "0  Existing loan takeover      New credit          20000.0  \n",
       "1  Existing loan takeover      New credit          20000.0  \n",
       "2  Existing loan takeover      New credit          20000.0  \n",
       "3  Existing loan takeover      New credit          20000.0  \n",
       "4  Existing loan takeover      New credit          20000.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting Data\n",
    "def random_split_traces(d, split=0.8, trace_id='trace_id'):\n",
    "    traces = d[trace_id].drop_duplicates()\n",
    "    shuffled = traces.iloc[np.random.permutation(len(traces))].values\n",
    "    split = int(len(traces) * split)\n",
    "    return shuffled[:split], shuffled[split:]\n",
    "\n",
    "def get_df(t, df): \n",
    "    return df[df[trace_id].isin(t)]\n",
    "\n",
    "# Processing Data\n",
    "def normalize_cont_column(x, mean, std, eps=1e-7): \n",
    "    return (x-mean)/(eps+std)\n",
    "\n",
    "#UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "#default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "#\n",
    "#cat_names = ['event_id', 'org:resource', 'lifecycle:transition', 'concept:name']\n",
    "##date_names = ['time:timestamp', 'REG_DATE']\n",
    "#date_names = ['time:timestamp']\n",
    "#cont_names = ['AMOUNT_REQ']\n",
    "\n",
    "def uniqueify(x, sort=False):\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res\n",
    "\n",
    "class Processor():\n",
    "    def process(self, items): return items\n",
    "\n",
    "class CategoryProcessor(Processor):\n",
    "    def __init__(self,default_token=None): \n",
    "        self.vocab = None\n",
    "        self.default_token = default_token\n",
    "\n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            self.vocab = uniqueify(items)\n",
    "            if self.default_token is not None:\n",
    "                for o in reversed(self.default_token):\n",
    "                    if o in self.vocab: self.vocab.remove(o)\n",
    "                    self.vocab.insert(0, o)\n",
    "            self.otoi = {v: k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    \n",
    "    def proc1(self, item): return self.otoi.get(item, 0)\n",
    "\n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    \n",
    "    def deproc1(self, idx): return self.vocab[idx]\n",
    "    \n",
    "def add_datepart(df, fldname, drop=True, time=False, utc=False):\n",
    "    \"Helper function that adds columns relevant to a date.\"\n",
    "    df = df.copy()\n",
    "    fld = df[fldname]\n",
    "    fld_dtype = fld.dtype\n",
    "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
    "        fld_dtype = np.datetime64\n",
    "\n",
    "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, utc=utc,infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
    "    cols=[]\n",
    "    for n in attr: \n",
    "        col_name=targ_pre +\"_\"+ n\n",
    "        df[col_name] = getattr(fld.dt, n.lower())\n",
    "        cols.append(col_name)\n",
    "    df[targ_pre + '_Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)\n",
    "    return df, cols, targ_pre + '_Elapsed'\n",
    "\n",
    "class TraceProcessor(Processor):\n",
    "    def __init__(self, cat_names, cont_names, date_names, vocabs={}):\n",
    "        self.vocabs = vocabs\n",
    "        self.cat_names, self.cont_names, self.date_names = cat_names, cont_names, date_names\n",
    "        \n",
    "    def __call__(self, df):\n",
    "        cat_names, cont_names = self.cat_names[:], self.cont_names[:]\n",
    "        for d in self.date_names:\n",
    "            df, cat, cont = add_datepart(df, d, utc=True)\n",
    "            cat_names += listify(cat)    \n",
    "            cont_names += listify(cont)\n",
    "\n",
    "        for c in cat_names:\n",
    "            if not c in self.vocabs.keys(): \n",
    "                self.vocabs[c] = CategoryProcessor(default_spec_tok)\n",
    "            df[c] = self.vocabs[c](df[c])\n",
    "            \n",
    "            \n",
    "        for c in cont_names:\n",
    "            df[c] = df[c].astype(float)\n",
    "\n",
    "            if not c in self.vocabs.keys(): \n",
    "                self.vocabs[c] = df[c].mean(), df[c].std()\n",
    "            df[c] = normalize_cont_column(df[c], *self.vocabs[c])\n",
    "    \n",
    "        return df\n",
    "    \n",
    "def create_traces(event_df, trace_id='trace_id'):\n",
    "    ll = []\n",
    "    trace_ids = []\n",
    "    cols = list(event_df)\n",
    "    cols.remove(trace_id)\n",
    "    for n, g in event_df.groupby(trace_id):\n",
    "        l = []\n",
    "        \n",
    "        for c in cols:\n",
    "            l.append(list(g[c]))\n",
    "        ll.append(l)\n",
    "        trace_ids.append(n)  \n",
    "        \n",
    "\n",
    "    df = pd.DataFrame(ll, columns=cols)\n",
    "    df.index = trace_ids\n",
    "    return df\n",
    "\n",
    "# Language Model DataLoader\n",
    "class LMDataSet():\n",
    "    def __init__(self, df, bs=64, bptt=70, shuffle=False):\n",
    "        self.bs, self.bptt, self.shuffle = bs, bptt, shuffle\n",
    "        self.cols = list(df)\n",
    "\n",
    "        total_len = sum(df.apply(lambda x: max([len(listify(x[k])) for k in self.cols]),axis=1))\n",
    "        self.n_batch = total_len // self.bs\n",
    "        self.batched = self.batchify(df)\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched[:, idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        x, y = source[:, seq_idx:seq_idx+self.bptt], source[:, seq_idx+1:seq_idx+self.bptt+1]\n",
    "        return x,y\n",
    "    \n",
    "    def batchify(self,df):\n",
    "        if self.shuffle: df=df.sample(frac=1)\n",
    "        \n",
    "        dd={}\n",
    "        for c in self.cols:\n",
    "            dd[c]=[]\n",
    "        for i, row in df.iterrows():\n",
    "            l = max([len(listify(row[c])) for c in self.cols])\n",
    "            for c in self.cols:\n",
    "                dd[c].append(tensor(row[c]).expand(l))\n",
    "        for c in self.cols:\n",
    "            s = torch.cat([torch.cat((tensor([2.0]), t.float(), tensor([3.0]))) for t in dd[c]])\n",
    "            dd[c] = s[:self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
    "        return torch.stack([dd[c] for c in self.cols])\n",
    "    \n",
    "def get_dls(train_ds, valid_ds,  **kwargs):\n",
    "    return (DataLoader(LMDataSet(train_ds, shuffle=True,bptt=bptt), batch_size=bs),\n",
    "            DataLoader(LMDataSet(valid_ds, shuffle=False,bptt=bptt), batch_size=bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.8 s, sys: 36.2 ms, total: 31.8 s\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trace_id = \"trace_id\"\n",
    "\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "\n",
    "cat_names = ['event_id', 'org:resource', 'lifecycle:transition', 'concept:name']\n",
    "date_names = ['time:timestamp', 'REG_DATE']\n",
    "cont_names = ['AMOUNT_REQ']\n",
    "\n",
    "# Split Data into train_indices and test_indices\n",
    "train_valid_ids, test_ids = random_split_traces(df0)\n",
    "\n",
    "# Get the Dataframe from the train_idices to split it into train and valid\n",
    "train_valid_df = get_df(train_valid_ids, df0)\n",
    "train_ids, valid_ids = random_split_traces(train_valid_df, split=0.9)\n",
    "\n",
    "# Get the Dataframe for train, valid and test\n",
    "train_df = get_df(train_ids, df0)\n",
    "valid_df = get_df(valid_ids, df0)\n",
    "test_df = get_df(test_ids, df0)\n",
    "\n",
    "# Encode Data\n",
    "tp = TraceProcessor(cat_names, cont_names, date_names)\n",
    "train_proc = tp(train_df)\n",
    "valid_proc = tp(valid_df)\n",
    "test_proc = tp(test_df)\n",
    "\n",
    "# Create Traces\n",
    "train_traces = create_traces(train_proc)\n",
    "valid_traces = create_traces(valid_proc)\n",
    "test_traces = create_traces(test_proc)\n",
    "\n",
    "# Create DataLoader\n",
    "bs, bptt = 128, 70\n",
    "data = DataBunch(*get_dls(train_traces, valid_traces))\n",
    "\n",
    "iter_dl = iter(data.train_dl)\n",
    "xb, yb = next(iter_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AWD-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div_(1-p)\n",
    "\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p=p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.: return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0'\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module,self.weight_p,self.layer_names = module,weight_p,layer_names\n",
    "        for layer in self.layer_names:\n",
    "            #Makes a copy of the weights of the selected layers.\n",
    "            w = getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)\n",
    "\n",
    "    def _setweights(self):\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            #To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)\n",
    "        \n",
    "class EmbeddingDropout(nn.Module):\n",
    "    \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb,self.embed_p = emb,embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "\n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p != 0:\n",
    "            size = (self.emb.weight.size(0),1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else: masked_embed = self.emb.weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n",
    "    \n",
    "def to_detach(h):\n",
    "    \"Detaches `h` from its history.\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)\n",
    "\n",
    "class AWD_LSTM(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input[:, 3].long()\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs=bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden,raw_outputs,outputs = [],[],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]\n",
    "        \n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs\n",
    "    \n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequential module that passes the reset call to its children.\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()\n",
    "                \n",
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, \n",
    "                       embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                       hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))\n",
    "\n",
    "def cross_entropy_activity(input, target):\n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.flatten().long())\n",
    "\n",
    "def accuracy_activity(input, target): \n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    return (torch.argmax(input.view(bs * sl, -1), dim=1)==target.flatten().long()).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip = clip\n",
    "    def after_backward(self):\n",
    "        if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)\n",
    "            \n",
    "class RNNTrainer(Callback):\n",
    "    def __init__(self, α, β): self.α,self.β = α,β\n",
    "    \n",
    "    def after_pred(self):\n",
    "        #Save the extra outputs for later and only returns the true output.\n",
    "        self.raw_out,self.out = self.pred[1],self.pred[2]\n",
    "        self.run.pred = self.pred[0]\n",
    "    \n",
    "    def after_loss(self):\n",
    "        #AR and TAR\n",
    "        if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean()\n",
    "        if self.β != 0.:\n",
    "            h = self.raw_out[-1]\n",
    "            if len(h)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "                \n",
    "    def begin_epoch(self):\n",
    "        pass\n",
    "        #Shuffle the texts at the beginning of the epoch\n",
    "        #if hasattr(self.dl.dataset, \"batchify\"): self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_activity</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_activity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.301018</td>\n",
       "      <td>0.381037</td>\n",
       "      <td>1.529319</td>\n",
       "      <td>0.552734</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.287843</td>\n",
       "      <td>0.602466</td>\n",
       "      <td>0.888631</td>\n",
       "      <td>0.747824</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.915237</td>\n",
       "      <td>0.708934</td>\n",
       "      <td>0.635054</td>\n",
       "      <td>0.795871</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.738355</td>\n",
       "      <td>0.771551</td>\n",
       "      <td>0.520768</td>\n",
       "      <td>0.825446</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.620418</td>\n",
       "      <td>0.802641</td>\n",
       "      <td>0.464158</td>\n",
       "      <td>0.843917</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.611407</td>\n",
       "      <td>0.806526</td>\n",
       "      <td>0.454764</td>\n",
       "      <td>0.845592</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.581666</td>\n",
       "      <td>0.811070</td>\n",
       "      <td>0.425296</td>\n",
       "      <td>0.845368</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.515097</td>\n",
       "      <td>0.830671</td>\n",
       "      <td>0.412761</td>\n",
       "      <td>0.853348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.536649</td>\n",
       "      <td>0.822226</td>\n",
       "      <td>0.405276</td>\n",
       "      <td>0.853516</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.502272</td>\n",
       "      <td>0.831580</td>\n",
       "      <td>0.397009</td>\n",
       "      <td>0.854129</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.499746</td>\n",
       "      <td>0.831213</td>\n",
       "      <td>0.393894</td>\n",
       "      <td>0.856473</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.492214</td>\n",
       "      <td>0.833312</td>\n",
       "      <td>0.389275</td>\n",
       "      <td>0.857645</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.503548</td>\n",
       "      <td>0.828508</td>\n",
       "      <td>0.385976</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.478812</td>\n",
       "      <td>0.838313</td>\n",
       "      <td>0.382414</td>\n",
       "      <td>0.858705</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.447104</td>\n",
       "      <td>0.844053</td>\n",
       "      <td>0.380402</td>\n",
       "      <td>0.859989</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.467479</td>\n",
       "      <td>0.837638</td>\n",
       "      <td>0.378254</td>\n",
       "      <td>0.860212</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.494879</td>\n",
       "      <td>0.833062</td>\n",
       "      <td>0.377673</td>\n",
       "      <td>0.860435</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.474243</td>\n",
       "      <td>0.833955</td>\n",
       "      <td>0.375907</td>\n",
       "      <td>0.855859</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.449634</td>\n",
       "      <td>0.838977</td>\n",
       "      <td>0.373375</td>\n",
       "      <td>0.860714</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.437954</td>\n",
       "      <td>0.845573</td>\n",
       "      <td>0.373461</td>\n",
       "      <td>0.860100</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Dropout-mask\n",
    "x = torch.randn(10,10)\n",
    "mask = dropout_mask(x, (10,10), 0.5) # display((x*mask).std(), x.std())\n",
    "# dp = RNNDropout(0.3)\n",
    "\n",
    "padding_index = 1\n",
    "vocab_size = len(tp.vocabs[\"concept:name\"].vocab)\n",
    "emb_sz, nh, nl = 300, 300, 2\n",
    "\n",
    "model = get_language_model(vocab_size, emb_sz, nh, nl, padding_index, input_p=0.6, output_p=0.4, weight_p=0.5, \n",
    "                           embed_p=0.1, hidden_p=0.2)\n",
    "\n",
    "cbs = [partial(AvgStatsCallback,accuracy_activity),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       partial(RNNTrainer, α=2., β=1.),\n",
    "       ProgressBarCallback]\n",
    "\n",
    "learn0 = Learner(model, data, cross_entropy_activity, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())\n",
    "learn0.fit(20)\n",
    "\n",
    "awd_lstm = learn0.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self, n_in, n_out, n_emb,nh):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(n_in, 7, padding_idx=1)\n",
    "        self.lin1 = nn.Linear(7, nh)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(nh, n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = x[:, 3] # magic number for 'concept:name'\n",
    "        x = x.long()\n",
    "        x = self.emb(x)\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        return x.float()\n",
    "    \n",
    "def getBasicModel():\n",
    "    vocab = len((tp.vocabs['concept:name']).vocab) # Stupid 'concept:name' model\n",
    "    n_emb, nh = int(vocab/2), 10\n",
    "    model = BasicModel(bs*bptt, vocab, n_emb, nh)\n",
    "    return model\n",
    "\n",
    "def cross_entropy_activity(input, target):\n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs * sl, -1), target.flatten().long())\n",
    "\n",
    "def accuracy_activity(input, target): \n",
    "    target = target[:, 3] # magic number for 'concept:name'\n",
    "    bs, sl = target.size()\n",
    "    return (torch.argmax(input.view(bs * sl, -1), dim=1) == target.flatten().long()).float().mean()\n",
    "\n",
    "class CudaCallback(Callback):\n",
    "    def begin_fit(self): self.model.cuda()\n",
    "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(), self.yb.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_activity</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_activity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.834084</td>\n",
       "      <td>0.256415</td>\n",
       "      <td>2.335463</td>\n",
       "      <td>0.400558</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.123303</td>\n",
       "      <td>0.457701</td>\n",
       "      <td>1.935490</td>\n",
       "      <td>0.497712</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.801295</td>\n",
       "      <td>0.495387</td>\n",
       "      <td>1.658341</td>\n",
       "      <td>0.556083</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.540730</td>\n",
       "      <td>0.614031</td>\n",
       "      <td>1.414832</td>\n",
       "      <td>0.630748</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.339196</td>\n",
       "      <td>0.629969</td>\n",
       "      <td>1.265034</td>\n",
       "      <td>0.638895</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.238882</td>\n",
       "      <td>0.636108</td>\n",
       "      <td>1.217808</td>\n",
       "      <td>0.638895</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.183578</td>\n",
       "      <td>0.636581</td>\n",
       "      <td>1.156898</td>\n",
       "      <td>0.638895</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.150111</td>\n",
       "      <td>0.636442</td>\n",
       "      <td>1.132803</td>\n",
       "      <td>0.639509</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.117328</td>\n",
       "      <td>0.642932</td>\n",
       "      <td>1.099438</td>\n",
       "      <td>0.653571</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.098077</td>\n",
       "      <td>0.650648</td>\n",
       "      <td>1.085022</td>\n",
       "      <td>0.652958</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.072290</td>\n",
       "      <td>0.654858</td>\n",
       "      <td>1.061512</td>\n",
       "      <td>0.657533</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.059116</td>\n",
       "      <td>0.656649</td>\n",
       "      <td>1.050187</td>\n",
       "      <td>0.657087</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.043685</td>\n",
       "      <td>0.657685</td>\n",
       "      <td>1.039056</td>\n",
       "      <td>0.657422</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.034825</td>\n",
       "      <td>0.657860</td>\n",
       "      <td>1.031099</td>\n",
       "      <td>0.657868</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.027737</td>\n",
       "      <td>0.658567</td>\n",
       "      <td>1.024839</td>\n",
       "      <td>0.660324</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.022081</td>\n",
       "      <td>0.659699</td>\n",
       "      <td>1.019892</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.017557</td>\n",
       "      <td>0.660332</td>\n",
       "      <td>1.015903</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.013889</td>\n",
       "      <td>0.660422</td>\n",
       "      <td>1.012618</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.010823</td>\n",
       "      <td>0.660401</td>\n",
       "      <td>1.009819</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.008172</td>\n",
       "      <td>0.660172</td>\n",
       "      <td>1.007315</td>\n",
       "      <td>0.660826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = getBasicModel()\n",
    "pred = model(xb)\n",
    "\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) \n",
    "cbfs = [partial(AvgStatsCallback,accuracy_activity),\n",
    "        CudaCallback, \n",
    "        Recorder,\n",
    "        partial(ParamScheduler, 'lr', sched),\n",
    "        ProgressBarCallback]\n",
    "\n",
    "opt_func = partial(Optimizer, steppers=[sgd_step])\n",
    "opt = opt_func(model.parameters(), lr=0.5)\n",
    "\n",
    "learn1 = Learner(model, data, cross_entropy_activity, cb_funcs=cbfs, opt_func=opt_func)\n",
    "learn1.fit(20)\n",
    "\n",
    "basic_model = learn1.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: [1.0081724439348494, tensor(0.6602, device='cuda:0')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn1.avg_stats.train_stats\n",
    "learn1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_for_next_step_prediction(test, input_cols=None, output_col=3, startIndex=1):\n",
    "    xs, ys = [], []\n",
    "    if input_cols == None: \n",
    "        input_cols=list(test)\n",
    "    \n",
    "    i = 0\n",
    "    input_cols = listify(input_cols)\n",
    "    for trace in test.values:\n",
    "        for i in range(startIndex, len(listify(trace[0]))):\n",
    "            x, y = [], []\n",
    "            for c in range(len(input_cols)):\n",
    "                x.append(trace[c][:i])\n",
    "                \n",
    "            xs.append(x)\n",
    "            ys.append(trace[output_col][i])\n",
    "            \n",
    "    return pd.DataFrame(xs, columns=input_cols), ys\n",
    "\n",
    "def pad_collate(samples, pad_idx=1, pad_first=True):\n",
    "    columns = list(samples)\n",
    "    values = []\n",
    "    \n",
    "    for col in columns:\n",
    "        max_len = max([len(s) for s in samples[col]])\n",
    "        res = torch.zeros(len(samples[col]), max_len).long() + pad_idx\n",
    "    \n",
    "        for i, s in enumerate(samples[col]):\n",
    "            if pad_first: res[i, -len(s):] = torch.LongTensor(s)\n",
    "            else:         res[i, :len(s) ] = torch.LongTensor(s)\n",
    "        values.append(res)\n",
    "\n",
    "    return values\n",
    "\n",
    "def predict_next_step(model, df):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = []\n",
    "    for e in df.values:\n",
    "        t = torch.stack([tensor(e[c]).float() for c in range(len(list(df)))])\n",
    "        pred = model(t[None])\n",
    "        preds.append(pred[0][-1].tolist())\n",
    "        \n",
    "    return np.argmax(np.array(preds), axis=1)\n",
    "\n",
    "def next_step_measure(preds, ys):\n",
    "    # Simple accuracy measure\n",
    "    # Do I have to weight it? Check Paper!\n",
    "    return (np.array(preds) == np.array(ys)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x, y = process_data_for_next_step_prediction(test_traces)\n",
    "xpad = pad_collate(x, pad_first=True)\n",
    "\n",
    "#preds0 = predict_next_step(awd_lstm, x)\n",
    "#next_step_measure(preds0, y)\n",
    "\n",
    "#preds1 = predict_next_step(basic_model, x)\n",
    "#next_step_measure(preds1, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
