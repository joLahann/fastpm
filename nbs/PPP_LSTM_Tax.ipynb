{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Tax et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/verenich/ProcessSequencePrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divisor: 131369.88879068676\n",
      "divisor2: 852193.9671329734\n",
      "['¤¦¦¦§', '¤¤¦¦¦¦§§§', '¤¤¤¤¦¦¦¦¦¦¦¦¦¦¦§', '¤¤', '¢¤¤¤', '¤¤¤¤', '¢¤¤¤', '¤¤¦¦§', '¤¦¦¦¦¦¦§¥¥¥¥¥¥¥¥¥¥¥§¥¥¥¥¥§', '¤¦¦¦¦§¥¥¥¥']\n",
      "[[0, 1915, 620092, 154865, 255872], [0, 10355, 37, 181196, 55, 580649, 4569, 9799, 201], [0, 171923, 7881, 454, 244, 615777, 10038, 340926, 264435, 659211, 115174, 237067, 270846, 8727, 57256, 197383], [0, 4908], [0, 5605, 221, 11167], [0, 4555, 799211, 1873915], [0, 10390, 166434, 12326], [0, 2658, 619673, 159347, 252684], [0, 15, 118, 204, 14, 625263, 152610, 265265, 1770, 2854, 11925, 132, 13, 165785, 5923, 160597, 3345, 94668, 3610, 54407, 1420, 438, 86773, 366863, 146863, 90323], [0, 122, 602, 270885, 154928, 346582, 835, 4948, 9582, 7575]]\n",
      "[[0, 1915, 622007, 776872, 1032744], [0, 10355, 10392, 191588, 191643, 772292, 776861, 786660, 786861], [0, 171923, 179804, 180258, 180502, 796279, 806317, 1147243, 1411678, 2070889, 2186063, 2423130, 2693976, 2702703, 2759959, 2957342], [0, 4908], [0, 5605, 5826, 16993], [0, 4555, 803766, 2677681], [0, 10390, 176824, 189150], [0, 2658, 622331, 781678, 1034362], [0, 15, 133, 337, 351, 625614, 778224, 1043489, 1045259, 1048113, 1060038, 1060170, 1060183, 1225968, 1231891, 1392488, 1395833, 1490501, 1494111, 1548518, 1549938, 1550376, 1637149, 2004012, 2150875, 2241198], [0, 122, 724, 271609, 426537, 773119, 773954, 778902, 788484, 796059]]\n",
      "total chars: 6, target chars: 7\n",
      "{0: '¢', 1: '£', 2: '¤', 3: '¥', 4: '¦', 5: '§'}\n",
      "nb sequences: 49211\n",
      "Vectorization...\n",
      "num features: 11\n",
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "this script trains an LSTM model on one of the data files in the data folder of\n",
    "this repository. the input file can be changed to another file from the data folder\n",
    "by changing its name in line 46.\n",
    "it is recommended to run this script on GPU, as recurrent networks are quite \n",
    "computationally intensive.\n",
    "Author: Niek Tax\n",
    "'''\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Input\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from collections import Counter\n",
    "import unicodecsv\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "#from itertools import izip\n",
    "from datetime import datetime\n",
    "from math import log\n",
    "\n",
    "\n",
    "#eventlog = \"helpdesk.csv\"\n",
    "eventlog = \"BPI_2012.csv\"\n",
    "\n",
    "########################################################################################\n",
    "#\n",
    "# this part of the code opens the file, reads it into three following variables\n",
    "#\n",
    "\n",
    "lines = [] #these are all the activity seq\n",
    "timeseqs = [] #time sequences (differences between two events)\n",
    "timeseqs2 = [] #time sequences (differences between the current and first)\n",
    "\n",
    "#helper variables\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "\n",
    "csvfile = open('./{}'.format(eventlog), 'r')\n",
    "#csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "\n",
    "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\n",
    "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "\n",
    "\n",
    "########################################\n",
    "\n",
    "divisor = np.mean([item for sublist in timeseqs for item in sublist]) #average time between events\n",
    "print('divisor: {}'.format(divisor))\n",
    "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist]) #average time between current and first events\n",
    "print('divisor2: {}'.format(divisor2))\n",
    "\n",
    "#########################################################################################################\n",
    "\n",
    "# separate training data into 3 parts\n",
    "\n",
    "elems_per_fold = int(round(numlines/3))\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "\n",
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "\n",
    "#leave away fold3 for now\n",
    "lines = fold1 + fold2\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "\n",
    "print(lines[:10])\n",
    "print(lines_t[:10])\n",
    "print(lines_t2[:10])\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = list(map(lambda x: x+'!',lines)) #put delimiter symbol\n",
    "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\n",
    "\n",
    "# next lines here to get all possible characters for events and annotate them with numbers\n",
    "chars = list(map(lambda x: set(x),lines))\n",
    "chars = list(set().union(*chars))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)\n",
    "\n",
    "\n",
    "#csvfile = open('../data/%s' % eventlog, 'r')\n",
    "csvfile = open('./{}'.format(eventlog), 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "timeseqs = []\n",
    "timeseqs2 = []\n",
    "timeseqs3 = []\n",
    "timeseqs4 = []\n",
    "times = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:\n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "            timeseqs3.append(times3)\n",
    "            timeseqs4.append(times4)\n",
    "        line = ''\n",
    "        times = []\n",
    "        times2 = []\n",
    "        times3 = []\n",
    "        times4 = []\n",
    "        numlines+=1\n",
    "    line+=chr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\n",
    "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    times3.append(timediff3)\n",
    "    times4.append(timediff4)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "timeseqs3.append(times3)\n",
    "timeseqs4.append(times4)\n",
    "numlines+=1\n",
    "\n",
    "elems_per_fold = int(round(numlines/3))\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "fold1_t3 = timeseqs3[:elems_per_fold]\n",
    "fold1_t4 = timeseqs4[:elems_per_fold]\n",
    "with open('./fold1.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row, timeseq in zip(fold1, fold1_t):\n",
    "        spamwriter.writerow([str(s) +'#{}'.format(t) for s, t in zip(row, timeseq)]) # str was unicode(s).encode(\"utf-8\")\n",
    "\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t3 = timeseqs3[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t4 = timeseqs4[elems_per_fold:2*elems_per_fold]\n",
    "with open('./fold2.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row, timeseq in zip(fold2, fold2_t):\n",
    "        spamwriter.writerow([str(s) +'#{}'.format(t) for s, t in zip(row, timeseq)])\n",
    "\n",
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
    "fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
    "with open('./fold3.csv', 'w') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row, timeseq in zip(fold3, fold3_t):\n",
    "        spamwriter.writerow([str(s) +'#{}'.format(t) for s, t in zip(row, timeseq)])\n",
    "\n",
    "lines = fold1 + fold2\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "lines_t3 = fold1_t3 + fold2_t3\n",
    "lines_t4 = fold1_t4 + fold2_t4\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = map(lambda x: x+'!',lines)\n",
    "\n",
    "sentences_t = []\n",
    "sentences_t2 = []\n",
    "sentences_t3 = []\n",
    "sentences_t4 = []\n",
    "next_chars_t = []\n",
    "next_chars_t2 = []\n",
    "next_chars_t3 = []\n",
    "next_chars_t4 = []\n",
    "for line, line_t, line_t2, line_t3, line_t4 in zip(lines, lines_t, lines_t2, lines_t3, lines_t4):\n",
    "    for i in range(0, len(line), step):\n",
    "        if i==0:\n",
    "            continue\n",
    "\n",
    "        #we add iteratively, first symbol of the line, then two first, three...\n",
    "\n",
    "        sentences.append(line[0: i])\n",
    "        sentences_t.append(line_t[0:i])\n",
    "        sentences_t2.append(line_t2[0:i])\n",
    "        sentences_t3.append(line_t3[0:i])\n",
    "        sentences_t4.append(line_t4[0:i])\n",
    "        next_chars.append(line[i])\n",
    "        if i==len(line)-1: # special case to deal time of end character\n",
    "            next_chars_t.append(0)\n",
    "            next_chars_t2.append(0)\n",
    "            next_chars_t3.append(0)\n",
    "            next_chars_t4.append(0)\n",
    "        else:\n",
    "            next_chars_t.append(line_t[i])\n",
    "            next_chars_t2.append(line_t2[i])\n",
    "            next_chars_t3.append(line_t3[i])\n",
    "            next_chars_t4.append(line_t4[i])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "num_features = len(chars)+5 # warum???\n",
    "print('num features: {}'.format(num_features))\n",
    "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\n",
    "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\n",
    "y_t = np.zeros((len(sentences)), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    leftpad = maxlen-len(sentence)\n",
    "    next_t = next_chars_t[i]\n",
    "    sentence_t = sentences_t[i]\n",
    "    sentence_t2 = sentences_t2[i]\n",
    "    sentence_t3 = sentences_t3[i]\n",
    "    sentence_t4 = sentences_t4[i]\n",
    "    for t, char in enumerate(sentence):\n",
    "        multiset_abstraction = Counter(sentence[:t+1])\n",
    "        for c in chars:\n",
    "            if c==char: #this will encode present events to the right places\n",
    "                X[i, t+leftpad, char_indices[c]] = 1\n",
    "        X[i, t+leftpad, len(chars)] = t+1\n",
    "        X[i, t+leftpad, len(chars)+1] = sentence_t[t]/divisor\n",
    "        X[i, t+leftpad, len(chars)+2] = sentence_t2[t]/divisor2\n",
    "        X[i, t+leftpad, len(chars)+3] = sentence_t3[t]/86400\n",
    "        X[i, t+leftpad, len(chars)+4] = sentence_t4[t]/7\n",
    "    for c in target_chars:\n",
    "        if c==next_chars[i]:\n",
    "            y_a[i, target_char_indices[c]] = 1-softness\n",
    "        else:\n",
    "            y_a[i, target_char_indices[c]] = softness/(len(target_chars)-1)\n",
    "    y_t[i] = next_t/divisor\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    \n",
    "print(X[:10]) # , y_a[:10], y_t[:10])\n",
    "\n",
    "# build the model: \n",
    "print('Build model...')\n",
    "main_input = Input(shape=(maxlen, num_features), name='main_input')\n",
    "# train a 2-layer LSTM with one shared layer\n",
    "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\n",
    "b1 = BatchNormalization()(l1)\n",
    "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\n",
    "b2_1 = BatchNormalization()(l2_1)\n",
    "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\n",
    "b2_2 = BatchNormalization()(l2_2)\n",
    "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\n",
    "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\n",
    "\n",
    "model = Model(inputs=[main_input], outputs=[act_output, time_output])\n",
    "\n",
    "opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "\n",
    "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\n",
    "#model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "model_checkpoint = ModelCheckpoint('./output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "#model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this script takes as input the LSTM or RNN weights found by train.py\n",
    "change the path in line 176 of this script to point to the h5 file\n",
    "with LSTM or RNN weights generated by train.py\n",
    "Author: Niek Tax\n",
    "'''\n",
    "\n",
    "from __future__ import division\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "import copy\n",
    "import numpy as np\n",
    "import distance\n",
    "#from itertools import izip\n",
    "from jellyfish._jellyfish import damerau_levenshtein_distance\n",
    "import unicodecsv\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "eventlog = \"helpdesk.csv\"\n",
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "ascii_offset = 161\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "caseids = []\n",
    "timeseqs = []\n",
    "timeseqs2 = []\n",
    "times = []\n",
    "times2 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        caseids.append(row[0])\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:        \n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "        line = ''\n",
    "        times = []\n",
    "        numlines+=1\n",
    "    line+=unichr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "numlines+=1\n",
    "\n",
    "divisor = np.mean([item for sublist in timeseqs for item in sublist])\n",
    "print('divisor: {}'.format(divisor))\n",
    "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist])\n",
    "print('divisor2: {}'.format(divisor2))\n",
    "\n",
    "elems_per_fold = int(round(numlines/3))\n",
    "fold1 = lines[:elems_per_fold]\n",
    "fold1_c = caseids[:elems_per_fold]\n",
    "fold1_t = timeseqs[:elems_per_fold]\n",
    "fold1_t2 = timeseqs2[:elems_per_fold]\n",
    "\n",
    "fold2 = lines[elems_per_fold:2*elems_per_fold]\n",
    "fold2_c = caseids[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\n",
    "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\n",
    "\n",
    "lines = fold1 + fold2\n",
    "caseids = fold1_c + fold2_c\n",
    "lines_t = fold1_t + fold2_t\n",
    "lines_t2 = fold1_t2 + fold2_t2\n",
    "\n",
    "step = 1\n",
    "sentences = []\n",
    "softness = 0\n",
    "next_chars = []\n",
    "lines = map(lambda x: x+'!',lines)\n",
    "maxlen = max(map(lambda x: len(x),lines))\n",
    "\n",
    "chars = map(lambda x : set(x),lines)\n",
    "chars = list(set().union(*chars))\n",
    "chars.sort()\n",
    "target_chars = copy.copy(chars)\n",
    "chars.remove('!')\n",
    "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\n",
    "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\n",
    "print(indices_char)\n",
    "\n",
    "\n",
    "lastcase = ''\n",
    "line = ''\n",
    "firstLine = True\n",
    "lines = []\n",
    "caseids = []\n",
    "timeseqs = []  # relative time since previous event\n",
    "timeseqs2 = [] # relative time since case start\n",
    "timeseqs3 = [] # absolute time of previous event\n",
    "times = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "numlines = 0\n",
    "casestarttime = None\n",
    "lasteventtime = None\n",
    "csvfile = open('../data/%s' % eventlog, 'r')\n",
    "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "next(spamreader, None)  # skip the headers\n",
    "for row in spamreader:\n",
    "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\n",
    "    if row[0]!=lastcase:\n",
    "        caseids.append(row[0])\n",
    "        casestarttime = t\n",
    "        lasteventtime = t\n",
    "        lastcase = row[0]\n",
    "        if not firstLine:        \n",
    "            lines.append(line)\n",
    "            timeseqs.append(times)\n",
    "            timeseqs2.append(times2)\n",
    "            timeseqs3.append(times3)\n",
    "        line = ''\n",
    "        times = []\n",
    "        numlines+=1\n",
    "    line+=unichr(int(row[1])+ascii_offset)\n",
    "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\n",
    "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\n",
    "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\n",
    "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\n",
    "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\n",
    "    #timediff = log(timediff+1)\n",
    "    times.append(timediff)\n",
    "    times2.append(timediff2)\n",
    "    times3.append(datetime.fromtimestamp(time.mktime(t)))\n",
    "    lasteventtime = t\n",
    "    firstLine = False\n",
    "\n",
    "# add last case\n",
    "lines.append(line)\n",
    "timeseqs.append(times)\n",
    "timeseqs2.append(times2)\n",
    "timeseqs3.append(times3)\n",
    "numlines+=1\n",
    "\n",
    "fold3 = lines[2*elems_per_fold:]\n",
    "fold3_c = caseids[2*elems_per_fold:]\n",
    "fold3_t = timeseqs[2*elems_per_fold:]\n",
    "fold3_t2 = timeseqs2[2*elems_per_fold:]\n",
    "fold3_t3 = timeseqs3[2*elems_per_fold:]\n",
    "#fold3_t4 = timeseqs4[2*elems_per_fold:]\n",
    "\n",
    "lines = fold3\n",
    "caseids = fold3_c\n",
    "lines_t = fold3_t\n",
    "lines_t2 = fold3_t2\n",
    "lines_t3 = fold3_t3\n",
    "#lines_t4 = fold1_t4 + fold2_t4\n",
    "\n",
    "# set parameters\n",
    "predict_size = 1\n",
    "\n",
    "# load model, set this to the model generated by train.py\n",
    "model = load_model('output_files/models/model_89-1.50.h5')\n",
    "\n",
    "# define helper functions\n",
    "def encode(sentence, times, times3, maxlen=maxlen):\n",
    "    num_features = len(chars)+5\n",
    "    X = np.zeros((1, maxlen, num_features), dtype=np.float32)\n",
    "    leftpad = maxlen-len(sentence)\n",
    "    times2 = np.cumsum(times)\n",
    "    for t, char in enumerate(sentence):\n",
    "        midnight = times3[t].replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        timesincemidnight = times3[t]-midnight\n",
    "        multiset_abstraction = Counter(sentence[:t+1])\n",
    "        for c in chars:\n",
    "            if c==char:\n",
    "                X[0, t+leftpad, char_indices[c]] = 1\n",
    "        X[0, t+leftpad, len(chars)] = t+1\n",
    "        X[0, t+leftpad, len(chars)+1] = times[t]/divisor\n",
    "        X[0, t+leftpad, len(chars)+2] = times2[t]/divisor2\n",
    "        X[0, t+leftpad, len(chars)+3] = timesincemidnight.seconds/86400\n",
    "        X[0, t+leftpad, len(chars)+4] = times3[t].weekday()/7\n",
    "    return X\n",
    "\n",
    "def getSymbol(predictions):\n",
    "    maxPrediction = 0\n",
    "    symbol = ''\n",
    "    i = 0;\n",
    "    for prediction in predictions:\n",
    "        if(prediction>=maxPrediction):\n",
    "            maxPrediction = prediction\n",
    "            symbol = target_indices_char[i]\n",
    "        i += 1\n",
    "    return symbol\n",
    "\n",
    "one_ahead_gt = []\n",
    "one_ahead_pred = []\n",
    "\n",
    "two_ahead_gt = []\n",
    "two_ahead_pred = []\n",
    "\n",
    "three_ahead_gt = []\n",
    "three_ahead_pred = []\n",
    "\n",
    "\n",
    "# make predictions\n",
    "with open('output_files/results/next_activity_and_time_%s' % eventlog, 'wb') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([\"CaseID\", \"Prefix length\", \"Groud truth\", \"Predicted\", \"Levenshtein\", \"Damerau\", \"Jaccard\", \"Ground truth times\", \"Predicted times\", \"RMSE\", \"MAE\"])\n",
    "    for prefix_size in range(2,maxlen):\n",
    "        print(prefix_size)\n",
    "        for line, caseid, times, times3 in zip(lines, caseids, lines_t, lines_t3):\n",
    "            times.append(0)\n",
    "            cropped_line = ''.join(line[:prefix_size])\n",
    "            cropped_times = times[:prefix_size]\n",
    "            cropped_times3 = times3[:prefix_size]\n",
    "            if '!' in cropped_line:\n",
    "                continue # make no prediction for this case, since this case has ended already\n",
    "            ground_truth = ''.join(line[prefix_size:prefix_size+predict_size])\n",
    "            ground_truth_t = times[prefix_size:prefix_size+predict_size]\n",
    "            predicted = ''\n",
    "            predicted_t = []\n",
    "            for i in range(predict_size):\n",
    "                if len(ground_truth)<=i:\n",
    "                    continue\n",
    "                enc = encode(cropped_line, cropped_times, cropped_times3)\n",
    "                y = model.predict(enc, verbose=0)\n",
    "                y_char = y[0][0]\n",
    "                y_t = y[1][0][0]\n",
    "                prediction = getSymbol(y_char)              \n",
    "                cropped_line += prediction\n",
    "                if y_t<0:\n",
    "                    y_t=0\n",
    "                cropped_times.append(y_t)\n",
    "                y_t = y_t * divisor\n",
    "                cropped_times3.append(cropped_times3[-1] + timedelta(seconds=y_t))\n",
    "                predicted_t.append(y_t)\n",
    "                if i==0:\n",
    "                    if len(ground_truth_t)>0:\n",
    "                        one_ahead_pred.append(y_t)\n",
    "                        one_ahead_gt.append(ground_truth_t[0])\n",
    "                if i==1:\n",
    "                    if len(ground_truth_t)>1:\n",
    "                        two_ahead_pred.append(y_t)\n",
    "                        two_ahead_gt.append(ground_truth_t[1])\n",
    "                if i==2:\n",
    "                    if len(ground_truth_t)>2:\n",
    "                        three_ahead_pred.append(y_t)\n",
    "                        three_ahead_gt.append(ground_truth_t[2])\n",
    "                if prediction == '!': # end of case was just predicted, therefore, stop predicting further into the future\n",
    "                    print('! predicted, end case')\n",
    "                    break\n",
    "                predicted += prediction\n",
    "            output = []\n",
    "            if len(ground_truth)>0:\n",
    "                output.append(caseid)\n",
    "                output.append(prefix_size)\n",
    "                output.append(unicode(ground_truth).encode(\"utf-8\"))\n",
    "                output.append(unicode(predicted).encode(\"utf-8\"))\n",
    "                output.append(1 - distance.nlevenshtein(predicted, ground_truth))\n",
    "                dls = 1 - (damerau_levenshtein_distance(unicode(predicted), unicode(ground_truth)) / max(len(predicted),len(ground_truth)))\n",
    "                if dls<0:\n",
    "                    dls=0 # we encountered problems with Damerau-Levenshtein Similarity on some linux machines where the default character encoding of the operating system caused it to be negative, this should never be the case\n",
    "                output.append(dls)\n",
    "                output.append(1 - distance.jaccard(predicted, ground_truth))\n",
    "                output.append('; '.join(str(x) for x in ground_truth_t))\n",
    "                output.append('; '.join(str(x) for x in predicted_t))\n",
    "                if len(predicted_t)>len(ground_truth_t): # if predicted more events than length of case, only use needed number of events for time evaluation\n",
    "                    predicted_t = predicted_t[:len(ground_truth_t)]\n",
    "                if len(ground_truth_t)>len(predicted_t): # if predicted less events than length of case, put 0 as placeholder prediction\n",
    "                    predicted_t.extend(range(len(ground_truth_t)-len(predicted_t)))\n",
    "                if len(ground_truth_t)>0 and len(predicted_t)>0:\n",
    "                    output.append('')\n",
    "                    output.append(metrics.mean_absolute_error([ground_truth_t[0]], [predicted_t[0]]))\n",
    "                    #output.append(metrics.median_absolute_error([ground_truth_t[0]], [predicted_t[0]]))\n",
    "                else:\n",
    "                    output.append('')\n",
    "                    output.append('')\n",
    "                    output.append('')\n",
    "                spamwriter.writerow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastpm2",
   "language": "python",
   "name": "fastpm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
